...../rl_toy/envs/rl_toy_env.py:300: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.
  self.is_terminal_state = self.config["is_terminal_state"] if callable(self.config["is_terminal_state"]) else lambda s: s in self.config["is_terminal_state"]
......
----------------------------------------------------------------------
Ran 10 tests in 0.124s

OK
[32;1;4mTEST_CONTINUOUS_DYNAMICS[0m
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'state_space': 10, 'action_space': 11}
#TODO for cont. spaces?: init_reward_function
#TODO for cont. spaces
#TODO for cont. spaces: reset
RESET called. State reset to: [1.59339006 0.68189965 1.49608203 0.19183192]
self.augmented_state, len [[nan, nan, nan, nan], [nan, nan, nan, nan], [nan, nan, nan, nan], [nan, nan, nan, nan], [nan, nan, nan, nan], [nan, nan, nan, nan], [nan, nan, nan, nan], [nan, nan, nan, nan], [nan, nan, nan, nan], [nan, nan, nan, nan], array([1.59339006, 0.68189965, 1.49608203, 0.19183192])] 11
toy env instantiated with config: {'seed': {'env': 0, 'state_space': 10, 'action_space': 11}, 'state_space_type': 'continuous', 'action_space_type': 'continuous', 'state_space_dim': 4, 'action_space_dim': 4, 'transition_dynamics_order': 1, 'inertia': 1, 'time_unit': 1, 'delay': 1, 'sequence_length': 10, 'reward_scale': 1.0, 'generate_random_mdp': True, 'state_space_relevant_indices': range(0, 4), 'action_space_relevant_indices': range(0, 4)}
sars', done = [1.59339006 0.68189965 1.49608203 0.19183192] [1 1 1 1] 0.0 [2.59339006 1.68189965 2.49608203 1.19183192] False 

sars', done = [2.59339006 1.68189965 2.49608203 1.19183192] [1 1 1 1] 0.0 [3.59339006 2.68189965 3.49608203 2.19183192] False 

sars', done = [3.59339006 2.68189965 3.49608203 2.19183192] [1 1 1 1] 0.0 [4.59339006 3.68189965 4.49608203 3.19183192] False 

sars', done = [4.59339006 3.68189965 4.49608203 3.19183192] [1 1 1 1] 0.0 [5.59339006 4.68189965 5.49608203 4.19183192] False 

sars', done = [5.59339006 4.68189965 5.49608203 4.19183192] [1 1 1 1] 0.0 [6.59339006 5.68189965 6.49608203 5.19183192] False 

sars', done = [6.59339006 5.68189965 6.49608203 5.19183192] [1 1 1 1] 0.0 [7.59339006 6.68189965 7.49608203 6.19183192] False 

sars', done = [7.59339006 6.68189965 7.49608203 6.19183192] [1 1 1 1] 0.0 [8.59339006 7.68189965 8.49608203 7.19183192] False 

sars', done = [8.59339006 7.68189965 8.49608203 7.19183192] [1 1 1 1] 0.0 [9.59339006 8.68189965 9.49608203 8.19183192] False 

sars', done = [9.59339006 8.68189965 9.49608203 8.19183192] [1 1 1 1] 0.0 [10.59339006  9.68189965 10.49608203  9.19183192] False 

sars', done = [10.59339006  9.68189965 10.49608203  9.19183192] [1 1 1 1] 0.0 [11.59339006 10.68189965 11.49608203 10.19183192] False 

uu.shape, dd.shape, vv.shape = (10, 10) (4,) (4, 4)
total_dist of pts from fit line: 1.2560739669470201e-15
sars', done = [11.59339006 10.68189965 11.49608203 10.19183192] [1 1 1 1] -1.25607396694702e-16 [12.59339006 11.68189965 12.49608203 11.19183192] False 

uu.shape, dd.shape, vv.shape = (10, 10) (4,) (4, 4)
total_dist of pts from fit line: 1.2560739669470201e-15
sars', done = [12.59339006 11.68189965 12.49608203 11.19183192] [1 1 1 1] -1.25607396694702e-16 [13.59339006 12.68189965 13.49608203 12.19183192] False 

uu.shape, dd.shape, vv.shape = (10, 10) (4,) (4, 4)
total_dist of pts from fit line: 7.691850745534255e-16
sars', done = [13.59339006 12.68189965 13.49608203 12.19183192] [1 1 1 1] -7.691850745534256e-17 [14.59339006 13.68189965 14.49608203 13.19183192] False 

uu.shape, dd.shape, vv.shape = (10, 10) (4,) (4, 4)
total_dist of pts from fit line: 1.538370149106851e-15
sars', done = [14.59339006 13.68189965 14.49608203 13.19183192] [1 1 1 1] -1.5383701491068512e-16 [15.59339006 14.68189965 15.49608203 14.19183192] False 

uu.shape, dd.shape, vv.shape = (10, 10) (4,) (4, 4)
total_dist of pts from fit line: 1.776356839400251e-15
sars', done = [15.59339006 14.68189965 15.49608203 14.19183192] [1 1 1 1] -1.7763568394002508e-16 [16.59339006 15.68189965 16.49608203 15.19183192] False 

uu.shape, dd.shape, vv.shape = (10, 10) (4,) (4, 4)
total_dist of pts from fit line: 1.776356839400251e-15
sars', done = [16.59339006 15.68189965 16.49608203 15.19183192] [1 1 1 1] -1.7763568394002508e-16 [17.59339006 16.68189965 17.49608203 16.19183192] False 

uu.shape, dd.shape, vv.shape = (10, 10) (4,) (4, 4)
total_dist of pts from fit line: 1.538370149106851e-15
sars', done = [17.59339006 16.68189965 17.49608203 16.19183192] [1 1 1 1] -1.5383701491068512e-16 [18.59339006 17.68189965 18.49608203 17.19183192] False 

uu.shape, dd.shape, vv.shape = (10, 10) (4,) (4, 4)
total_dist of pts from fit line: 1.776356839400251e-15
sars', done = [18.59339006 17.68189965 18.49608203 17.19183192] [1 1 1 1] -1.7763568394002508e-16 [19.59339006 18.68189965 19.49608203 18.19183192] False 

uu.shape, dd.shape, vv.shape = (10, 10) (4,) (4, 4)
total_dist of pts from fit line: 2.9457545650361178e-15
sars', done = [19.59339006 18.68189965 19.49608203 18.19183192] [1 1 1 1] -2.9457545650361176e-16 [20.59339006 19.68189965 20.49608203 19.19183192] False 

uu.shape, dd.shape, vv.shape = (10, 10) (4,) (4, 4)
total_dist of pts from fit line: 2.9457545650361178e-15
sars', done = [20.59339006 19.68189965 20.49608203 19.19183192] [1 1 1 1] -2.9457545650361176e-16 [21.59339006 20.68189965 21.49608203 20.19183192] False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 -1.7578652954934155e-15 0 20
#TODO for cont. spaces: reset
RESET called. State reset to: [ 2.53644409 -0.33852992 -0.16403416 -0.76406872]
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'state_space': 10, 'action_space': 11}
#TODO for cont. spaces?: init_reward_function
#TODO for cont. spaces
#TODO for cont. spaces: reset
RESET called. State reset to: [ 1.59339006  0.68189965  1.49608203  0.19183192  2.53644409 -0.33852992
 -0.16403416]
self.augmented_state, len [[nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], array([ 1.59339006,  0.68189965,  1.49608203,  0.19183192,  2.53644409,
       -0.33852992, -0.16403416])] 11
toy env instantiated with config: {'seed': {'env': 0, 'state_space': 10, 'action_space': 11}, 'state_space_type': 'continuous', 'action_space_type': 'continuous', 'state_space_dim': 7, 'action_space_dim': 7, 'transition_dynamics_order': 1, 'inertia': 1, 'time_unit': 1, 'delay': 1, 'sequence_length': 10, 'reward_scale': 1.0, 'generate_random_mdp': True, 'state_space_relevant_indices': [0, 1, 2, 6], 'action_space_relevant_indices': [0, 2, 3, 5]}
sars', done = [ 1.59339006  0.68189965  1.49608203  0.19183192  2.53644409 -0.33852992
 -0.16403416] [1 1 1 1 1 1 1] 0.0 [2.59339006 1.68189965 2.49608203 1.19183192 3.53644409 0.66147008
 0.83596584] False 

sars', done = [2.59339006 1.68189965 2.49608203 1.19183192 3.53644409 0.66147008
 0.83596584] [1 1 1 1 1 1 1] 0.0 [3.59339006 2.68189965 3.49608203 2.19183192 4.53644409 1.66147008
 1.83596584] False 

sars', done = [3.59339006 2.68189965 3.49608203 2.19183192 4.53644409 1.66147008
 1.83596584] [1 1 1 1 1 1 1] 0.0 [4.59339006 3.68189965 4.49608203 3.19183192 5.53644409 2.66147008
 2.83596584] False 

sars', done = [4.59339006 3.68189965 4.49608203 3.19183192 5.53644409 2.66147008
 2.83596584] [1 1 1 1 1 1 1] 0.0 [5.59339006 4.68189965 5.49608203 4.19183192 6.53644409 3.66147008
 3.83596584] False 

sars', done = [5.59339006 4.68189965 5.49608203 4.19183192 6.53644409 3.66147008
 3.83596584] [1 1 1 1 1 1 1] 0.0 [6.59339006 5.68189965 6.49608203 5.19183192 7.53644409 4.66147008
 4.83596584] False 

sars', done = [6.59339006 5.68189965 6.49608203 5.19183192 7.53644409 4.66147008
 4.83596584] [1 1 1 1 1 1 1] 0.0 [7.59339006 6.68189965 7.49608203 6.19183192 8.53644409 5.66147008
 5.83596584] False 

sars', done = [7.59339006 6.68189965 7.49608203 6.19183192 8.53644409 5.66147008
 5.83596584] [1 1 1 1 1 1 1] 0.0 [8.59339006 7.68189965 8.49608203 7.19183192 9.53644409 6.66147008
 6.83596584] False 

sars', done = [8.59339006 7.68189965 8.49608203 7.19183192 9.53644409 6.66147008
 6.83596584] [1 1 1 1 1 1 1] 0.0 [ 9.59339006  8.68189965  9.49608203  8.19183192 10.53644409  7.66147008
  7.83596584] False 

sars', done = [ 9.59339006  8.68189965  9.49608203  8.19183192 10.53644409  7.66147008
  7.83596584] [1 1 1 1 1 1 1] 0.0 [10.59339006  9.68189965 10.49608203  9.19183192 11.53644409  8.66147008
  8.83596584] False 

sars', done = [10.59339006  9.68189965 10.49608203  9.19183192 11.53644409  8.66147008
  8.83596584] [1 1 1 1 1 1 1] 0.0 [11.59339006 10.68189965 11.49608203 10.19183192 12.53644409  9.66147008
  9.83596584] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 2.6656007498500226e-07
sars', done = [11.59339006 10.68189965 11.49608203 10.19183192 12.53644409  9.66147008
  9.83596584] [1 1 1 1 1 1 1] -2.6656007498500227e-08 [12.59339006 11.68189965 12.49608203 11.19183192 13.53644409 10.66147008
 10.83596584] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 4.2146848510894035e-08
sars', done = [12.59339006 11.68189965 12.49608203 11.19183192 13.53644409 10.66147008
 10.83596584] [1 1 1 1 1 1 1] -4.214684851089403e-09 [13.59339006 12.68189965 13.49608203 12.19183192 14.53644409 11.66147008
 11.83596584] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 4.2146848510894035e-08
sars', done = [13.59339006 12.68189965 13.49608203 12.19183192 14.53644409 11.66147008
 11.83596584] [1 1 1 1 1 1 1] -4.214684851089403e-09 [14.59339006 13.68189965 14.49608203 13.19183192 15.53644409 12.66147008
 12.83596584] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 1.264405455326821e-07
sars', done = [14.59339006 13.68189965 14.49608203 13.19183192 15.53644409 12.66147008
 12.83596584] [1 1 1 1 1 1 1] -1.2644054553268211e-08 [15.59339006 14.68189965 15.49608203 14.19183192 16.53644409 13.66147008
 13.83596584] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 0.0
sars', done = [15.59339006 14.68189965 15.49608203 14.19183192 16.53644409 13.66147008
 13.83596584] [1 1 1 1 1 1 1] 0.0 [16.59339006 15.68189965 16.49608203 15.19183192 17.53644409 14.66147008
 14.83596584] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 1.0323827311807139e-07
sars', done = [16.59339006 15.68189965 16.49608203 15.19183192 17.53644409 14.66147008
 14.83596584] [1 1 1 1 1 1 1] -1.032382731180714e-08 [17.59339006 16.68189965 17.49608203 16.19183192 18.53644409 15.66147008
 15.83596584] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 8.429369702178807e-08
sars', done = [17.59339006 16.68189965 17.49608203 16.19183192 18.53644409 15.66147008
 15.83596584] [1 1 1 1 1 1 1] -8.429369702178806e-09 [18.59339006 17.68189965 18.49608203 17.19183192 19.53644409 16.66147008
 16.83596584] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 0.0
sars', done = [18.59339006 17.68189965 18.49608203 17.19183192 19.53644409 16.66147008
 16.83596584] [1 1 1 1 1 1 1] 0.0 [19.59339006 18.68189965 19.49608203 18.19183192 20.53644409 17.66147008
 17.83596584] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 3.2271227612335056e-07
sars', done = [19.59339006 18.68189965 19.49608203 18.19183192 20.53644409 17.66147008
 17.83596584] [1 1 1 1 1 1 1] -3.2271227612335054e-08 [20.59339006 19.68189965 20.49608203 19.19183192 21.53644409 18.66147008
 18.83596584] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 4.2146848510894035e-08
sars', done = [20.59339006 19.68189965 20.49608203 19.19183192 21.53644409 18.66147008
 18.83596584] [1 1 1 1 1 1 1] -4.214684851089403e-09 [21.59339006 20.68189965 21.49608203 20.19183192 22.53644409 19.66147008
 19.83596584] False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 -1.0296854123135764e-07 0 20
#TODO for cont. spaces: reset
RESET called. State reset to: [-0.76406872 -0.29131992 -0.60891991  1.8963932  -1.16305124  0.5513001
  1.19679292]
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'state_space': 10, 'action_space': 11}
#TODO for cont. spaces?: init_reward_function
#TODO for cont. spaces
#TODO for cont. spaces: reset
RESET called. State reset to: [ 0.92834036  2.16924632 -4.88226269  2.80702663  0.36006653  2.80813054
 -0.12869191]
self.augmented_state, len [[nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], array([ 0.92834036,  2.16924632, -4.88226269,  2.80702663,  0.36006653,
        2.80813054, -0.12869191])] 11
toy env instantiated with config: {'seed': {'env': 0, 'state_space': 10, 'action_space': 11}, 'state_space_type': 'continuous', 'action_space_type': 'continuous', 'state_space_dim': 7, 'action_space_dim': 7, 'transition_dynamics_order': 1, 'inertia': 1, 'time_unit': 1, 'delay': 1, 'sequence_length': 10, 'reward_scale': 1.0, 'generate_random_mdp': True, 'state_space_relevant_indices': [0, 1, 2, 6], 'action_space_relevant_indices': [0, 2, 3, 5], 'state_space_max': 5, 'action_space_max': 1}
next_state out of bounds. next_state, clipping to [-0.07165964  1.16924632 -5.88226269  1.80702663 -0.63993347  1.80813054
 -1.12869191] [-0.07165964  1.16924632 -5.          1.80702663 -0.63993347  1.80813054
 -1.12869191]
sars', done = [ 0.92834036  2.16924632 -4.88226269  2.80702663  0.36006653  2.80813054
 -0.12869191] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-0.07165964  1.16924632 -5.          1.80702663 -0.63993347  1.80813054
 -1.12869191] False 

next_state out of bounds. next_state, clipping to [-1.07165964  0.16924632 -6.          0.80702663 -1.63993347  0.80813054
 -2.12869191] [-1.07165964  0.16924632 -5.          0.80702663 -1.63993347  0.80813054
 -2.12869191]
sars', done = [-0.07165964  1.16924632 -5.          1.80702663 -0.63993347  1.80813054
 -1.12869191] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-1.07165964  0.16924632 -5.          0.80702663 -1.63993347  0.80813054
 -2.12869191] False 

next_state out of bounds. next_state, clipping to [-2.07165964 -0.83075368 -6.         -0.19297337 -2.63993347 -0.19186946
 -3.12869191] [-2.07165964 -0.83075368 -5.         -0.19297337 -2.63993347 -0.19186946
 -3.12869191]
sars', done = [-1.07165964  0.16924632 -5.          0.80702663 -1.63993347  0.80813054
 -2.12869191] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-2.07165964 -0.83075368 -5.         -0.19297337 -2.63993347 -0.19186946
 -3.12869191] False 

next_state out of bounds. next_state, clipping to [-3.07165964 -1.83075368 -6.         -1.19297337 -3.63993347 -1.19186946
 -4.12869191] [-3.07165964 -1.83075368 -5.         -1.19297337 -3.63993347 -1.19186946
 -4.12869191]
sars', done = [-2.07165964 -0.83075368 -5.         -0.19297337 -2.63993347 -0.19186946
 -3.12869191] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-3.07165964 -1.83075368 -5.         -1.19297337 -3.63993347 -1.19186946
 -4.12869191] False 

next_state out of bounds. next_state, clipping to [-4.07165964 -2.83075368 -6.         -2.19297337 -4.63993347 -2.19186946
 -5.12869191] [-4.07165964 -2.83075368 -5.         -2.19297337 -4.63993347 -2.19186946
 -5.        ]
sars', done = [-3.07165964 -1.83075368 -5.         -1.19297337 -3.63993347 -1.19186946
 -4.12869191] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-4.07165964 -2.83075368 -5.         -2.19297337 -4.63993347 -2.19186946
 -5.        ] False 

next_state out of bounds. next_state, clipping to [-5.07165964 -3.83075368 -6.         -3.19297337 -5.63993347 -3.19186946
 -6.        ] [-5.         -3.83075368 -5.         -3.19297337 -5.         -3.19186946
 -5.        ]
sars', done = [-4.07165964 -2.83075368 -5.         -2.19297337 -4.63993347 -2.19186946
 -5.        ] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-5.         -3.83075368 -5.         -3.19297337 -5.         -3.19186946
 -5.        ] False 

next_state out of bounds. next_state, clipping to [-6.         -4.83075368 -6.         -4.19297337 -6.         -4.19186946
 -6.        ] [-5.         -4.83075368 -5.         -4.19297337 -5.         -4.19186946
 -5.        ]
sars', done = [-5.         -3.83075368 -5.         -3.19297337 -5.         -3.19186946
 -5.        ] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-5.         -4.83075368 -5.         -4.19297337 -5.         -4.19186946
 -5.        ] False 

next_state out of bounds. next_state, clipping to [-6.         -5.83075368 -6.         -5.19297337 -6.         -5.19186946
 -6.        ] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5.         -4.83075368 -5.         -4.19297337 -5.         -4.19186946
 -5.        ] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-5. -5. -5. -5. -5. -5. -5.] False 

next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-5. -5. -5. -5. -5. -5. -5.] False 

next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-5. -5. -5. -5. -5. -5. -5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 6.800110954003138
next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] -0.6800110954003138 [-5. -5. -5. -5. -5. -5. -5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 6.6540913723199315
next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] -0.6654091372319931 [-5. -5. -5. -5. -5. -5. -5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 5.85986732214371
next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] -0.585986732214371 [-5. -5. -5. -5. -5. -5. -5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 4.600092978467991
next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] -0.4600092978467991 [-5. -5. -5. -5. -5. -5. -5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 3.1640828313481997
next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] -0.31640828313481995 [-5. -5. -5. -5. -5. -5. -5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 1.5105220119376443
next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] -0.15105220119376445 [-5. -5. -5. -5. -5. -5. -5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 0.5338360254751129
next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] -0.05338360254751129 [-5. -5. -5. -5. -5. -5. -5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 0.0
next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-5. -5. -5. -5. -5. -5. -5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 0.0
next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-5. -5. -5. -5. -5. -5. -5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 0.0
next_state out of bounds. next_state, clipping to [-6. -6. -6. -6. -6. -6. -6.] [-5. -5. -5. -5. -5. -5. -5.]
sars', done = [-5. -5. -5. -5. -5. -5. -5.] [-1 -1 -1 -1 -1 -1 -1] 0.0 [-5. -5. -5. -5. -5. -5. -5.] False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 -2.9122603495695727 0 20
#TODO for cont. spaces: reset
RESET called. State reset to: [ 0.96422742 -4.17263562 -4.71264267 -4.19641802 -0.90090835 -4.02478309
 -1.92553976]
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'state_space': 10, 'action_space': 11}
self.term_spaces samples: [ 0.48270042  2.63463726 -4.74957174 -0.33867457] [ 2.51858748 -1.70724468 -2.57995172 -0.13552242]
#TODO for cont. spaces?: init_reward_function
#TODO for cont. spaces
#TODO for cont. spaces: reset
A state was sampled in term state subspace. Therefore, resampling. State was, subspace was: [ 0.92834036  2.16924632 -4.88226269  2.80702663  0.36006653  2.80813054
 -0.12869191] 0
RESET called. State reset to: [ 0.96422742 -4.17263562 -4.71264267 -4.19641802 -0.90090835 -4.02478309
 -1.92553976]
self.augmented_state, len [[nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], array([ 0.96422742, -4.17263562, -4.71264267, -4.19641802, -0.90090835,
       -4.02478309, -1.92553976])] 11
toy env instantiated with config: {'seed': {'env': 0, 'state_space': 10, 'action_space': 11}, 'state_space_type': 'continuous', 'action_space_type': 'continuous', 'state_space_dim': 7, 'action_space_dim': 7, 'transition_dynamics_order': 1, 'inertia': 1, 'time_unit': 1, 'delay': 1, 'sequence_length': 10, 'reward_scale': 1.0, 'generate_random_mdp': True, 'state_space_relevant_indices': [0, 1, 2, 6], 'action_space_relevant_indices': [0, 2, 3, 5], 'state_space_max': 5, 'action_space_max': 1, 'terminal_states': [[0.92834036, 2.16924632, -4.88226269, -0.12869191], [2.96422742, -2.17263562, -2.71264267, 0.07446024]], 'term_state_edge': 1.0}
sars', done = [ 0.96422742 -4.17263562 -4.71264267 -4.19641802 -0.90090835 -4.02478309
 -1.92553976] [1 1 1 1 1 1 1] 0.0 [ 1.96422742 -3.17263562 -3.71264267 -3.19641802  0.09909165 -3.02478309
 -0.92553976] False 

sars', done = [ 1.96422742 -3.17263562 -3.71264267 -3.19641802  0.09909165 -3.02478309
 -0.92553976] [1 1 1 1 1 1 1] 0.0 [ 2.96422742 -2.17263562 -2.71264267 -2.19641802  1.09909165 -2.02478309
  0.07446024] True 

sars', done = [ 2.96422742 -2.17263562 -2.71264267 -2.19641802  1.09909165 -2.02478309
  0.07446024] [1 1 1 1 1 1 1] 0.0 [ 3.96422742 -1.17263562 -1.71264267 -1.19641802  2.09909165 -1.02478309
  1.07446024] False 

sars', done = [ 3.96422742 -1.17263562 -1.71264267 -1.19641802  2.09909165 -1.02478309
  1.07446024] [1 1 1 1 1 1 1] 0.0 [ 4.96422742 -0.17263562 -0.71264267 -0.19641802  3.09909165 -0.02478309
  2.07446024] False 

next_state out of bounds. next_state, clipping to [5.96422742 0.82736438 0.28735733 0.80358198 4.09909165 0.97521691
 3.07446024] [5.         0.82736438 0.28735733 0.80358198 4.09909165 0.97521691
 3.07446024]
sars', done = [ 4.96422742 -0.17263562 -0.71264267 -0.19641802  3.09909165 -0.02478309
  2.07446024] [1 1 1 1 1 1 1] 0.0 [5.         0.82736438 0.28735733 0.80358198 4.09909165 0.97521691
 3.07446024] False 

next_state out of bounds. next_state, clipping to [6.         1.82736438 1.28735733 1.80358198 5.09909165 1.97521691
 4.07446024] [5.         1.82736438 1.28735733 1.80358198 5.         1.97521691
 4.07446024]
sars', done = [5.         0.82736438 0.28735733 0.80358198 4.09909165 0.97521691
 3.07446024] [1 1 1 1 1 1 1] 0.0 [5.         1.82736438 1.28735733 1.80358198 5.         1.97521691
 4.07446024] False 

next_state out of bounds. next_state, clipping to [6.         2.82736438 2.28735733 2.80358198 6.         2.97521691
 5.07446024] [5.         2.82736438 2.28735733 2.80358198 5.         2.97521691
 5.        ]
sars', done = [5.         1.82736438 1.28735733 1.80358198 5.         1.97521691
 4.07446024] [1 1 1 1 1 1 1] 0.0 [5.         2.82736438 2.28735733 2.80358198 5.         2.97521691
 5.        ] False 

next_state out of bounds. next_state, clipping to [6.         3.82736438 3.28735733 3.80358198 6.         3.97521691
 6.        ] [5.         3.82736438 3.28735733 3.80358198 5.         3.97521691
 5.        ]
sars', done = [5.         2.82736438 2.28735733 2.80358198 5.         2.97521691
 5.        ] [1 1 1 1 1 1 1] 0.0 [5.         3.82736438 3.28735733 3.80358198 5.         3.97521691
 5.        ] False 

next_state out of bounds. next_state, clipping to [6.         4.82736438 4.28735733 4.80358198 6.         4.97521691
 6.        ] [5.         4.82736438 4.28735733 4.80358198 5.         4.97521691
 5.        ]
sars', done = [5.         3.82736438 3.28735733 3.80358198 5.         3.97521691
 5.        ] [1 1 1 1 1 1 1] 0.0 [5.         4.82736438 4.28735733 4.80358198 5.         4.97521691
 5.        ] False 

next_state out of bounds. next_state, clipping to [6.         5.82736438 5.28735733 5.80358198 6.         5.97521691
 6.        ] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5.         4.82736438 4.28735733 4.80358198 5.         4.97521691
 5.        ] [1 1 1 1 1 1 1] 0.0 [5. 5. 5. 5. 5. 5. 5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 8.675946437108651
next_state out of bounds. next_state, clipping to [6. 6. 6. 6. 6. 6. 6.] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5. 5. 5. 5. 5. 5. 5.] [1 1 1 1 1 1 1] -0.8675946437108651 [5. 5. 5. 5. 5. 5. 5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 9.005169266620024
next_state out of bounds. next_state, clipping to [6. 6. 6. 6. 6. 6. 6.] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5. 5. 5. 5. 5. 5. 5.] [1 1 1 1 1 1 1] -0.9005169266620024 [5. 5. 5. 5. 5. 5. 5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 8.21174706911415
next_state out of bounds. next_state, clipping to [6. 6. 6. 6. 6. 6. 6.] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5. 5. 5. 5. 5. 5. 5.] [1 1 1 1 1 1 1] -0.821174706911415 [5. 5. 5. 5. 5. 5. 5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 6.661622213937021
next_state out of bounds. next_state, clipping to [6. 6. 6. 6. 6. 6. 6.] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5. 5. 5. 5. 5. 5. 5.] [1 1 1 1 1 1 1] -0.6661622213937022 [5. 5. 5. 5. 5. 5. 5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 4.840290844089009
next_state out of bounds. next_state, clipping to [6. 6. 6. 6. 6. 6. 6.] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5. 5. 5. 5. 5. 5. 5.] [1 1 1 1 1 1 1] -0.4840290844089009 [5. 5. 5. 5. 5. 5. 5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 3.4287470397611908
next_state out of bounds. next_state, clipping to [6. 6. 6. 6. 6. 6. 6.] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5. 5. 5. 5. 5. 5. 5.] [1 1 1 1 1 1 1] -0.3428747039761191 [5. 5. 5. 5. 5. 5. 5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 1.9979526111130854
next_state out of bounds. next_state, clipping to [6. 6. 6. 6. 6. 6. 6.] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5. 5. 5. 5. 5. 5. 5.] [1 1 1 1 1 1 1] -0.19979526111130855 [5. 5. 5. 5. 5. 5. 5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 1.039328264166574
next_state out of bounds. next_state, clipping to [6. 6. 6. 6. 6. 6. 6.] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5. 5. 5. 5. 5. 5. 5.] [1 1 1 1 1 1 1] -0.1039328264166574 [5. 5. 5. 5. 5. 5. 5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 0.7050810876018141
next_state out of bounds. next_state, clipping to [6. 6. 6. 6. 6. 6. 6.] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5. 5. 5. 5. 5. 5. 5.] [1 1 1 1 1 1 1] -0.0705081087601814 [5. 5. 5. 5. 5. 5. 5.] False 

uu.shape, dd.shape, vv.shape = (10, 10) (7,) (7, 7)
total_dist of pts from fit line: 0.0
next_state out of bounds. next_state, clipping to [6. 6. 6. 6. 6. 6. 6.] [5. 5. 5. 5. 5. 5. 5.]
sars', done = [5. 5. 5. 5. 5. 5. 5.] [1 1 1 1 1 1 1] 0.0 [5. 5. 5. 5. 5. 5. 5.] False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 -4.456588483351152 0 20
#TODO for cont. spaces: reset
RESET called. State reset to: [-0.75854262  1.23682862  2.94220605  1.35532093  2.67297672  2.55933763
  1.57803874]
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'state_space': 10, 'action_space': 11}
self.term_spaces samples: [ 0.48270042  2.63463726 -4.74957174 -0.33867457] [ 2.51858748 -1.70724468 -2.57995172 -0.13552242]
#TODO for cont. spaces?: init_reward_function
#TODO for cont. spaces
#TODO for cont. spaces: reset
A state was sampled in term state subspace. Therefore, resampling. State was, subspace was: [ 0.92834036  2.16924632 -4.88226269  2.80702663  0.36006653  2.80813054
 -0.12869191] 0
RESET called. State reset to: [ 0.96422742 -4.17263562 -4.71264267 -4.19641802 -0.90090835 -4.02478309
 -1.92553976]
self.augmented_state, len [[nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan], array([ 0.96422742, -4.17263562, -4.71264267, -4.19641802, -0.90090835,
       -4.02478309, -1.92553976])] 11
toy env instantiated with config: {'seed': {'env': 0, 'state_space': 10, 'action_space': 11}, 'state_space_type': 'continuous', 'action_space_type': 'continuous', 'state_space_dim': 7, 'action_space_dim': 7, 'transition_dynamics_order': 1, 'inertia': 1, 'time_unit': 1, 'delay': 1, 'sequence_length': 10, 'reward_scale': 1.0, 'generate_random_mdp': True, 'state_space_relevant_indices': [0, 1, 2, 6], 'action_space_relevant_indices': [0, 2, 3, 5], 'state_space_max': 5, 'action_space_max': 1, 'terminal_states': [[0.92834036, 2.16924632, -4.88226269, -0.12869191], [2.96422742, -2.17263562, -2.71264267, 0.07446024]], 'term_state_edge': 1.0, 'transition_noise': <function TestRLToyEnv.test_continuous_dynamics.<locals>.<lambda> at 0x7f887dddb378>}
sars', done = [ 0.96422742 -4.17263562 -4.71264267 -4.19641802 -0.90090835 -4.02478309
 -1.92553976] [1 1 1 1 1 1 1] 0.0 [ 1.25715391 -2.72582608 -3.56190734 -3.5426217   0.90596197 -3.53510777
 -0.90385213] False 

sars', done = [ 1.25715391 -2.72582608 -3.56190734 -3.5426217   0.90596197 -3.53510777
 -0.90385213] [1 1 1 1 1 1 1] 0.0 [ 1.90342939 -0.6251458  -2.87656563 -2.48317271  2.03932642 -2.08032816
  0.04361135] False 

sars', done = [ 1.90342939 -0.6251458  -2.87656563 -2.48317271  2.03932642 -2.08032816
  0.04361135] [1 1 1 1 1 1 1] 0.0 [ 3.77936892  0.04741981 -1.71519289 -1.28301072  2.80375652 -0.92779405
  0.18543041] False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 [1.93673756 1.8749242  0.62676637 0.60581466 1.17580467 1.1176384
 0.93240509] 0.0 0 3
#TODO for cont. spaces: reset
RESET called. State reset to: [-0.75854262  1.23682862  2.94220605  1.35532093  2.67297672  2.55933763
  1.57803874]
[32;1;4mTEST_CONTINUOUS_DYNAMICS_ORDER[0m
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'state_space': 10, 'action_space': 11}
#TODO for cont. spaces?: init_reward_function
#TODO for cont. spaces
#TODO for cont. spaces: reset
RESET called. State reset to: [1.59339006 0.68189965]
self.augmented_state, len [[nan, nan], [nan, nan], array([1.59339006, 0.68189965])] 3
toy env instantiated with config: {'seed': {'env': 0, 'state_space': 10, 'action_space': 11}, 'state_space_type': 'continuous', 'action_space_type': 'continuous', 'state_space_dim': 2, 'action_space_dim': 2, 'transition_dynamics_order': 3, 'inertia': 2.0, 'time_unit': 0.01, 'delay': 0, 'sequence_length': 3, 'reward_scale': 1.0, 'generate_random_mdp': True, 'state_space_relevant_indices': range(0, 2), 'action_space_relevant_indices': range(0, 2)}
sars', done = [1.59339006 0.68189965] [2. 1.] 0.0 [1.59339023 0.68189973] False 

sars', done = [1.59339023 0.68189973] [2. 1.] 0.0 [1.59339139 0.68190032] False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 0.0 0 2
#TODO for cont. spaces: reset
RESET called. State reset to: [1.49608203 0.19183192]
TEST_DISCRETE_ALL_META_FEATURES
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}
config["relevant_state_space_size"] inited to: 8
config["irrelevant_state_space_size"] inited to: 0
config["relevant_action_space_size"] inited to: 8
config["irrelevant_action_space_size"] inited to: 0
Inited terminal states to self.config['is_terminal_state']: [7 6] total 2
self.relevant_init_state_dist: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667
 0.         0.        ]
No. of choices for each element in a possible sequence (Total no. of permutations will be a product of this), 1 random number out of possible perms, no. of possible perms [4, 5, 6] 9 120
specific_sequence that will be rewarded [0, 1, 5]
specific_sequence that will be rewarded [1, 0, 5]
specific_sequence that will be rewarded [5, 3, 4]
specific_sequence that will be rewarded [2, 4, 3]
specific_sequence that will be rewarded [5, 1, 3]
specific_sequence that will be rewarded [3, 4, 5]
specific_sequence that will be rewarded [2, 3, 1]
specific_sequence that will be rewarded [3, 5, 1]
specific_sequence that will be rewarded [4, 1, 5]
specific_sequence that will be rewarded [2, 3, 0]
specific_sequence that will be rewarded [0, 5, 4]
specific_sequence that will be rewarded [4, 5, 2]
specific_sequence that will be rewarded [2, 1, 0]
specific_sequence that will be rewarded [2, 1, 5]
specific_sequence that will be rewarded [1, 2, 5]
specific_sequence that will be rewarded [3, 1, 2]
specific_sequence that will be rewarded [2, 1, 3]
specific_sequence that will be rewarded [5, 3, 2]
specific_sequence that will be rewarded [4, 0, 5]
specific_sequence that will be rewarded [2, 0, 1]
specific_sequence that will be rewarded [5, 0, 4]
specific_sequence that will be rewarded [4, 2, 0]
specific_sequence that will be rewarded [3, 5, 0]
specific_sequence that will be rewarded [0, 5, 1]
specific_sequence that will be rewarded [1, 2, 3]
specific_sequence that will be rewarded [3, 1, 4]
specific_sequence that will be rewarded [3, 2, 4]
specific_sequence that will be rewarded [1, 3, 0]
specific_sequence that will be rewarded [0, 4, 5]
specific_sequence that will be rewarded [0, 1, 4]
Number of generated sequences that did not clash with an existing one when it was generated: 0
Total no. of rewarded sequences: 30 Out of 120
[[5 4 0 6 7 3 2 1]
 [4 7 2 6 5 1 0 3]
 [4 7 2 1 6 5 3 0]
 [7 2 5 3 6 0 1 4]
 [4 1 6 2 7 3 0 5]
 [1 6 5 0 2 7 3 4]
 [6 6 6 6 6 6 6 6]
 [7 7 7 7 7 7 7 7]] init_transition_function <class 'int'>
RESET called. curr_state set to: 2
self.augmented_state, len [nan, nan, nan, 2] 4
toy env instantiated with config: {'seed': {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}, 'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 8, 'action_space_size': 8, 'reward_density': 0.25, 'make_denser': False, 'terminal_state_density': 0.25, 'completely_connected': True, 'repeats_in_sequences': False, 'delay': 1, 'sequence_length': 3, 'reward_unit': 1.0, 'reward_noise': <function TestRLToyEnv.test_discrete_all_meta_features.<locals>.<lambda> at 0x7f887dddbd90>, 'generate_random_mdp': True, 'state_space_relevant_indices': range(0, 8), 'action_space_relevant_indices': range(0, 8), 'relevant_state_space_size': 8, 'irrelevant_state_space_size': 0, 'relevant_action_space_size': 8, 'irrelevant_action_space_size': 0, 'is_terminal_state': array([7, 6]), 'relevant_init_state_dist': array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667, 0.        , 0.        ]), 'transition_function': array([[5, 4, 0, 6, 7, 3, 2, 1],
       [4, 7, 2, 6, 5, 1, 0, 3],
       [4, 7, 2, 1, 6, 5, 3, 0],
       [7, 2, 5, 3, 6, 0, 1, 4],
       [4, 1, 6, 2, 7, 3, 0, 5],
       [1, 6, 5, 0, 2, 7, 3, 4],
       [6, 6, 6, 6, 6, 6, 6, 6],
       [7, 7, 7, 7, 7, 7, 7, 7]], dtype=object)}
[nan, nan, nan, 2] with delay 1
sars', done = 2 6 -0.2928080556634023 3 False 

[nan, nan, 2, 3] with delay 1
sars', done = 3 6 0.7706963434374121 1 False 

[nan, 2, 3, 1] with delay 1
sars', done = 1 2 -1.0174361161135361 2 False 

[2, 3, 1, 2] with delay 1
sars', done = 2 3 0.9572316829702407 1 False 

[3, 1, 2, 1] with delay 1
sars', done = 1 4 1.7876131086463287 5 False 

[1, 2, 1, 5] with delay 1
sars', done = 5 2 -0.5100869633167762 5 False 

[2, 1, 5, 5] with delay 1
sars', done = 5 7 0.9100216163737499 4 False 

[1, 5, 5, 4] with delay 1
sars', done = 4 5 0.48654863938747184 3 False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 3.9979359272209365 0 3.0 0 8
RESET called. curr_state set to: 4
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'relevant_state_space': 6, 'relevant_action_space': 6}
config["relevant_state_space_size"] inited to: 6
config["irrelevant_state_space_size"] inited to: 0
config["relevant_action_space_size"] inited to: 6
config["irrelevant_action_space_size"] inited to: 0
Inited terminal states to self.config['is_terminal_state']: [5] total 1
self.relevant_init_state_dist: [0.2 0.2 0.2 0.2 0.2 0. ]
No. of choices for each element in a possible sequence (Total no. of permutations will be a product of this), 1 random number out of possible perms, no. of possible perms [3, 4, 5] 2 60
specific_sequence that will be rewarded [4, 0, 3]
specific_sequence that will be rewarded [3, 4, 2]
specific_sequence that will be rewarded [3, 2, 4]
specific_sequence that will be rewarded [0, 1, 4]
specific_sequence that will be rewarded [4, 1, 3]
specific_sequence that will be rewarded [1, 0, 4]
specific_sequence that will be rewarded [1, 2, 4]
specific_sequence that will be rewarded [1, 2, 3]
specific_sequence that will be rewarded [0, 3, 2]
specific_sequence that will be rewarded [2, 0, 1]
specific_sequence that will be rewarded [0, 3, 4]
specific_sequence that will be rewarded [1, 0, 2]
specific_sequence that will be rewarded [1, 4, 0]
specific_sequence that will be rewarded [0, 2, 1]
specific_sequence that will be rewarded [2, 1, 0]
Number of generated sequences that did not clash with an existing one when it was generated: 0
Total no. of rewarded sequences: 15 Out of 60
[[2 5 4 3 0 1]
 [4 3 5 0 2 1]
 [3 5 1 2 0 4]
 [1 4 2 0 3 5]
 [2 4 3 5 0 1]
 [5 5 5 5 5 5]] init_transition_function <class 'int'>
RESET called. curr_state set to: 2
self.possible_remaining_sequences [[[4], [3], [3], [0], [4], [1], [1], [1], [0], [2], [0], [1], [1], [0], [2]], [], []]
 self.delay, self.sequence_length: 0 3
self.augmented_state, len [nan, nan, 2] 3
toy env instantiated with config: {'seed': {'env': 0, 'relevant_state_space': 6, 'relevant_action_space': 6}, 'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 6, 'action_space_size': 6, 'reward_density': 0.25, 'make_denser': True, 'terminal_state_density': 0.25, 'completely_connected': True, 'repeats_in_sequences': False, 'delay': 0, 'sequence_length': 3, 'reward_unit': 1.0, 'generate_random_mdp': True, 'state_space_relevant_indices': range(0, 6), 'action_space_relevant_indices': range(0, 6), 'relevant_state_space_size': 6, 'irrelevant_state_space_size': 0, 'relevant_action_space_size': 6, 'irrelevant_action_space_size': 0, 'is_terminal_state': array([5]), 'relevant_init_state_dist': array([0.2, 0.2, 0.2, 0.2, 0.2, 0. ]), 'transition_function': array([[2, 5, 4, 3, 0, 1],
       [4, 3, 5, 0, 2, 1],
       [3, 5, 1, 2, 0, 4],
       [1, 4, 2, 0, 3, 5],
       [2, 4, 3, 5, 0, 1],
       [5, 5, 5, 5, 5, 5]], dtype=object)}
rew 0.6666666666666666
self.possible_remaining_sequences [[[4], [3], [3], [0], [4], [1], [1], [1], [0], [2], [0], [1], [1], [0], [2]], [[2, 0], [2, 1]], []]
sars', done = 2 2 0.6666666666666666 1 False 

rew 2.3333333333333335
self.possible_remaining_sequences [[[4], [3], [3], [0], [4], [1], [1], [1], [0], [2], [0], [1], [1], [0], [2]], [[1, 0], [1, 2], [1, 2], [1, 0], [1, 4]], [[2, 1, 0]]]
sars', done = 1 4 2.3333333333333335 2 False 

rew 2.0
self.possible_remaining_sequences [[[4], [3], [3], [0], [4], [1], [1], [1], [0], [2], [0], [1], [1], [0], [2]], [[2, 0], [2, 1]], [[1, 2, 4], [1, 2, 3]]]
sars', done = 2 1 2.0 5 True 

rew 0.0
self.possible_remaining_sequences [[[4], [3], [3], [0], [4], [1], [1], [1], [0], [2], [0], [1], [1], [0], [2]], [], []]
sars', done = 5 1 0.0 5 True 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 5.0 0 4
RESET called. curr_state set to: 4
self.possible_remaining_sequences [[[4], [3], [3], [0], [4], [1], [1], [1], [0], [2], [0], [1], [1], [0], [2]], [], []]
 self.delay, self.sequence_length: 0 3
TEST_DISCRETE_MULTI_DISCRETE
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}
config["relevant_state_space_size"] inited to: 8
config["irrelevant_state_space_size"] inited to: 0
config["relevant_action_space_size"] inited to: 8
config["irrelevant_action_space_size"] inited to: 0
Inited terminal states to self.config['is_terminal_state']: [7 6] total 2
self.relevant_init_state_dist: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667
 0.         0.        ]
No. of choices for each element in a possible sequence (Total no. of permutations will be a product of this), 1 random number out of possible perms, no. of possible perms [6] 0 6
specific_sequence that will be rewarded [2]
Number of generated sequences that did not clash with an existing one when it was generated: 0
Total no. of rewarded sequences: 1 Out of 6
[[5 4 0 6 7 3 2 1]
 [4 7 2 6 5 1 0 3]
 [4 7 2 1 6 5 3 0]
 [7 2 5 3 6 0 1 4]
 [4 1 6 2 7 3 0 5]
 [1 6 5 0 2 7 3 4]
 [6 6 6 6 6 6 6 6]
 [7 7 7 7 7 7 7 7]] init_transition_function <class 'int'>
RESET called. Relevant part of state reset to: 0
RESET called. curr_state set to: [0, 0, 0]
self.possible_remaining_sequences [[[2]]]
 self.delay, self.sequence_length: 3 1
self.augmented_state, len [nan, nan, nan, 0] 4
toy env instantiated with config: {'seed': {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}, 'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': [2, 2, 2], 'state_space_relevant_indices': [0, 1, 2], 'action_space_size': [2, 2, 2], 'action_space_relevant_indices': [0, 1, 2], 'reward_density': 0.25, 'make_denser': True, 'terminal_state_density': 0.25, 'completely_connected': True, 'repeats_in_sequences': False, 'delay': 3, 'sequence_length': 1, 'reward_unit': 1.0, 'generate_random_mdp': True, 'state_space_multi_discrete_sizes': [2, 2, 2], 'relevant_state_space_size': 8, 'state_space_irrelevant_indices': [], 'irrelevant_state_space_size': 0, 'action_space_multi_discrete_sizes': [2, 2, 2], 'relevant_action_space_size': 8, 'action_space_irrelevant_indices': [], 'irrelevant_action_space_size': 0, 'is_terminal_state': array([7, 6]), 'relevant_init_state_dist': array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667, 0.        , 0.        ]), 'transition_function': array([[5, 4, 0, 6, 7, 3, 2, 1],
       [4, 7, 2, 6, 5, 1, 0, 3],
       [4, 7, 2, 1, 6, 5, 3, 0],
       [7, 2, 5, 3, 6, 0, 1, 4],
       [4, 1, 6, 2, 7, 3, 0, 5],
       [1, 6, 5, 0, 2, 7, 3, 4],
       [6, 6, 6, 6, 6, 6, 6, 6],
       [7, 7, 7, 7, 7, 7, 7, 7]], dtype=object)}
rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 0, 0] [1, 1, 0] 0.0 [0, 1, 0] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 1, 0] [0, 1, 0] 0.0 [0, 1, 0] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 1, 0] [1, 0, 1] 0.0 [1, 0, 1] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 1] [1, 0, 0] 0.0 [0, 1, 0] False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 1, 0] [1, 0, 1] 1.0 [1, 0, 1] False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 1] [0, 1, 0] 1.0 [1, 0, 1] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 1] [0, 1, 1] 0.0 [0, 0, 0] False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 0, 0] [0, 0, 1] 1.0 [1, 0, 0] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 0] [1, 0, 0] 0.0 [1, 1, 1] False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 3.0 0 9
RESET called. Relevant part of state reset to: 4
RESET called. curr_state set to: [1, 0, 0]
self.possible_remaining_sequences [[[2]]]
 self.delay, self.sequence_length: 3 1
[32;1;4mTEST_DISCRETE_MULTI_DISCRETE_IRRELEVANT_DIMENSIONS[0m
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8, 'irrelevant_state_space': 52, 'irrelevant_action_space': 65, 'state_space': 87, 'action_space': 89}
config["relevant_state_space_size"] inited to: 8
config["irrelevant_state_space_size"] inited to: 3
config["relevant_action_space_size"] inited to: 8
config["irrelevant_action_space_size"] inited to: 5
Caught Expected exception: config["irrelevant_state_space_size"] != config["irrelevant_action_space_size"]. For completely_connected transition graphs, they should be equal. Please provide valid values! Vals: 3 5. In future, "maximally_connected" graphs are planned to be supported!
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8, 'irrelevant_state_space': 52, 'irrelevant_action_space': 65, 'state_space': 87, 'action_space': 89}
config["relevant_state_space_size"] inited to: 8
config["irrelevant_state_space_size"] inited to: 5
config["relevant_action_space_size"] inited to: 8
config["irrelevant_action_space_size"] inited to: 5
Inited terminal states to self.config['is_terminal_state']: [7 6] total 2
self.relevant_init_state_dist: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667
 0.         0.        ]
self.irrelevant_init_state_dist: [0.2 0.2 0.2 0.2 0.2]
No. of choices for each element in a possible sequence (Total no. of permutations will be a product of this), 1 random number out of possible perms, no. of possible perms [6] 2 6
specific_sequence that will be rewarded [2]
Number of generated sequences that did not clash with an existing one when it was generated: 0
Total no. of rewarded sequences: 1 Out of 6
[[5 4 0 6 7 3 2 1]
 [4 7 2 6 5 1 0 3]
 [4 7 2 1 6 5 3 0]
 [7 2 5 3 6 0 1 4]
 [4 1 6 2 7 3 0 5]
 [1 6 5 0 2 7 3 4]
 [6 6 6 6 6 6 6 6]
 [7 7 7 7 7 7 7 7]] init_transition_function <class 'int'>
[[0 3 2 1 4]
 [2 4 0 1 3]
 [4 2 0 3 1]
 [2 0 4 3 1]
 [1 4 0 3 2]] init_transition_function _irrelevant <class 'int'>
RESET called. Relevant part of state reset to: 0
Irrelevant part of state reset to: 3
RESET called. curr_state set to: [0, 0, 0, 3]
self.possible_remaining_sequences [[[2]]]
 self.delay, self.sequence_length: 3 1
self.augmented_state, len [nan, nan, nan, 0] 4
toy env instantiated with config: {'seed': {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8, 'irrelevant_state_space': 52, 'irrelevant_action_space': 65, 'state_space': 87, 'action_space': 89}, 'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': [2, 2, 2, 5], 'state_space_relevant_indices': [0, 1, 2], 'action_space_size': [2, 5, 2, 2], 'action_space_relevant_indices': [0, 2, 3], 'reward_density': 0.25, 'make_denser': True, 'terminal_state_density': 0.25, 'completely_connected': True, 'repeats_in_sequences': False, 'delay': 3, 'sequence_length': 1, 'reward_unit': 1.0, 'generate_random_mdp': True, 'state_space_multi_discrete_sizes': [2, 2, 2, 5], 'relevant_state_space_size': 8, 'state_space_irrelevant_indices': [3], 'irrelevant_state_space_size': 5, 'action_space_multi_discrete_sizes': [2, 5, 2, 2], 'relevant_action_space_size': 8, 'action_space_irrelevant_indices': [1], 'irrelevant_action_space_size': 5, 'is_terminal_state': array([7, 6]), 'relevant_init_state_dist': array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667, 0.        , 0.        ]), 'irrelevant_init_state_dist': array([0.2, 0.2, 0.2, 0.2, 0.2]), 'transition_function': array([[5, 4, 0, 6, 7, 3, 2, 1],
       [4, 7, 2, 6, 5, 1, 0, 3],
       [4, 7, 2, 1, 6, 5, 3, 0],
       [7, 2, 5, 3, 6, 0, 1, 4],
       [4, 1, 6, 2, 7, 3, 0, 5],
       [1, 6, 5, 0, 2, 7, 3, 4],
       [6, 6, 6, 6, 6, 6, 6, 6],
       [7, 7, 7, 7, 7, 7, 7, 7]], dtype=object), 'transition_function_irrelevant': array([[0, 3, 2, 1, 4],
       [2, 4, 0, 1, 3],
       [4, 2, 0, 3, 1],
       [2, 0, 4, 3, 1],
       [1, 4, 0, 3, 2]], dtype=object)}
rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 0, 0, 3] [1, 4, 1, 0] 0.0 [0, 1, 0, 1] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 1, 0, 1] [0, 3, 1, 0] 0.0 [0, 1, 0, 1] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 1, 0, 1] [1, 4, 0, 1] 0.0 [1, 0, 1, 3] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 1, 3] [1, 0, 0, 0] 0.0 [0, 1, 0, 2] False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 1, 0, 2] [1, 2, 0, 1] 1.0 [1, 0, 1, 0] False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 1, 0] [0, 3, 1, 0] 1.0 [1, 0, 1, 1] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 1, 1] [0, 1, 1, 1] 0.0 [0, 0, 0, 4] False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 0, 0, 4] [0, 4, 0, 1] 1.0 [1, 0, 0, 2] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 0, 2] [1, 4, 0, 0] 0.0 [1, 1, 1, 1] False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 3.0 0 9
RESET called. Relevant part of state reset to: 2
Irrelevant part of state reset to: 3
RESET called. curr_state set to: [0, 1, 0, 3]
self.possible_remaining_sequences [[[2]]]
 self.delay, self.sequence_length: 3 1
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8, 'irrelevant_state_space': 52, 'irrelevant_action_space': 65, 'state_space': 87, 'action_space': 89}
config["relevant_state_space_size"] inited to: 8
config["irrelevant_state_space_size"] inited to: 5
config["relevant_action_space_size"] inited to: 8
config["irrelevant_action_space_size"] inited to: 5
Inited terminal states to self.config['is_terminal_state']: [7 6] total 2
self.relevant_init_state_dist: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667
 0.         0.        ]
self.irrelevant_init_state_dist: [0.2 0.2 0.2 0.2 0.2]
No. of choices for each element in a possible sequence (Total no. of permutations will be a product of this), 1 random number out of possible perms, no. of possible perms [6] 0 6
specific_sequence that will be rewarded [2]
Number of generated sequences that did not clash with an existing one when it was generated: 0
Total no. of rewarded sequences: 1 Out of 6
[[5 4 0 6 7 3 2 1]
 [4 7 2 6 5 1 0 3]
 [4 7 2 1 6 5 3 0]
 [7 2 5 3 6 0 1 4]
 [4 1 6 2 7 3 0 5]
 [1 6 5 0 2 7 3 4]
 [6 6 6 6 6 6 6 6]
 [7 7 7 7 7 7 7 7]] init_transition_function <class 'int'>
[[0 3 2 1 4]
 [2 4 0 1 3]
 [4 2 0 3 1]
 [2 0 4 3 1]
 [1 4 0 3 2]] init_transition_function _irrelevant <class 'int'>
RESET called. Relevant part of state reset to: 0
Irrelevant part of state reset to: 3
RESET called. curr_state set to: [0, 0, 0, 0, 3]
self.possible_remaining_sequences [[[2]]]
 self.delay, self.sequence_length: 3 1
self.augmented_state, len [nan, nan, nan, 0] 4
toy env instantiated with config: {'seed': {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8, 'irrelevant_state_space': 52, 'irrelevant_action_space': 65, 'state_space': 87, 'action_space': 89}, 'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': [2, 2, 2, 1, 5], 'state_space_relevant_indices': [0, 1, 2], 'action_space_size': [2, 5, 1, 1, 2, 2], 'action_space_relevant_indices': [0, 4, 5], 'reward_density': 0.25, 'make_denser': True, 'terminal_state_density': 0.25, 'completely_connected': True, 'repeats_in_sequences': False, 'delay': 3, 'sequence_length': 1, 'reward_unit': 1.0, 'generate_random_mdp': True, 'state_space_multi_discrete_sizes': [2, 2, 2, 1, 5], 'relevant_state_space_size': 8, 'state_space_irrelevant_indices': [3, 4], 'irrelevant_state_space_size': 5, 'action_space_multi_discrete_sizes': [2, 5, 1, 1, 2, 2], 'relevant_action_space_size': 8, 'action_space_irrelevant_indices': [1, 2, 3], 'irrelevant_action_space_size': 5, 'is_terminal_state': array([7, 6]), 'relevant_init_state_dist': array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667, 0.        , 0.        ]), 'irrelevant_init_state_dist': array([0.2, 0.2, 0.2, 0.2, 0.2]), 'transition_function': array([[5, 4, 0, 6, 7, 3, 2, 1],
       [4, 7, 2, 6, 5, 1, 0, 3],
       [4, 7, 2, 1, 6, 5, 3, 0],
       [7, 2, 5, 3, 6, 0, 1, 4],
       [4, 1, 6, 2, 7, 3, 0, 5],
       [1, 6, 5, 0, 2, 7, 3, 4],
       [6, 6, 6, 6, 6, 6, 6, 6],
       [7, 7, 7, 7, 7, 7, 7, 7]], dtype=object), 'transition_function_irrelevant': array([[0, 3, 2, 1, 4],
       [2, 4, 0, 1, 3],
       [4, 2, 0, 3, 1],
       [2, 0, 4, 3, 1],
       [1, 4, 0, 3, 2]], dtype=object)}
rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 0, 0, 0, 3] [1, 4, 0, 0, 1, 0] 0.0 [0, 1, 0, 0, 1] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 1, 0, 0, 1] [0, 3, 0, 0, 1, 0] 0.0 [0, 1, 0, 0, 1] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 1, 0, 0, 1] [1, 4, 0, 0, 0, 1] 0.0 [1, 0, 1, 0, 3] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 1, 0, 3] [1, 0, 0, 0, 0, 0] 0.0 [0, 1, 0, 0, 2] False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 1, 0, 0, 2] [1, 2, 0, 0, 0, 1] 1.0 [1, 0, 1, 0, 0] False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 1, 0, 0] [0, 3, 0, 0, 1, 0] 1.0 [1, 0, 1, 0, 1] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 1, 0, 1] [0, 1, 0, 0, 1, 1] 0.0 [0, 0, 0, 0, 4] False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = [0, 0, 0, 0, 4] [0, 4, 0, 0, 0, 1] 1.0 [1, 0, 0, 0, 2] False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = [1, 0, 0, 0, 2] [1, 4, 0, 0, 0, 0] 0.0 [1, 1, 1, 0, 1] False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 3.0 0 9
RESET called. Relevant part of state reset to: 2
Irrelevant part of state reset to: 3
RESET called. curr_state set to: [0, 1, 0, 0, 3]
self.possible_remaining_sequences [[[2]]]
 self.delay, self.sequence_length: 3 1
TEST_DISCRETE_P_NOISE
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}
config["relevant_state_space_size"] inited to: 8
config["irrelevant_state_space_size"] inited to: 0
config["relevant_action_space_size"] inited to: 8
config["irrelevant_action_space_size"] inited to: 0
Inited terminal states to self.config['is_terminal_state']: [7 6] total 2
self.relevant_init_state_dist: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667
 0.         0.        ]
No. of choices for each element in a possible sequence (Total no. of permutations will be a product of this), 1 random number out of possible perms, no. of possible perms [6] 0 6
specific_sequence that will be rewarded [2]
Number of generated sequences that did not clash with an existing one when it was generated: 0
Total no. of rewarded sequences: 1 Out of 6
[[5 4 0 6 7 3 2 1]
 [4 7 2 6 5 1 0 3]
 [4 7 2 1 6 5 3 0]
 [7 2 5 3 6 0 1 4]
 [4 1 6 2 7 3 0 5]
 [1 6 5 0 2 7 3 4]
 [6 6 6 6 6 6 6 6]
 [7 7 7 7 7 7 7 7]] init_transition_function <class 'int'>
RESET called. curr_state set to: 0
self.augmented_state, len [0] 1
toy env instantiated with config: {'seed': {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}, 'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 8, 'action_space_size': 8, 'reward_density': 0.25, 'make_denser': False, 'terminal_state_density': 0.25, 'completely_connected': True, 'repeats_in_sequences': False, 'delay': 0, 'sequence_length': 1, 'reward_unit': 1.0, 'transition_noise': 0.5, 'generate_random_mdp': True, 'state_space_relevant_indices': range(0, 8), 'action_space_relevant_indices': range(0, 8), 'relevant_state_space_size': 8, 'irrelevant_state_space_size': 0, 'relevant_action_space_size': 8, 'irrelevant_action_space_size': 0, 'is_terminal_state': array([7, 6]), 'relevant_init_state_dist': array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667, 0.        , 0.        ]), 'transition_function': array([[5, 4, 0, 6, 7, 3, 2, 1],
       [4, 7, 2, 6, 5, 1, 0, 3],
       [4, 7, 2, 1, 6, 5, 3, 0],
       [7, 2, 5, 3, 6, 0, 1, 4],
       [4, 1, 6, 2, 7, 3, 0, 5],
       [1, 6, 5, 0, 2, 7, 3, 4],
       [6, 6, 6, 6, 6, 6, 6, 6],
       [7, 7, 7, 7, 7, 7, 7, 7]], dtype=object)}
[0] with delay 0
sars', done = 0 6 0.0 2 False 

[2] with delay 0
NOISE inserted! old next_state, new_next_state 3 6
sars', done = 2 6 1.0 6 True 

[6] with delay 0
sars', done = 6 2 0.0 6 True 

[6] with delay 0
NOISE inserted! old next_state, new_next_state 6 3
sars', done = 6 6 0.0 3 False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 1.0 2 4
RESET called. curr_state set to: 4
TEST_DISCRETE_R_NOISE
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}
config["relevant_state_space_size"] inited to: 8
config["irrelevant_state_space_size"] inited to: 0
config["relevant_action_space_size"] inited to: 8
config["irrelevant_action_space_size"] inited to: 0
Inited terminal states to self.config['is_terminal_state']: [7 6] total 2
self.relevant_init_state_dist: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667
 0.         0.        ]
No. of choices for each element in a possible sequence (Total no. of permutations will be a product of this), 1 random number out of possible perms, no. of possible perms [6] 3 6
specific_sequence that will be rewarded [2]
Number of generated sequences that did not clash with an existing one when it was generated: 0
Total no. of rewarded sequences: 1 Out of 6
[[5 4 0 6 7 3 2 1]
 [4 7 2 6 5 1 0 3]
 [4 7 2 1 6 5 3 0]
 [7 2 5 3 6 0 1 4]
 [4 1 6 2 7 3 0 5]
 [1 6 5 0 2 7 3 4]
 [6 6 6 6 6 6 6 6]
 [7 7 7 7 7 7 7 7]] init_transition_function <class 'int'>
RESET called. curr_state set to: 0
self.augmented_state, len [0] 1
toy env instantiated with config: {'seed': {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}, 'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 8, 'action_space_size': 8, 'reward_density': 0.25, 'make_denser': False, 'terminal_state_density': 0.25, 'completely_connected': True, 'repeats_in_sequences': False, 'delay': 0, 'sequence_length': 1, 'reward_unit': 1.0, 'reward_noise': <function TestRLToyEnv.test_discrete_r_noise.<locals>.<lambda> at 0x7f887db4aea0>, 'generate_random_mdp': True, 'state_space_relevant_indices': range(0, 8), 'action_space_relevant_indices': range(0, 8), 'relevant_state_space_size': 8, 'irrelevant_state_space_size': 0, 'relevant_action_space_size': 8, 'irrelevant_action_space_size': 0, 'is_terminal_state': array([7, 6]), 'relevant_init_state_dist': array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667, 0.        , 0.        ]), 'transition_function': array([[5, 4, 0, 6, 7, 3, 2, 1],
       [4, 7, 2, 6, 5, 1, 0, 3],
       [4, 7, 2, 1, 6, 5, 3, 0],
       [7, 2, 5, 3, 6, 0, 1, 4],
       [4, 1, 6, 2, 7, 3, 0, 5],
       [1, 6, 5, 0, 2, 7, 3, 4],
       [6, 6, 6, 6, 6, 6, 6, 6],
       [7, 7, 7, 7, 7, 7, 7, 7]], dtype=object)}
[0] with delay 0
sars', done = 0 6 -0.4997164068740706 2 False 

[2] with delay 0
sars', done = 2 6 1.8051244284682069 3 False 

[3] with delay 0
sars', done = 3 2 -0.22481242175109611 5 False 

[5] with delay 0
sars', done = 5 1 0.08674894641195517 6 True 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 1.6164022035053287 0 1.0 0 4
RESET called. curr_state set to: 3
[32;1;4mTEST_DISCRETE_REWARD_DELAY[0m
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}
config["relevant_state_space_size"] inited to: 8
config["irrelevant_state_space_size"] inited to: 0
config["relevant_action_space_size"] inited to: 8
config["irrelevant_action_space_size"] inited to: 0
Inited terminal states to self.config['is_terminal_state']: [7 6] total 2
self.relevant_init_state_dist: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667
 0.         0.        ]
No. of choices for each element in a possible sequence (Total no. of permutations will be a product of this), 1 random number out of possible perms, no. of possible perms [6] 1 6
specific_sequence that will be rewarded [2]
Number of generated sequences that did not clash with an existing one when it was generated: 0
Total no. of rewarded sequences: 1 Out of 6
[[5 4 0 6 7 3 2 1]
 [4 7 2 6 5 1 0 3]
 [4 7 2 1 6 5 3 0]
 [7 2 5 3 6 0 1 4]
 [4 1 6 2 7 3 0 5]
 [1 6 5 0 2 7 3 4]
 [6 6 6 6 6 6 6 6]
 [7 7 7 7 7 7 7 7]] init_transition_function <class 'int'>
RESET called. curr_state set to: 0
self.possible_remaining_sequences [[[2]]]
 self.delay, self.sequence_length: 3 1
self.augmented_state, len [nan, nan, nan, 0] 4
toy env instantiated with config: {'seed': {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}, 'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 8, 'action_space_size': 8, 'reward_density': 0.25, 'make_denser': True, 'terminal_state_density': 0.25, 'completely_connected': True, 'repeats_in_sequences': False, 'delay': 3, 'sequence_length': 1, 'reward_unit': 1.0, 'generate_random_mdp': True, 'state_space_relevant_indices': range(0, 8), 'action_space_relevant_indices': range(0, 8), 'relevant_state_space_size': 8, 'irrelevant_state_space_size': 0, 'relevant_action_space_size': 8, 'irrelevant_action_space_size': 0, 'is_terminal_state': array([7, 6]), 'relevant_init_state_dist': array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667, 0.        , 0.        ]), 'transition_function': array([[5, 4, 0, 6, 7, 3, 2, 1],
       [4, 7, 2, 6, 5, 1, 0, 3],
       [4, 7, 2, 1, 6, 5, 3, 0],
       [7, 2, 5, 3, 6, 0, 1, 4],
       [4, 1, 6, 2, 7, 3, 0, 5],
       [1, 6, 5, 0, 2, 7, 3, 4],
       [6, 6, 6, 6, 6, 6, 6, 6],
       [7, 7, 7, 7, 7, 7, 7, 7]], dtype=object)}
rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = 0 6 0.0 2 False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = 2 2 0.0 2 False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = 2 5 0.0 5 False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = 5 4 0.0 2 False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = 2 5 1.0 5 False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = 5 2 1.0 5 False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = 5 3 0.0 0 False 

rew 1.0
self.possible_remaining_sequences [[[2]]]
sars', done = 0 1 1.0 4 False 

rew 0.0
self.possible_remaining_sequences [[[2]]]
sars', done = 4 4 0.0 7 True 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 3.0 0 9
RESET called. curr_state set to: 4
self.possible_remaining_sequences [[[2]]]
 self.delay, self.sequence_length: 3 1
Env SEED set to: 0 Returned seed from Gym: 0
Seeds set to: {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}
config["relevant_state_space_size"] inited to: 8
config["irrelevant_state_space_size"] inited to: 0
config["relevant_action_space_size"] inited to: 8
config["irrelevant_action_space_size"] inited to: 0
Inited terminal states to self.config['is_terminal_state']: [7 6] total 2
self.relevant_init_state_dist: [0.16666667 0.16666667 0.16666667 0.16666667 0.16666667 0.16666667
 0.         0.        ]
No. of choices for each element in a possible sequence (Total no. of permutations will be a product of this), 1 random number out of possible perms, no. of possible perms [4, 5, 6] 23 120
specific_sequence that will be rewarded [0, 1, 5]
specific_sequence that will be rewarded [1, 0, 5]
specific_sequence that will be rewarded [5, 3, 4]
specific_sequence that will be rewarded [2, 4, 3]
specific_sequence that will be rewarded [5, 1, 3]
specific_sequence that will be rewarded [3, 4, 5]
specific_sequence that will be rewarded [2, 3, 1]
specific_sequence that will be rewarded [3, 5, 1]
specific_sequence that will be rewarded [4, 1, 5]
specific_sequence that will be rewarded [2, 3, 0]
specific_sequence that will be rewarded [0, 5, 4]
specific_sequence that will be rewarded [4, 5, 2]
specific_sequence that will be rewarded [2, 1, 0]
specific_sequence that will be rewarded [2, 1, 5]
specific_sequence that will be rewarded [1, 2, 5]
specific_sequence that will be rewarded [3, 1, 2]
specific_sequence that will be rewarded [2, 1, 3]
specific_sequence that will be rewarded [5, 3, 2]
specific_sequence that will be rewarded [4, 0, 5]
specific_sequence that will be rewarded [2, 0, 1]
specific_sequence that will be rewarded [5, 0, 4]
specific_sequence that will be rewarded [4, 2, 0]
specific_sequence that will be rewarded [3, 5, 0]
specific_sequence that will be rewarded [0, 5, 1]
specific_sequence that will be rewarded [1, 2, 3]
specific_sequence that will be rewarded [3, 1, 4]
specific_sequence that will be rewarded [3, 2, 4]
specific_sequence that will be rewarded [1, 3, 0]
specific_sequence that will be rewarded [0, 4, 5]
specific_sequence that will be rewarded [0, 1, 4]
Number of generated sequences that did not clash with an existing one when it was generated: 0
Total no. of rewarded sequences: 30 Out of 120
[[5 4 0 6 7 3 2 1]
 [4 7 2 6 5 1 0 3]
 [4 7 2 1 6 5 3 0]
 [7 2 5 3 6 0 1 4]
 [4 1 6 2 7 3 0 5]
 [1 6 5 0 2 7 3 4]
 [6 6 6 6 6 6 6 6]
 [7 7 7 7 7 7 7 7]] init_transition_function <class 'int'>
RESET called. curr_state set to: 2
self.augmented_state, len [nan, nan, 2] 3
toy env instantiated with config: {'seed': {'env': 0, 'relevant_state_space': 8, 'relevant_action_space': 8}, 'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 8, 'action_space_size': 8, 'reward_density': 0.25, 'make_denser': False, 'terminal_state_density': 0.25, 'completely_connected': True, 'repeats_in_sequences': False, 'delay': 0, 'sequence_length': 3, 'reward_unit': 1.0, 'generate_random_mdp': True, 'state_space_relevant_indices': range(0, 8), 'action_space_relevant_indices': range(0, 8), 'relevant_state_space_size': 8, 'irrelevant_state_space_size': 0, 'relevant_action_space_size': 8, 'irrelevant_action_space_size': 0, 'is_terminal_state': array([7, 6]), 'relevant_init_state_dist': array([0.16666667, 0.16666667, 0.16666667, 0.16666667, 0.16666667,
       0.16666667, 0.        , 0.        ]), 'transition_function': array([[5, 4, 0, 6, 7, 3, 2, 1],
       [4, 7, 2, 6, 5, 1, 0, 3],
       [4, 7, 2, 1, 6, 5, 3, 0],
       [7, 2, 5, 3, 6, 0, 1, 4],
       [4, 1, 6, 2, 7, 3, 0, 5],
       [1, 6, 5, 0, 2, 7, 3, 4],
       [6, 6, 6, 6, 6, 6, 6, 6],
       [7, 7, 7, 7, 7, 7, 7, 7]], dtype=object)}
[nan, nan, 2] with delay 0
sars', done = 2 6 0.0 3 False 

[nan, 2, 3] with delay 0
sars', done = 3 6 0.0 1 False 

[2, 3, 1] with delay 0
sars', done = 1 2 1.0 2 False 

[3, 1, 2] with delay 0
sars', done = 2 3 1.0 1 False 

[1, 2, 1] with delay 0
sars', done = 1 4 0.0 5 False 

[2, 1, 5] with delay 0
sars', done = 5 2 1.0 5 False 

[1, 5, 5] with delay 0
sars', done = 5 6 0.0 3 False 

[5, 5, 3] with delay 0
sars', done = 3 5 0.0 0 False 

Noise stats for previous episode num.: 1 (total abs. noise in rewards, total abs. noise in transitions, total reward, total noisy transitions, total transitions): 0 0 3.0 0 8
RESET called. curr_state set to: 4
