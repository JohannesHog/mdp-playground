{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dtype    = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "\n",
    "from geomloss import SamplesLoss\n",
    "# from pykeops.torch.cluster import grid_cluster, cluster_ranges_centroids\n",
    "\n",
    "np.random.seed(0)\n",
    "# torch.manual_seed(0)\n",
    "num_points = 10000\n",
    "num_dims = 2\n",
    "# A_i = np.random.rand(num_points, 1) # this imposes some random categorical distribution drawn from ...\n",
    "# A_i = A_i / np.sum(A_i)\n",
    "# A_i = A_i.ravel() ######IMP causing bug otherwise\n",
    "A_i = np.ones((num_points,)) / num_points # this imposes a uniform distribution over sampled points and may be used to see empirically how the loss behaves when we discretise a continuous distribution by approximating it with a categorical distribution.\n",
    "X_i = np.random.rand(num_points, num_dims) - np.ones((num_points, num_dims))/2\n",
    "# B_j = np.random.rand(num_points, 1)\n",
    "# B_j = B_j / np.sum(B_j)\n",
    "# B_j = B_j.ravel()\n",
    "B_j = np.ones((num_points,)) / num_points\n",
    "Y_j = np.random.rand(num_points, num_dims) - np.ones((num_points, num_dims))/2\n",
    "\n",
    "print(A_i.shape, X_i.shape, B_j.shape, Y_j.shape)\n",
    "print(A_i, '\\n', X_i)\n",
    "print(B_j, '\\n', Y_j)\n",
    "\n",
    "#plot variations across num_points, num_dims, p = 1 or 2, blur, diameter, scaling (for speed vs accuracy), also for discretisation of continuous distribution (e.g. sampling leads to categorical distribution)\n",
    "\n",
    "# epsilon schedule goes from diameter**p to blur**p linearly on a logarithmic scale,\n",
    "# with stepsize = p*log(scaling).\n",
    "\n",
    "# for low no. of points(= 10)\n",
    "p = 2. # seems more robust for p = 2 than 1\n",
    "diameter = 1. # seems to be somewhat significant if selected too low (when smaller than blur?) e.g. 0.1: value of loss even goes in the wrong direction i.e. away from the correct value as more outer iterations elapse\n",
    "blur = 0.6561 # seems too low a value (was 0.0 to at least 4 decimal places) will lead to NaN loss; need more iterations if this value is large\n",
    "scaling = 0.7 # Seems robust to too low a value e.g. 0.007\n",
    "# Seems we don't need multiple outer iterations if we have enough inner ones (for low no. of points?)\n",
    "# Seems that at a coarse scale the loss is usually underestimated\n",
    "# When approximating 2 of the same uniform distributions over 2-D space with 2 sampled categorical distributions, we know there true Wasserstein distance is 0, but the sampled one for 10 points = 0.03361376, 20 points = 0.098184645, 100 p = 0.003122977, 1000p = 0.0007166295, 10000p = 7.326214e-05\n",
    "print(\"Epsilon schedule for p = 2:\", [diameter**p] +\n",
    "      [np.exp(e) for e in np.arange(p*np.log(diameter), p*np.log(blur), p*np.log(scaling))]\n",
    "     + [blur**p])\n",
    "\n",
    "p = 1.\n",
    "print(\"Epsilon schedule for p = 1:\", [diameter**p] +\n",
    "      [np.exp(e) for e in np.arange(p*np.log(diameter), p*np.log(blur), p*np.log(scaling))]\n",
    "     + [blur**p])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "scaling, Nits = .5, 9\n",
    "cluster_scale = .1 if not use_cuda else .05\n",
    "i = 3\n",
    "blur = scaling**i\n",
    "diameter = 1.\n",
    "\n",
    "if blur > cluster_scale:\n",
    "    print('Calculating Sinkhorn divergences over coarse clusters. blur, cluster_scale =', blur, cluster_scale)\n",
    "else:\n",
    "    print('Calculating Sinkhorn divergences over actual points. blur, cluster_scale =', blur, cluster_scale)\n",
    "\n",
    "# Create a copy of the data...\n",
    "A_i_torch = torch.from_numpy(A_i).type(dtype)\n",
    "X_i_torch = torch.from_numpy(X_i).contiguous().type(dtype)\n",
    "B_j_torch = torch.from_numpy(B_j).type(dtype)\n",
    "Y_j_torch = torch.from_numpy(Y_j).contiguous().type(dtype)\n",
    "a_i, x_i = A_i_torch.clone(), X_i_torch.clone()\n",
    "b_j, y_j = B_j_torch.clone(), Y_j_torch.clone()\n",
    "\n",
    "# And require grad:\n",
    "a_i.requires_grad = True\n",
    "x_i.requires_grad = True\n",
    "b_j.requires_grad = True\n",
    "\n",
    "# Compute the loss + gradients:\n",
    "Loss_p1 = SamplesLoss(\"sinkhorn\", p=1, blur=blur, diameter=diameter, cluster_scale = cluster_scale,\n",
    "                        scaling=scaling, backend=\"multiscale\", verbose=True)\n",
    "loss_p1 = Loss_p1(a_i, x_i, b_j, y_j)\n",
    "Loss_p2 = SamplesLoss(\"sinkhorn\", p=2, blur=blur, diameter=diameter, cluster_scale = cluster_scale,\n",
    "                        scaling=scaling, backend=\"multiscale\", verbose=True)\n",
    "loss_p2 = Loss_p2(a_i, x_i, b_j, y_j)\n",
    "\n",
    "\n",
    "print(\"Loss_p1 =\", Loss_p1, \"Loss_p2 =\", Loss_p2)\n",
    "print(\"loss_p1 =\", loss_p1.detach().numpy(), \"loss_p2 =\", loss_p2.detach().numpy())\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=((6, 4.5)))\n",
    "\n",
    "size_scale = 1000\n",
    "ax = plt.scatter(X_i[:, 0], X_i[:, 1], s=size_scale * A_i, c='blue')\n",
    "ax = plt.scatter(Y_j[:, 0], Y_j[:, 1], s=size_scale * B_j, c='red')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "i = 4\n",
    "blur = scaling**i\n",
    "\n",
    "if blur > cluster_scale:\n",
    "    print('Calculating Sinkhorn divergences over coarse clusters. blur, cluster_scale =', blur, cluster_scale)\n",
    "else:\n",
    "    print('Calculating Sinkhorn divergences over actual points. blur, cluster_scale =', blur, cluster_scale)\n",
    "Loss_p1 = SamplesLoss(\"sinkhorn\", p=1, blur=blur, diameter=diameter, cluster_scale = cluster_scale,\n",
    "                        scaling=scaling, backend=\"multiscale\", verbose=True)\n",
    "loss_p1 = Loss_p1(a_i, x_i, b_j, y_j)\n",
    "Loss_p2 = SamplesLoss(\"sinkhorn\", p=2, blur=blur, diameter=diameter, cluster_scale = cluster_scale,\n",
    "                        scaling=scaling, backend=\"multiscale\", verbose=True)\n",
    "loss_p2 = Loss_p2(a_i, x_i, b_j, y_j)\n",
    "\n",
    "print(\"loss_p1 =\", loss_p1.detach().numpy(), \"loss_p2 =\", loss_p2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For p = 2\n",
    "plt.figure(figsize=( (12, ((Nits-1)//3 + 1) * 4)))\n",
    "\n",
    "from pykeops.torch.cluster import grid_cluster, cluster_ranges_centroids\n",
    "for i in range(Nits):\n",
    "    blur = scaling**i\n",
    "    Loss = SamplesLoss(\"sinkhorn\", p=2, blur=blur, diameter=diameter, cluster_scale = cluster_scale,\n",
    "                        scaling=scaling, backend=\"multiscale\")#, verbose=True)\n",
    "\n",
    "    # Create a copy of the data...\n",
    "    A_i_torch = torch.from_numpy(A_i).type(dtype)\n",
    "    X_i_torch = torch.from_numpy(X_i).contiguous().type(dtype)\n",
    "    B_j_torch = torch.from_numpy(B_j).type(dtype)\n",
    "    Y_j_torch = torch.from_numpy(Y_j).contiguous().type(dtype)\n",
    "    a_i, x_i = A_i_torch.clone(), X_i_torch.clone()\n",
    "    b_j, y_j = B_j_torch.clone(), Y_j_torch.clone()\n",
    "\n",
    "\n",
    "    # And require grad:\n",
    "    a_i.requires_grad = True\n",
    "    x_i.requires_grad = True\n",
    "    b_j.requires_grad = True\n",
    "\n",
    "    # Compute the loss + gradients:\n",
    "    Loss_xy = Loss(a_i, x_i, b_j, y_j)\n",
    "    [F_i, G_j, dx_i] = grad( Loss_xy, [a_i, b_j, x_i] )\n",
    "\n",
    "#     print(\"F_i.shape, dx_i.shape\", F_i.shape, dx_i.shape)\n",
    "    print(\"Iteration:\", i, \"Loss_xy\", Loss_xy.detach().numpy())\n",
    "    #Â The generalized \"Brenier map\" is (minus) the gradient of the Sinkhorn loss\n",
    "    # with respect to the Wasserstein metric:\n",
    "    BrenierMap = - dx_i / (a_i.view(-1, 1) + 1e-7)\n",
    "\n",
    "    # Compute the coarse measures for display ----------------------------------\n",
    "\n",
    "    x_lab = grid_cluster(x_i, cluster_scale)\n",
    "    _, x_c, a_c = cluster_ranges_centroids(x_i, x_lab, weights=a_i)\n",
    "#     print(\"Clustered array size:\", x_c.size())\n",
    "\n",
    "    y_lab = grid_cluster(y_j, cluster_scale)\n",
    "    _, y_c, b_c = cluster_ranges_centroids(y_j, y_lab, weights=b_j)\n",
    "\n",
    "\n",
    "    # Fancy display: -----------------------------------------------------------\n",
    "\n",
    "    ax = plt.subplot(((Nits-1)//3 + 1) , 3, i+1)\n",
    "#     ax.scatter( [10], [10] )  # shameless hack to prevent a slight change of axis...\n",
    "\n",
    "    #added by me\n",
    "    size_scale = 200\n",
    "    ax.scatter(X_i[:, 0], X_i[:, 1], s=size_scale * A_i, c='blue')\n",
    "    ax.scatter(Y_j[:, 0], Y_j[:, 1], s=size_scale * B_j, c='red')\n",
    "    if blur > cluster_scale:\n",
    "        x_c_ = x_c.detach().cpu().numpy()\n",
    "        a_c_ = x_c.detach().cpu().numpy()\n",
    "        y_c_ = y_c.detach().cpu().numpy()\n",
    "        b_c_ = x_c.detach().cpu().numpy()\n",
    "        ax.scatter(x_c_[:, 0], x_c_[:, 1], s=size_scale * a_c_, c='purple')\n",
    "        ax.scatter(y_c_[:, 0], y_c_[:, 1], s=size_scale * b_c_, c='yellow')\n",
    "\n",
    "    v_ = BrenierMap.detach().cpu().numpy()\n",
    "    x_ = x_i.detach().cpu().numpy()\n",
    "    ax.quiver( x_[:,0], x_[:,1], v_[:,0], v_[:,1], \n",
    "                scale = 1, scale_units=\"xy\",# angles='xy',\n",
    "                color=\"#5CBF3A\", zorder= 3, width= 0.02/3 ) #/ len(x_)\n",
    "#     display_potential(ax, G_j, \"#E2C5C5\")\n",
    "#     display_potential(ax, F_i, \"#C8DFF9\")\n",
    "\n",
    "\n",
    "#     if blur > cluster_scale:\n",
    "#         display_samples(ax, y_j, b_j, [(.55,.55,.95, .2)])\n",
    "#         display_samples(ax, x_i, a_i, [(.95,.55,.55, .2)], v = BrenierMap)\n",
    "#         display_samples(ax, y_c, b_c, [(.55,.55,.95)])\n",
    "#         display_samples(ax, x_c, a_c, [(.95,.55,.55)])\n",
    "\n",
    "#     else:\n",
    "#     display_samples(ax, y_j, b_j, [(.55,.55,.95)])\n",
    "#     display_samples(ax, x_i, a_i, [(.95,.55,.55)])#, v = BrenierMap)\n",
    "\n",
    "\n",
    "    ax.set_title(\"iteration {}, blur = {:.3f}\".format(i+1, blur))\n",
    "\n",
    "#     ax.set_xticks([0, 1]) ; ax.set_yticks([0, 1])\n",
    "    ax.axis([-1, 1, -1, 1]) ; ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For p = 1\n",
    "plt.figure(figsize=( (12, ((Nits-1)//3 + 1) * 4)))\n",
    "\n",
    "from pykeops.torch.cluster import grid_cluster, cluster_ranges_centroids\n",
    "for i in range(Nits):\n",
    "    blur = scaling**i\n",
    "    Loss = SamplesLoss(\"sinkhorn\", p=1, blur=blur, diameter=1., cluster_scale = cluster_scale,\n",
    "                        scaling=scaling, backend=\"multiscale\")\n",
    "\n",
    "    # Create a copy of the data...\n",
    "    A_i_torch = torch.from_numpy(A_i).type(dtype)\n",
    "    X_i_torch = torch.from_numpy(X_i).contiguous().type(dtype)\n",
    "    B_j_torch = torch.from_numpy(B_j).type(dtype)\n",
    "    Y_j_torch = torch.from_numpy(Y_j).contiguous().type(dtype)\n",
    "    a_i, x_i = A_i_torch.clone(), X_i_torch.clone()\n",
    "    b_j, y_j = B_j_torch.clone(), Y_j_torch.clone()\n",
    "\n",
    "\n",
    "    # And require grad:\n",
    "    a_i.requires_grad = True\n",
    "    x_i.requires_grad = True\n",
    "    b_j.requires_grad = True\n",
    "\n",
    "    # Compute the loss + gradients:\n",
    "    Loss_xy = Loss(a_i, x_i, b_j, y_j)\n",
    "    [F_i, G_j, dx_i] = grad( Loss_xy, [a_i, b_j, x_i] )\n",
    "\n",
    "#     print(\"F_i.shape, dx_i.shape\", F_i.shape, dx_i.shape)\n",
    "    print(\"Iteration:\", i, \"Loss_xy\", Loss_xy.detach().numpy())\n",
    "    #Â The generalized \"Brenier map\" is (minus) the gradient of the Sinkhorn loss\n",
    "    # with respect to the Wasserstein metric:\n",
    "    BrenierMap = - dx_i / (a_i.view(-1,1) + 1e-7)\n",
    "\n",
    "    # Compute the coarse measures for display ----------------------------------\n",
    "\n",
    "    x_lab = grid_cluster(x_i, cluster_scale)\n",
    "    _, x_c, a_c = cluster_ranges_centroids(x_i, x_lab, weights=a_i)\n",
    "    print(\"Clustered array size:\", x_c.size())\n",
    "\n",
    "    y_lab = grid_cluster(y_j, cluster_scale)\n",
    "    _, y_c, b_c = cluster_ranges_centroids(y_j, y_lab, weights=b_j)\n",
    "\n",
    "\n",
    "    # Fancy display: -----------------------------------------------------------\n",
    "\n",
    "    ax = plt.subplot(((Nits-1)//3 + 1) , 3, i+1)\n",
    "#     ax.scatter( [10], [10] )  # shameless hack to prevent a slight change of axis...\n",
    "\n",
    "    #added by me\n",
    "    size_scale = 200\n",
    "    ax.scatter(X_i[:, 0], X_i[:, 1], s=size_scale * A_i, c='blue')\n",
    "    ax.scatter(Y_j[:, 0], Y_j[:, 1], s=size_scale * B_j, c='red')\n",
    "    if blur > cluster_scale:\n",
    "        x_c_ = x_c.detach().cpu().numpy()\n",
    "        a_c_ = x_c.detach().cpu().numpy()\n",
    "        y_c_ = y_c.detach().cpu().numpy()\n",
    "        b_c_ = x_c.detach().cpu().numpy()\n",
    "        ax.scatter(x_c_[:, 0], x_c_[:, 1], s=size_scale * a_c_, c='purple')\n",
    "        ax.scatter(y_c_[:, 0], y_c_[:, 1], s=size_scale * b_c_, c='yellow')\n",
    "\n",
    "    v_ = BrenierMap.detach().cpu().numpy()\n",
    "    x_ = x_i.detach().cpu().numpy()\n",
    "    ax.quiver( x_[:,0], x_[:,1], v_[:,0], v_[:,1], \n",
    "                scale = 1, scale_units=\"xy\",# angles='xy',\n",
    "                color=\"#5CBF3A\", zorder= 3, width= 0.02/3 ) #/ len(x_)\n",
    "#     display_potential(ax, G_j, \"#E2C5C5\")\n",
    "#     display_potential(ax, F_i, \"#C8DFF9\")\n",
    "\n",
    "\n",
    "#     if blur > cluster_scale:\n",
    "#         display_samples(ax, y_j, b_j, [(.55,.55,.95, .2)])\n",
    "#         display_samples(ax, x_i, a_i, [(.95,.55,.55, .2)], v = BrenierMap)\n",
    "#         display_samples(ax, y_c, b_c, [(.55,.55,.95)])\n",
    "#         display_samples(ax, x_c, a_c, [(.95,.55,.55)])\n",
    "\n",
    "#     else:\n",
    "#     display_samples(ax, y_j, b_j, [(.55,.55,.95)])\n",
    "#     display_samples(ax, x_i, a_i, [(.95,.55,.55)])#, v = BrenierMap)\n",
    "\n",
    "\n",
    "    ax.set_title(\"iteration {}, blur = {:.3f}\".format(i+1, blur))\n",
    "\n",
    "#     ax.set_xticks([0, 1]) ; ax.set_yticks([0, 1])\n",
    "    ax.axis([-1, 1, -1, 1]) ; ax.set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied code from above to see empirical sample complexities of sampling and approximating.\n",
    "for num_points in [10, 20, 100, 1000]:\n",
    "    all_losses = []\n",
    "    for seed_ in range(1000):\n",
    "        np.random.seed(seed_)\n",
    "        num_dims = 2\n",
    "        A_i = np.ones((num_points,)) / num_points\n",
    "        X_i = np.random.rand(num_points, num_dims) - np.ones((num_points, num_dims))/2\n",
    "        B_j = np.ones((num_points,)) / num_points\n",
    "        Y_j = np.random.rand(num_points, num_dims) - np.ones((num_points, num_dims))/2\n",
    "\n",
    "        for i in range(Nits):\n",
    "            blur = scaling**i\n",
    "            Loss = SamplesLoss(\"sinkhorn\", p=2, blur=blur, diameter=diameter, cluster_scale = cluster_scale,\n",
    "                                scaling=scaling, backend=\"multiscale\")#, verbose=True)\n",
    "\n",
    "            # Create a copy of the data...\n",
    "            A_i_torch = torch.from_numpy(A_i).type(dtype)\n",
    "            X_i_torch = torch.from_numpy(X_i).contiguous().type(dtype)\n",
    "            B_j_torch = torch.from_numpy(B_j).type(dtype)\n",
    "            Y_j_torch = torch.from_numpy(Y_j).contiguous().type(dtype)\n",
    "            a_i, x_i = A_i_torch.clone(), X_i_torch.clone()\n",
    "            b_j, y_j = B_j_torch.clone(), Y_j_torch.clone()\n",
    "\n",
    "\n",
    "            # Compute the loss + gradients:\n",
    "            Loss_xy = Loss(a_i, x_i, b_j, y_j)\n",
    "#             [F_i, G_j, dx_i] = grad( Loss_xy, [a_i, b_j, x_i] )\n",
    "\n",
    "            #Â The generalized \"Brenier map\" is (minus) the gradient of the Sinkhorn loss\n",
    "            # with respect to the Wasserstein metric:\n",
    "#             BrenierMap = - dx_i / (a_i.view(-1, 1) + 1e-7)\n",
    "\n",
    "            # Compute the coarse measures for display ----------------------------------\n",
    "\n",
    "#             x_lab = grid_cluster(x_i, cluster_scale)\n",
    "#             _, x_c, a_c = cluster_ranges_centroids(x_i, x_lab, weights=a_i)\n",
    "#         #     print(\"Clustered array size:\", x_c.size())\n",
    "\n",
    "#             y_lab = grid_cluster(y_j, cluster_scale)\n",
    "#             _, y_c, b_c = cluster_ranges_centroids(y_j, y_lab, weights=b_j)\n",
    "\n",
    "        Loss_xy_ = Loss_xy.detach().numpy()\n",
    "#         print(\"Seed:\", seed_, \"num_points:\", num_points, \"Iteration:\", i, \"Loss_xy\", Loss_xy_)\n",
    "        all_losses.append(Loss_xy_)\n",
    "#     plt.xscale('log')\n",
    "    plt.hist(all_losses, bins=100, range=(0.0, 0.2))#, log=True)\n",
    "    plt.title(\"num_points: \" + str(num_points))\n",
    "    plt.show()\n",
    "    print(\"Mean:\", np.mean(all_losses), \"Median:\", np.median(all_losses))\n",
    "# For 10, 20, 100, 1000 points over 100 seeds:\n",
    "# Mean: 0.038914967 Median: 0.037568074\n",
    "# Mean: 0.025193183 Median: 0.021923613\n",
    "# Mean: 0.005761823 Median: 0.0054472107\n",
    "# Mean: 0.00069120317 Median: 0.00065572176\n",
    "# Over 1000 seeds:\n",
    "# Mean: 0.04238657 Median: 0.03862535\n",
    "# Mean: 0.023622321 Median: 0.021923613\n",
    "# Mean: 0.00586633 Median: 0.005333068\n",
    "# Mean: 0.00069400994 Median: 0.0006544423"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
