{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/rajanr/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.agents.trainer import Trainer, with_common_config\n",
    "from ray.rllib.utils.annotations import override\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yapf: disable\n",
    "# __sphinx_doc_begin__\n",
    "class RandomAgent(Trainer):\n",
    "    \"\"\"Policy that takes random actions and never learns.\"\"\"\n",
    "\n",
    "    _name = \"RandomAgent\"\n",
    "    _default_config = with_common_config({\n",
    "        \"rollouts_per_iteration\": 10,\n",
    "    })\n",
    "\n",
    "    @override(Trainer)\n",
    "    def _init(self, config, env_creator):\n",
    "        self.env = env_creator(config[\"env_config\"])\n",
    "\n",
    "    @override(Trainer)\n",
    "    def _train(self):\n",
    "        rewards = []\n",
    "        steps = 0\n",
    "        for _ in range(self.config[\"rollouts_per_iteration\"]):\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            reward = 0.0\n",
    "            while not done:\n",
    "                action = self.env.action_space.sample()\n",
    "                obs, r, done, info = self.env.step(action)\n",
    "\n",
    "                reward += r\n",
    "                steps += 1\n",
    "            rewards.append(reward)\n",
    "        return {\n",
    "            \"episode_reward_mean\": np.mean(rewards),\n",
    "            \"timesteps_this_iter\": steps,\n",
    "        }\n",
    "\n",
    "class VIAgent(Trainer):\n",
    "    \"\"\"Value Iteration.\n",
    "    #TODO Make it Generalized PI.\n",
    "    \"\"\"\n",
    "\n",
    "    _name = \"VIAgent\"\n",
    "    _default_config = with_common_config({\n",
    "        \"tolerance\": 0.01,\n",
    "        \"discount_factor\": 0.5,\n",
    "        \"rollouts_per_iteration\": 10,\n",
    "        \"episode_length\": 200,\n",
    "        # \"lr\": 0.5\n",
    "    })\n",
    "\n",
    "    @override(Trainer)\n",
    "    def _init(self, config, env_creator):\n",
    "        self.env = env_creator(config[\"env_config\"])\n",
    "        self.V = np.zeros(self.env.observation_space.n)\n",
    "        self.policy = np.zeros(self.env.observation_space.n, dtype=int)\n",
    "        self.policy[:] = -1 #IMP # To avoid initing it to a value within action_space range\n",
    "\n",
    "    @override(Trainer)\n",
    "    def _train(self):\n",
    "        max_diff = np.inf # Maybe keep a state variable so that we don't need to update every train iteration??\n",
    "        state_space_size = self.env.observation_space.n\n",
    "        gamma = self.config[\"discount_factor\"]\n",
    "        total_iterations = 0\n",
    "        while max_diff > self.config[\"tolerance\"]:\n",
    "            total_iterations += 1\n",
    "            for s in range(state_space_size):\n",
    "                # print(\"self.V[:]\", s, max_diff, self.V, [self.env.R(s, a) for a in range(self.env.action_space.n)], self.policy[s])\n",
    "                self.V_old = self.V.copy() # Is this asynchronous? V_old should be held constant for all states in the for loop?\n",
    "                # print([self.env.R(s, a) for a in range(self.env.action_space.n)], [gamma * self.V[self.env.P(s, a)] for a in range(self.env.action_space.n)], [self.env.R(s, a) + gamma * self.V[self.env.P(s, a)] for a in range(self.env.action_space.n)])\n",
    "                self.policy[s] = np.argmax([self.env.R(s, a) + gamma * self.V[self.env.P(s, a)] for a in range(self.env.action_space.n)])\n",
    "                self.V[s] = np.max([self.env.R(s, a) + gamma * self.V[self.env.P(s, a)] for a in range(self.env.action_space.n)]) # We want R to be a callable function, so I guess we have to keep a for loop here??\n",
    "                # print(\"self.V, self.V_old, self.policy[s]\", self.V, self.V_old, self.policy[s], self.env.P(s, self.policy[s]))\n",
    "\n",
    "                max_diff = np.max(np.absolute(self.V_old - self.V))\n",
    "        # import time\n",
    "        # time.sleep(2)\n",
    "#         for s in range(state_space_size):\n",
    "#             print(\"FINAL self.V[:]\", s, max_diff, self.V[:], [self.env.R(s, a) for a in range(self.env.action_space.n)])\n",
    "\n",
    "        print(\"Total iterations:\", total_iterations)\n",
    "        rewards = []\n",
    "        steps = 0\n",
    "        for _ in range(self.config[\"rollouts_per_iteration\"]):\n",
    "            obs = self.env.reset()\n",
    "            done = False\n",
    "            reward = 0.0\n",
    "            for _ in range(self.config[\"episode_length\"]):\n",
    "                action = self.policy[obs]\n",
    "                obs, r, done, info = self.env.step(action)\n",
    "\n",
    "                reward += r\n",
    "                steps += 1\n",
    "            rewards.append(reward)\n",
    "        return {\n",
    "            \"episode_reward_mean\": np.mean(rewards),\n",
    "            \"timesteps_this_iter\": steps,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-17 21:51:40,487\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-08-17_21-51-40_487449_24872/logs.\n",
      "2019-08-17 21:51:40,608\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:28790 to respond...\n",
      "2019-08-17 21:51:40,726\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:56305 to respond...\n",
      "2019-08-17 21:51:40,729\tINFO services.py:809 -- Starting Redis shard with 6.72 GB max memory.\n",
      "2019-08-17 21:51:40,754\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-08-17_21-51-40_487449_24872/logs.\n",
      "2019-08-17 21:51:40,756\tWARNING services.py:1330 -- WARNING: The default object store size of 10.08 GB will use more than 50% of the available memory on this node (17.17 GB). Consider setting the object store memory manually to a smaller size to avoid memory contention with other applications.\n",
      "2019-08-17 21:51:40,757\tINFO services.py:1475 -- Starting the Plasma object store with 10.08 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.5.150.104',\n",
       " 'redis_address': '10.5.150.104:28790',\n",
       " 'object_store_address': '/tmp/ray/session_2019-08-17_21-51-40_487449_24872/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-08-17_21-51-40_487449_24872/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2019-08-17_21-51-40_487449_24872'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.utils.seed import seed as rllib_seed\n",
    "import rl_toy\n",
    "from rl_toy.envs import RLToyEnv\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"RLToy-v0\", lambda config: RLToyEnv(config))\n",
    "\n",
    "# rllib_seed(0, 0, 0)\n",
    "ray.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-08-18 00:41:15,614\tINFO trial_runner.py:176 -- Starting a new experiment.\n",
      "2019-08-18 00:41:15,621\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.0/33.6 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.0/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_RLToy-v0_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m WARNING:tensorflow:From /home/rajanr/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Inited terminal states to: [15 14 13 12] total 4\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m specific_sequence that will be rewarded [1]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m specific_sequence that will be rewarded [5]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m specific_sequence that will be rewarded [0]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Total no. of sequences reward: 3 Out of 12\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m [[12 3 8 8 6 0 7 1 13 11 2 6 4 1 10 4]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [1 7 14 13 12 8 2 13 14 13 2 12 13 6 14 5]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [14 13 5 8 5 5 14 1 4 8 4 0 5 7 10 12]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [8 4 13 1 0 2 9 8 15 9 4 15 7 10 2 6]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [12 8 10 2 4 5 6 14 3 6 6 8 5 6 13 0]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [10 14 14 4 13 10 14 14 15 15 13 12 5 0 12 0]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [11 11 12 12 0 0 10 5 15 3 3 2 14 4 2 9]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [4 9 10 5 12 13 12 2 13 9 13 7 7 11 15 3]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [4 3 15 4 2 11 7 15 9 9 13 8 7 9 8 5]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [15 7 14 14 6 13 10 2 9 13 12 0 11 10 12 3]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [3 8 9 0 1 13 13 15 9 6 10 6 0 0 6 4]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [5 6 6 1 4 12 5 5 13 10 10 1 10 0 7 14]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [0 9 0 10 14 7 1 13 9 3 6 12 3 1 5 10]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [5 1 4 15 3 9 0 2 7 1 9 11 14 3 10 0]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [8 1 4 5 14 8 12 1 13 5 11 15 11 5 14 14]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  [5 13 9 0 5 10 4 14 5 1 6 15 7 13 6 12]] init_transition_function <class 'int'>\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m self.init_state_dist: [0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m  0.         0.         0.         0.        ]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m self.augmented_state [nan, nan, nan, nan, nan, nan, 6]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m self.config['is_terminal_state']: [15 14 13 12]\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:20,361\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:20.361816: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:20.383093: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3192695000 Hz\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:20.383324: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55acb742bcc0 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:20.383341: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m WARNING:tensorflow:From /home/rajanr/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Colocations handled automatically by placer.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m WARNING:tensorflow:From /home/rajanr/anaconda3/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Use keras.layers.dense instead.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m WARNING:tensorflow:From /home/rajanr/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m WARNING:tensorflow:From /home/rajanr/anaconda3/envs/py36/lib/python3.6/site-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Use tf.random.categorical instead.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:20,522\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   'q_values': <tf.Tensor 'default_policy/q_values:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m WARNING:tensorflow:From /home/rajanr/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m Use tf.cast instead.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:20,995\tINFO rollout_worker.py:742 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f23b21c8208>}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:20,995\tINFO rollout_worker.py:743 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.OneHotPreprocessor object at 0x7f23b21abe80>}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:20,995\tINFO rollout_worker.py:356 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7f23b21abcc0>}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:21,016\tINFO rollout_worker.py:451 -- Generating sample batch of size 4\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:21,016\tINFO sampler.py:304 -- Raw obs from env: {0: {'agent0': 0}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:21,016\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:21,016\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((16,), dtype=float64, min=0.0, max=1.0, mean=0.062)\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:21,016\tINFO sampler.py:407 -- Filtered obs: np.ndarray((16,), dtype=float64, min=0.0, max=1.0, mean=0.062)\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:21,017\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                   'obs': np.ndarray((16,), dtype=float64, min=0.0, max=1.0, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:21,017\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:21,036\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=11.0, max=11.0, mean=11.0),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                       { 'q_values': np.ndarray((1, 16), dtype=float32, min=-0.374, max=0.329, mean=0.014)})}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:21,039\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((3,), dtype=int64, min=3.0, max=15.0, mean=9.667),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'agent_index': np.ndarray((3,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'dones': np.ndarray((3,), dtype=bool, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'eps_id': np.ndarray((3,), dtype=int64, min=1244727293.0, max=1244727293.0, mean=1244727293.0),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'infos': np.ndarray((3,), dtype=object, head={'curr_state': 6}),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'new_obs': np.ndarray((3, 16), dtype=float32, min=0.0, max=1.0, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'obs': np.ndarray((3, 16), dtype=float32, min=0.0, max=1.0, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'prev_actions': np.ndarray((3,), dtype=int64, min=0.0, max=15.0, mean=8.667),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'prev_rewards': np.ndarray((3,), dtype=float32, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'q_values': np.ndarray((3, 16), dtype=float32, min=-0.374, max=0.398, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'rewards': np.ndarray((3,), dtype=int64, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         't': np.ndarray((3,), dtype=int64, min=0.0, max=2.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'unroll_id': np.ndarray((3,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                         'weights': np.ndarray((3,), dtype=int64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:21,040\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m { 'data': { 'actions': np.ndarray((4,), dtype=int64, min=3.0, max=15.0, mean=10.25),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'agent_index': np.ndarray((4,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'dones': np.ndarray((4,), dtype=bool, min=0.0, max=1.0, mean=0.25),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'eps_id': np.ndarray((4,), dtype=int64, min=1168405665.0, max=1244727293.0, mean=1225646886.0),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'infos': np.ndarray((4,), dtype=object, head={'curr_state': 6}),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'new_obs': np.ndarray((4, 16), dtype=float32, min=0.0, max=1.0, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'obs': np.ndarray((4, 16), dtype=float32, min=0.0, max=1.0, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'prev_actions': np.ndarray((4,), dtype=int64, min=0.0, max=15.0, mean=6.5),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'prev_rewards': np.ndarray((4,), dtype=float32, min=0.0, max=1.0, mean=0.25),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'q_values': np.ndarray((4, 16), dtype=float32, min=-0.374, max=0.399, mean=0.023),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'rewards': np.ndarray((4,), dtype=int64, min=0.0, max=1.0, mean=0.5),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             't': np.ndarray((4,), dtype=int64, min=0.0, max=2.0, mean=0.75),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'unroll_id': np.ndarray((4,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m             'weights': np.ndarray((4,), dtype=int64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_RLToy-v0_0:\n",
      "  callback_ok: true\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-18_00-41-22\n",
      "  done: false\n",
      "  episode_len_mean: 3.475836431226766\n",
      "  episode_reward_max: 5.0\n",
      "  episode_reward_mean: 0.895910780669145\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 269\n",
      "  episodes_total: 269\n",
      "  experiment_id: 52abb92d04254715928ae3620e2b94a8\n",
      "  hostname: mlstaff04\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 1.0\n",
      "    min_exploration: 1.0\n",
      "    num_steps_sampled: 936\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 11\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 3.46\n",
      "    update_time_ms: 0.001\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.5.150.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 515\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.059969524436470294\n",
      "    mean_inference_ms: 0.5707908719937089\n",
      "    mean_processing_ms: 0.21816139669021387\n",
      "  time_since_restore: 1.0066626071929932\n",
      "  time_this_iter_s: 1.0066626071929932\n",
      "  time_total_s: 1.0066626071929932\n",
      "  timestamp: 1566081682\n",
      "  timesteps_since_restore: 936\n",
      "  timesteps_this_iter: 936\n",
      "  timesteps_total: 936\n",
      "  training_iteration: 1\n",
      "  trial_id: 1f264556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.2/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_RLToy-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=515], 1 s, 1 iter, 936 ts, 0.896 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 269 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 5.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 0.895910780669145, 'episode_len_mean': 3.475836431226766, 'episodes_this_iter': 269, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.059969524436470294, 'mean_processing_ms': 0.21816139669021387, 'mean_inference_ms': 0.5707908719937089}, 'off_policy_estimator': {}, 'info': {'min_exploration': 1.0, 'max_exploration': 1.0, 'num_target_updates': 11, 'num_steps_trained': 0, 'num_steps_sampled': 936, 'sample_time_ms': 3.46, 'replay_time_ms': nan, 'grad_time_ms': nan, 'update_time_ms': 0.001, 'opt_peak_throughput': 0.0, 'opt_samples': nan, 'learner': {}}, 'timesteps_this_iter': 936, 'done': False, 'timesteps_total': 936, 'episodes_total': 269, 'training_iteration': 1, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-22', 'timestamp': 1566081682, 'time_this_iter_s': 1.0066626071929932, 'time_total_s': 1.0066626071929932, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 1.0066626071929932, 'timesteps_since_restore': 936, 'iterations_since_restore': 1}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m /home/rajanr/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   out=out, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m /home/rajanr/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 295 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 4.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 0.5627118644067797, 'episode_len_mean': 3.077966101694915, 'episodes_this_iter': 295, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06178956690842541, 'mean_processing_ms': 0.22754604577372067, 'mean_inference_ms': 0.5794628525814067}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.53668, 'max_exploration': 0.53668, 'num_target_updates': 21, 'num_steps_trained': 0, 'num_steps_sampled': 1844, 'sample_time_ms': 3.747, 'replay_time_ms': nan, 'grad_time_ms': nan, 'update_time_ms': 0.002, 'opt_peak_throughput': 0.0, 'opt_samples': nan, 'learner': {}}, 'timesteps_this_iter': 908, 'done': False, 'timesteps_total': 1844, 'episodes_total': 564, 'training_iteration': 2, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-23', 'timestamp': 1566081683, 'time_this_iter_s': 1.0045976638793945, 'time_total_s': 2.0112602710723877, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 2.0112602710723877, 'timesteps_since_restore': 1844, 'iterations_since_restore': 2}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:23,250\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m { 'count': 32,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((32,), dtype=int64, min=0.0, max=15.0, mean=7.625),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                                     'batch_indexes': np.ndarray((32,), dtype=int64, min=-1.0, max=-1.0, mean=-1.0),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                                     'dones': np.ndarray((32,), dtype=bool, min=0.0, max=1.0, mean=0.344),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                                     'new_obs': np.ndarray((32, 16), dtype=float32, min=0.0, max=1.0, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                                     'obs': np.ndarray((32, 16), dtype=float32, min=0.0, max=1.0, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                                     'rewards': np.ndarray((32,), dtype=int64, min=0.0, max=1.0, mean=0.312),\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                                     'weights': np.ndarray((32,), dtype=int64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:23,250\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:23,250\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:23,251\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:23,251\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:23,251\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/kernel:0' shape=(16, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:23,251\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:23,251\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:23,251\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m 2019-08-18 00:41:23,339\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 6.25000029685907e-05,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                          'max_q': 0.49059683,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                          'mean_q': 0.14500237,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                          'mean_td_error': -0.40883553,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                          'min_q': -0.21656229,\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m                       'td_error': np.ndarray((32,), dtype=float32, min=-1.543, max=0.398, mean=-0.409)}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 104 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 3.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 0.20192307692307693, 'episode_len_mean': 4.269230769230769, 'episodes_this_iter': 104, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06163206542994734, 'mean_processing_ms': 0.22095677800866684, 'mean_inference_ms': 0.5710154403874902}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.08721999999999996, 'max_exploration': 0.08721999999999996, 'num_target_updates': 27, 'num_steps_trained': 2784, 'num_steps_sampled': 2348, 'sample_time_ms': 3.461, 'replay_time_ms': 2.548, 'grad_time_ms': 2.269, 'update_time_ms': 0.003, 'opt_peak_throughput': 14106.058, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 0.28982365, 'min_q': -0.08598224, 'max_q': 1.0100355, 'mean_td_error': -0.28080362, 'model': {}}}}, 'timesteps_this_iter': 504, 'done': False, 'timesteps_total': 2348, 'episodes_total': 668, 'training_iteration': 3, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-24', 'timestamp': 1566081684, 'time_this_iter_s': 1.009376049041748, 'time_total_s': 3.0206363201141357, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 3.0206363201141357, 'timesteps_since_restore': 2348, 'iterations_since_restore': 3}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 50 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 3.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 0.71, 'episode_len_mean': 7.8, 'episodes_this_iter': 50, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06167557679553973, 'mean_processing_ms': 0.21702204265660222, 'mean_inference_ms': 0.5702078057393867}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 33, 'num_steps_trained': 6400, 'num_steps_sampled': 2800, 'sample_time_ms': 4.616, 'replay_time_ms': 2.502, 'grad_time_ms': 2.419, 'update_time_ms': 0.003, 'opt_peak_throughput': 13227.331, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 0.5450195, 'min_q': -0.122599974, 'max_q': 1.4525111, 'mean_td_error': -0.27274776, 'model': {}}}}, 'timesteps_this_iter': 452, 'done': False, 'timesteps_total': 2800, 'episodes_total': 718, 'training_iteration': 4, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-25', 'timestamp': 1566081685, 'time_this_iter_s': 1.0086803436279297, 'time_total_s': 4.029316663742065, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 4.029316663742065, 'timesteps_since_restore': 2800, 'iterations_since_restore': 4}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 72.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 1.43, 'episode_len_mean': 9.19, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061700652210101535, 'mean_processing_ms': 0.21688648400238147, 'mean_inference_ms': 0.5703839219548776}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 37, 'num_steps_trained': 9504, 'num_steps_sampled': 3188, 'sample_time_ms': 4.701, 'replay_time_ms': 2.826, 'grad_time_ms': 2.601, 'update_time_ms': 0.003, 'opt_peak_throughput': 12301.477, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 1.0839771, 'min_q': 0.021517882, 'max_q': 3.1276402, 'mean_td_error': -0.27623194, 'model': {}}}}, 'timesteps_this_iter': 388, 'done': False, 'timesteps_total': 3188, 'episodes_total': 719, 'training_iteration': 5, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-26', 'timestamp': 1566081686, 'time_this_iter_s': 1.009138822555542, 'time_total_s': 5.038455486297607, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 5.038455486297607, 'timesteps_since_restore': 3188, 'iterations_since_restore': 5}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_RLToy-v0_0:\n",
      "  callback_ok: true\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-18_00-41-27\n",
      "  done: false\n",
      "  episode_len_mean: 13.43\n",
      "  episode_reward_max: 213.0\n",
      "  episode_reward_mean: 3.56\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 720\n",
      "  experiment_id: 52abb92d04254715928ae3620e2b94a8\n",
      "  hostname: mlstaff04\n",
      "  info:\n",
      "    grad_time_ms: 2.489\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 6.25000029685907e-05\n",
      "        max_q: 5.943103790283203\n",
      "        mean_q: 3.706404685974121\n",
      "        mean_td_error: -0.7341693043708801\n",
      "        min_q: 0.9697749614715576\n",
      "        model: {}\n",
      "    max_exploration: 0.010000000000000009\n",
      "    min_exploration: 0.010000000000000009\n",
      "    num_steps_sampled: 3628\n",
      "    num_steps_trained: 13024\n",
      "    num_target_updates: 43\n",
      "    opt_peak_throughput: 12856.843\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.877\n",
      "    sample_time_ms: 3.916\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.5.150.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 515\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06172313764318348\n",
      "    mean_inference_ms: 0.5705431277497249\n",
      "    mean_processing_ms: 0.216670585651768\n",
      "  time_since_restore: 6.041963338851929\n",
      "  time_this_iter_s: 1.0035078525543213\n",
      "  time_total_s: 6.041963338851929\n",
      "  timestamp: 1566081687\n",
      "  timesteps_since_restore: 3628\n",
      "  timesteps_this_iter: 440\n",
      "  timesteps_total: 3628\n",
      "  training_iteration: 6\n",
      "  trial_id: 1f264556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.2/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_RLToy-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=515], 6 s, 6 iter, 3628 ts, 3.56 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 213.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 3.56, 'episode_len_mean': 13.43, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06172313764318348, 'mean_processing_ms': 0.216670585651768, 'mean_inference_ms': 0.5705431277497249}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 43, 'num_steps_trained': 13024, 'num_steps_sampled': 3628, 'sample_time_ms': 3.916, 'replay_time_ms': 2.877, 'grad_time_ms': 2.489, 'update_time_ms': 0.003, 'opt_peak_throughput': 12856.843, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 3.7064047, 'min_q': 0.96977496, 'max_q': 5.943104, 'mean_td_error': -0.7341693, 'model': {}}}}, 'timesteps_this_iter': 440, 'done': False, 'timesteps_total': 3628, 'episodes_total': 720, 'training_iteration': 6, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-27', 'timestamp': 1566081687, 'time_this_iter_s': 1.0035078525543213, 'time_total_s': 6.041963338851929, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 6.041963338851929, 'timesteps_since_restore': 3628, 'iterations_since_restore': 6}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 3 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 213.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.88, 'episode_len_mean': 18.13, 'episodes_this_iter': 3, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.0617899345590867, 'mean_processing_ms': 0.21583085858479628, 'mean_inference_ms': 0.5708792901071648}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 48, 'num_steps_trained': 16704, 'num_steps_sampled': 4088, 'sample_time_ms': 3.684, 'replay_time_ms': 2.506, 'grad_time_ms': 2.252, 'update_time_ms': 0.003, 'opt_peak_throughput': 14208.648, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 4.985343, 'min_q': 0.031932376, 'max_q': 9.088097, 'mean_td_error': -0.55795956, 'model': {}}}}, 'timesteps_this_iter': 460, 'done': False, 'timesteps_total': 4088, 'episodes_total': 723, 'training_iteration': 7, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-28', 'timestamp': 1566081688, 'time_this_iter_s': 1.0030245780944824, 'time_total_s': 7.044987916946411, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 7.044987916946411, 'timesteps_since_restore': 4088, 'iterations_since_restore': 7}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 213.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 5.88, 'episode_len_mean': 18.13, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.0617899345590867, 'mean_processing_ms': 0.2158308585847963, 'mean_inference_ms': 0.5708792901071646}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 54, 'num_steps_trained': 20512, 'num_steps_sampled': 4564, 'sample_time_ms': 3.556, 'replay_time_ms': 2.478, 'grad_time_ms': 2.429, 'update_time_ms': 0.002, 'opt_peak_throughput': 13174.229, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 7.3580704, 'min_q': 1.1780237, 'max_q': 12.075437, 'mean_td_error': -0.41783413, 'model': {}}}}, 'timesteps_this_iter': 476, 'done': False, 'timesteps_total': 4564, 'episodes_total': 723, 'training_iteration': 8, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-29', 'timestamp': 1566081689, 'time_this_iter_s': 1.0074989795684814, 'time_total_s': 8.052486896514893, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 8.052486896514893, 'timesteps_since_restore': 4564, 'iterations_since_restore': 8}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 2 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 437.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 10.51, 'episode_len_mean': 28.32, 'episodes_this_iter': 2, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06182386004420471, 'mean_processing_ms': 0.2150556224392928, 'mean_inference_ms': 0.5708799998714108}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 59, 'num_steps_trained': 24192, 'num_steps_sampled': 5024, 'sample_time_ms': 3.763, 'replay_time_ms': 2.746, 'grad_time_ms': 2.562, 'update_time_ms': 0.003, 'opt_peak_throughput': 12489.901, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 9.490773, 'min_q': 0.59585786, 'max_q': 13.851362, 'mean_td_error': 0.10022502, 'model': {}}}}, 'timesteps_this_iter': 460, 'done': False, 'timesteps_total': 5024, 'episodes_total': 725, 'training_iteration': 9, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-30', 'timestamp': 1566081690, 'time_this_iter_s': 1.0026772022247314, 'time_total_s': 9.055164098739624, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 9.055164098739624, 'timesteps_since_restore': 5024, 'iterations_since_restore': 9}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 437.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 10.51, 'episode_len_mean': 28.32, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061823860044204725, 'mean_processing_ms': 0.21505562243929277, 'mean_inference_ms': 0.5708799998714108}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 65, 'num_steps_trained': 27776, 'num_steps_sampled': 5472, 'sample_time_ms': 3.82, 'replay_time_ms': 2.595, 'grad_time_ms': 2.469, 'update_time_ms': 0.003, 'opt_peak_throughput': 12958.506, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 9.670429, 'min_q': 0.18056229, 'max_q': 15.274121, 'mean_td_error': -1.1980594, 'model': {}}}}, 'timesteps_this_iter': 448, 'done': False, 'timesteps_total': 5472, 'episodes_total': 725, 'training_iteration': 10, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-31', 'timestamp': 1566081691, 'time_this_iter_s': 1.0072815418243408, 'time_total_s': 10.062445640563965, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 10.062445640563965, 'timesteps_since_restore': 5472, 'iterations_since_restore': 10}}\n",
      "Result for DQN_RLToy-v0_0:\n",
      "  callback_ok: true\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-18_00-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 37.66\n",
      "  episode_reward_max: 437.0\n",
      "  episode_reward_mean: 15.14\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 2\n",
      "  episodes_total: 727\n",
      "  experiment_id: 52abb92d04254715928ae3620e2b94a8\n",
      "  hostname: mlstaff04\n",
      "  info:\n",
      "    grad_time_ms: 2.31\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 6.25000029685907e-05\n",
      "        max_q: 16.748088836669922\n",
      "        mean_q: 12.9344482421875\n",
      "        mean_td_error: -0.7323184013366699\n",
      "        min_q: 0.9950675368309021\n",
      "        model: {}\n",
      "    max_exploration: 0.010000000000000009\n",
      "    min_exploration: 0.010000000000000009\n",
      "    num_steps_sampled: 5936\n",
      "    num_steps_trained: 31488\n",
      "    num_target_updates: 70\n",
      "    opt_peak_throughput: 13852.302\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.85\n",
      "    sample_time_ms: 3.724\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 10.5.150.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 515\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06184998771392063\n",
      "    mean_inference_ms: 0.570752925783581\n",
      "    mean_processing_ms: 0.21414294430124323\n",
      "  time_since_restore: 11.068598985671997\n",
      "  time_this_iter_s: 1.0061533451080322\n",
      "  time_total_s: 11.068598985671997\n",
      "  timestamp: 1566081692\n",
      "  timesteps_since_restore: 5936\n",
      "  timesteps_this_iter: 464\n",
      "  timesteps_total: 5936\n",
      "  training_iteration: 11\n",
      "  trial_id: 1f264556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.3/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_RLToy-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=515], 11 s, 11 iter, 5936 ts, 15.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 2 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 437.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 15.14, 'episode_len_mean': 37.66, 'episodes_this_iter': 2, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06184998771392063, 'mean_processing_ms': 0.21414294430124323, 'mean_inference_ms': 0.570752925783581}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 70, 'num_steps_trained': 31488, 'num_steps_sampled': 5936, 'sample_time_ms': 3.724, 'replay_time_ms': 2.85, 'grad_time_ms': 2.31, 'update_time_ms': 0.003, 'opt_peak_throughput': 13852.302, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 12.934448, 'min_q': 0.99506754, 'max_q': 16.748089, 'mean_td_error': -0.7323184, 'model': {}}}}, 'timesteps_this_iter': 464, 'done': False, 'timesteps_total': 5936, 'episodes_total': 727, 'training_iteration': 11, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-32', 'timestamp': 1566081692, 'time_this_iter_s': 1.0061533451080322, 'time_total_s': 11.068598985671997, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 11.068598985671997, 'timesteps_since_restore': 5936, 'iterations_since_restore': 11}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 437.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 16.74, 'episode_len_mean': 40.96, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061862206756178394, 'mean_processing_ms': 0.2136594930965912, 'mean_inference_ms': 0.570670992686581}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 76, 'num_steps_trained': 35136, 'num_steps_sampled': 6392, 'sample_time_ms': 3.362, 'replay_time_ms': 2.471, 'grad_time_ms': 2.073, 'update_time_ms': 0.002, 'opt_peak_throughput': 15434.066, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 17.614458, 'min_q': -0.04294192, 'max_q': 20.80141, 'mean_td_error': -0.5046795, 'model': {}}}}, 'timesteps_this_iter': 456, 'done': False, 'timesteps_total': 6392, 'episodes_total': 728, 'training_iteration': 12, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-33', 'timestamp': 1566081693, 'time_this_iter_s': 1.008082389831543, 'time_total_s': 12.07668137550354, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 12.07668137550354, 'timesteps_since_restore': 6392, 'iterations_since_restore': 12}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 437.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 16.74, 'episode_len_mean': 40.96, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.0618622067561784, 'mean_processing_ms': 0.2136594930965912, 'mean_inference_ms': 0.570670992686581}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 81, 'num_steps_trained': 38528, 'num_steps_sampled': 6816, 'sample_time_ms': 4.889, 'replay_time_ms': 2.956, 'grad_time_ms': 3.46, 'update_time_ms': 0.003, 'opt_peak_throughput': 9247.594, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 18.033817, 'min_q': 0.34936243, 'max_q': 22.45277, 'mean_td_error': -0.63700205, 'model': {}}}}, 'timesteps_this_iter': 424, 'done': False, 'timesteps_total': 6816, 'episodes_total': 728, 'training_iteration': 13, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-34', 'timestamp': 1566081694, 'time_this_iter_s': 1.0086901187896729, 'time_total_s': 13.085371494293213, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 13.085371494293213, 'timesteps_since_restore': 6816, 'iterations_since_restore': 13}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 2 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 442.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 21.56, 'episode_len_mean': 50.83, 'episodes_this_iter': 2, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061893187852741456, 'mean_processing_ms': 0.212632621293418, 'mean_inference_ms': 0.5705420506837384}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 86, 'num_steps_trained': 42144, 'num_steps_sampled': 7268, 'sample_time_ms': 3.631, 'replay_time_ms': 2.803, 'grad_time_ms': 2.082, 'update_time_ms': 0.002, 'opt_peak_throughput': 15369.733, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 20.904346, 'min_q': 1.0548233, 'max_q': 23.380405, 'mean_td_error': -1.034174, 'model': {}}}}, 'timesteps_this_iter': 452, 'done': False, 'timesteps_total': 7268, 'episodes_total': 730, 'training_iteration': 14, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-35', 'timestamp': 1566081695, 'time_this_iter_s': 1.002342939376831, 'time_total_s': 14.087714433670044, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 14.087714433670044, 'timesteps_since_restore': 7268, 'iterations_since_restore': 14}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 442.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 21.56, 'episode_len_mean': 50.83, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06189318785274144, 'mean_processing_ms': 0.21263262129341798, 'mean_inference_ms': 0.5705420506837383}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 91, 'num_steps_trained': 45760, 'num_steps_sampled': 7720, 'sample_time_ms': 3.499, 'replay_time_ms': 2.692, 'grad_time_ms': 2.217, 'update_time_ms': 0.003, 'opt_peak_throughput': 14432.324, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 19.791588, 'min_q': 0.09299664, 'max_q': 27.235966, 'mean_td_error': 0.99430114, 'model': {}}}}, 'timesteps_this_iter': 452, 'done': False, 'timesteps_total': 7720, 'episodes_total': 730, 'training_iteration': 15, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-36', 'timestamp': 1566081696, 'time_this_iter_s': 1.0073738098144531, 'time_total_s': 15.095088243484497, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 15.095088243484497, 'timesteps_since_restore': 7720, 'iterations_since_restore': 15}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_RLToy-v0_0:\n",
      "  callback_ok: true\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-18_00-41-37\n",
      "  done: false\n",
      "  episode_len_mean: 60.52\n",
      "  episode_reward_max: 449.0\n",
      "  episode_reward_mean: 26.04\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 731\n",
      "  experiment_id: 52abb92d04254715928ae3620e2b94a8\n",
      "  hostname: mlstaff04\n",
      "  info:\n",
      "    grad_time_ms: 2.418\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 6.25000029685907e-05\n",
      "        max_q: 29.109338760375977\n",
      "        mean_q: 24.061538696289062\n",
      "        mean_td_error: 0.6820318102836609\n",
      "        min_q: 0.45398035645484924\n",
      "        model: {}\n",
      "    max_exploration: 0.010000000000000009\n",
      "    min_exploration: 0.010000000000000009\n",
      "    num_steps_sampled: 8196\n",
      "    num_steps_trained: 49568\n",
      "    num_target_updates: 97\n",
      "    opt_peak_throughput: 13236.201\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.649\n",
      "    sample_time_ms: 3.753\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 10.5.150.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 515\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06190557469373266\n",
      "    mean_inference_ms: 0.5704239719125748\n",
      "    mean_processing_ms: 0.21208181536513077\n",
      "  time_since_restore: 16.09817099571228\n",
      "  time_this_iter_s: 1.0030827522277832\n",
      "  time_total_s: 16.09817099571228\n",
      "  timestamp: 1566081697\n",
      "  timesteps_since_restore: 8196\n",
      "  timesteps_this_iter: 476\n",
      "  timesteps_total: 8196\n",
      "  training_iteration: 16\n",
      "  trial_id: 1f264556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.3/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_RLToy-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=515], 16 s, 16 iter, 8196 ts, 26 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 449.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 26.04, 'episode_len_mean': 60.52, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06190557469373266, 'mean_processing_ms': 0.21208181536513077, 'mean_inference_ms': 0.5704239719125748}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 97, 'num_steps_trained': 49568, 'num_steps_sampled': 8196, 'sample_time_ms': 3.753, 'replay_time_ms': 2.649, 'grad_time_ms': 2.418, 'update_time_ms': 0.003, 'opt_peak_throughput': 13236.201, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 24.061539, 'min_q': 0.45398036, 'max_q': 29.109339, 'mean_td_error': 0.6820318, 'model': {}}}}, 'timesteps_this_iter': 476, 'done': False, 'timesteps_total': 8196, 'episodes_total': 731, 'training_iteration': 16, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-37', 'timestamp': 1566081697, 'time_this_iter_s': 1.0030827522277832, 'time_total_s': 16.09817099571228, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 16.09817099571228, 'timesteps_since_restore': 8196, 'iterations_since_restore': 16}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 449.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 26.04, 'episode_len_mean': 60.52, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061905574693732666, 'mean_processing_ms': 0.21208181536513082, 'mean_inference_ms': 0.5704239719125748}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 102, 'num_steps_trained': 53056, 'num_steps_sampled': 8632, 'sample_time_ms': 4.111, 'replay_time_ms': 2.962, 'grad_time_ms': 2.502, 'update_time_ms': 0.003, 'opt_peak_throughput': 12792.143, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 26.63927, 'min_q': 9.361272, 'max_q': 28.847013, 'mean_td_error': 0.5000924, 'model': {}}}}, 'timesteps_this_iter': 436, 'done': False, 'timesteps_total': 8632, 'episodes_total': 731, 'training_iteration': 17, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-38', 'timestamp': 1566081698, 'time_this_iter_s': 1.0013468265533447, 'time_total_s': 17.099517822265625, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 17.099517822265625, 'timesteps_since_restore': 8632, 'iterations_since_restore': 17}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 449.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 30.22, 'episode_len_mean': 69.69, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061917190705405495, 'mean_processing_ms': 0.21150380612167707, 'mean_inference_ms': 0.5702711379623148}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 108, 'num_steps_trained': 56896, 'num_steps_sampled': 9112, 'sample_time_ms': 3.53, 'replay_time_ms': 2.75, 'grad_time_ms': 2.211, 'update_time_ms': 0.002, 'opt_peak_throughput': 14473.255, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 27.60534, 'min_q': 1.1748892, 'max_q': 30.918491, 'mean_td_error': -0.59627515, 'model': {}}}}, 'timesteps_this_iter': 480, 'done': False, 'timesteps_total': 9112, 'episodes_total': 732, 'training_iteration': 18, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-39', 'timestamp': 1566081699, 'time_this_iter_s': 1.0015122890472412, 'time_total_s': 18.101030111312866, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 18.101030111312866, 'timesteps_since_restore': 9112, 'iterations_since_restore': 18}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 2 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 449.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 32.1, 'episode_len_mean': 73.89, 'episodes_this_iter': 2, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061935828305406254, 'mean_processing_ms': 0.21031871647869438, 'mean_inference_ms': 0.569907134189682}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 114, 'num_steps_trained': 60672, 'num_steps_sampled': 9584, 'sample_time_ms': 3.615, 'replay_time_ms': 2.639, 'grad_time_ms': 2.547, 'update_time_ms': 0.003, 'opt_peak_throughput': 12564.262, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 27.001823, 'min_q': -0.062567964, 'max_q': 33.964874, 'mean_td_error': -1.9442084, 'model': {}}}}, 'timesteps_this_iter': 472, 'done': False, 'timesteps_total': 9584, 'episodes_total': 734, 'training_iteration': 19, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-40', 'timestamp': 1566081700, 'time_this_iter_s': 1.0023870468139648, 'time_total_s': 19.10341715812683, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 19.10341715812683, 'timesteps_since_restore': 9584, 'iterations_since_restore': 19}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 2 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 449.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 34.04, 'episode_len_mean': 78.57, 'episodes_this_iter': 2, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06195099876704971, 'mean_processing_ms': 0.2091096380469297, 'mean_inference_ms': 0.5694939262916346}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 119, 'num_steps_trained': 64448, 'num_steps_sampled': 10056, 'sample_time_ms': 3.366, 'replay_time_ms': 2.645, 'grad_time_ms': 1.955, 'update_time_ms': 0.002, 'opt_peak_throughput': 16366.618, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 31.767694, 'min_q': 0.5469009, 'max_q': 36.51048, 'mean_td_error': 0.13555562, 'model': {}}}}, 'timesteps_this_iter': 472, 'done': False, 'timesteps_total': 10056, 'episodes_total': 736, 'training_iteration': 20, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-41', 'timestamp': 1566081701, 'time_this_iter_s': 1.0042920112609863, 'time_total_s': 20.107709169387817, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 20.107709169387817, 'timesteps_since_restore': 10056, 'iterations_since_restore': 20}}\n",
      "Result for DQN_RLToy-v0_0:\n",
      "  callback_ok: true\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-18_00-41-42\n",
      "  done: false\n",
      "  episode_len_mean: 78.57\n",
      "  episode_reward_max: 449.0\n",
      "  episode_reward_mean: 34.04\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 736\n",
      "  experiment_id: 52abb92d04254715928ae3620e2b94a8\n",
      "  hostname: mlstaff04\n",
      "  info:\n",
      "    grad_time_ms: 2.249\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 6.25000029685907e-05\n",
      "        max_q: 37.69129943847656\n",
      "        mean_q: 34.83421325683594\n",
      "        mean_td_error: 0.003982484340667725\n",
      "        min_q: 4.215227127075195\n",
      "        model: {}\n",
      "    max_exploration: 0.010000000000000009\n",
      "    min_exploration: 0.010000000000000009\n",
      "    num_steps_sampled: 10528\n",
      "    num_steps_trained: 68224\n",
      "    num_target_updates: 125\n",
      "    opt_peak_throughput: 14231.699\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.556\n",
      "    sample_time_ms: 3.48\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 10.5.150.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 515\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06195099876704969\n",
      "    mean_inference_ms: 0.5694939262916345\n",
      "    mean_processing_ms: 0.2091096380469296\n",
      "  time_since_restore: 21.112524271011353\n",
      "  time_this_iter_s: 1.0048151016235352\n",
      "  time_total_s: 21.112524271011353\n",
      "  timestamp: 1566081702\n",
      "  timesteps_since_restore: 10528\n",
      "  timesteps_this_iter: 472\n",
      "  timesteps_total: 10528\n",
      "  training_iteration: 21\n",
      "  trial_id: 1f264556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.3/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_RLToy-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=515], 21 s, 21 iter, 10528 ts, 34 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 449.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 34.04, 'episode_len_mean': 78.57, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06195099876704969, 'mean_processing_ms': 0.2091096380469296, 'mean_inference_ms': 0.5694939262916345}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 125, 'num_steps_trained': 68224, 'num_steps_sampled': 10528, 'sample_time_ms': 3.48, 'replay_time_ms': 2.556, 'grad_time_ms': 2.249, 'update_time_ms': 0.003, 'opt_peak_throughput': 14231.699, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 34.834213, 'min_q': 4.215227, 'max_q': 37.6913, 'mean_td_error': 0.0039824843, 'model': {}}}}, 'timesteps_this_iter': 472, 'done': False, 'timesteps_total': 10528, 'episodes_total': 736, 'training_iteration': 21, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-42', 'timestamp': 1566081702, 'time_this_iter_s': 1.0048151016235352, 'time_total_s': 21.112524271011353, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 21.112524271011353, 'timesteps_since_restore': 10528, 'iterations_since_restore': 21}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 449.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 34.04, 'episode_len_mean': 78.57, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06195099876704969, 'mean_processing_ms': 0.2091096380469296, 'mean_inference_ms': 0.5694939262916345}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 130, 'num_steps_trained': 71936, 'num_steps_sampled': 10992, 'sample_time_ms': 3.385, 'replay_time_ms': 2.648, 'grad_time_ms': 2.49, 'update_time_ms': 0.003, 'opt_peak_throughput': 12852.042, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 36.5079, 'min_q': 9.792348, 'max_q': 40.168697, 'mean_td_error': -3.1631713, 'model': {}}}}, 'timesteps_this_iter': 464, 'done': False, 'timesteps_total': 10992, 'episodes_total': 736, 'training_iteration': 22, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-43', 'timestamp': 1566081703, 'time_this_iter_s': 1.0011076927185059, 'time_total_s': 22.11363196372986, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 22.11363196372986, 'timesteps_since_restore': 10992, 'iterations_since_restore': 22}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 449.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 34.04, 'episode_len_mean': 78.57, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06195099876704969, 'mean_processing_ms': 0.2091096380469296, 'mean_inference_ms': 0.5694939262916345}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 136, 'num_steps_trained': 75552, 'num_steps_sampled': 11444, 'sample_time_ms': 3.351, 'replay_time_ms': 2.637, 'grad_time_ms': 2.311, 'update_time_ms': 0.002, 'opt_peak_throughput': 13847.015, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 40.367058, 'min_q': 0.8938181, 'max_q': 42.731617, 'mean_td_error': -0.08479857, 'model': {}}}}, 'timesteps_this_iter': 452, 'done': False, 'timesteps_total': 11444, 'episodes_total': 736, 'training_iteration': 23, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-44', 'timestamp': 1566081704, 'time_this_iter_s': 1.0057823657989502, 'time_total_s': 23.11941432952881, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 23.11941432952881, 'timesteps_since_restore': 11444, 'iterations_since_restore': 23}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 449.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 34.04, 'episode_len_mean': 78.57, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06195099876704969, 'mean_processing_ms': 0.2091096380469296, 'mean_inference_ms': 0.5694939262916345}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 141, 'num_steps_trained': 79328, 'num_steps_sampled': 11916, 'sample_time_ms': 3.603, 'replay_time_ms': 2.735, 'grad_time_ms': 2.809, 'update_time_ms': 0.002, 'opt_peak_throughput': 11392.825, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 38.93161, 'min_q': 1.0343115, 'max_q': 45.537735, 'mean_td_error': -0.9322922, 'model': {}}}}, 'timesteps_this_iter': 472, 'done': False, 'timesteps_total': 11916, 'episodes_total': 736, 'training_iteration': 24, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-45', 'timestamp': 1566081705, 'time_this_iter_s': 1.0067102909088135, 'time_total_s': 24.126124620437622, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 24.126124620437622, 'timesteps_since_restore': 11916, 'iterations_since_restore': 24}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 449.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 34.04, 'episode_len_mean': 78.57, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06195099876704969, 'mean_processing_ms': 0.2091096380469296, 'mean_inference_ms': 0.5694939262916345}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 147, 'num_steps_trained': 82944, 'num_steps_sampled': 12368, 'sample_time_ms': 3.476, 'replay_time_ms': 2.662, 'grad_time_ms': 2.132, 'update_time_ms': 0.003, 'opt_peak_throughput': 15011.49, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 42.155544, 'min_q': 0.14306153, 'max_q': 47.079952, 'mean_td_error': 1.1862432, 'model': {}}}}, 'timesteps_this_iter': 452, 'done': False, 'timesteps_total': 12368, 'episodes_total': 736, 'training_iteration': 25, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-46', 'timestamp': 1566081706, 'time_this_iter_s': 1.008178472518921, 'time_total_s': 25.134303092956543, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 25.134303092956543, 'timesteps_since_restore': 12368, 'iterations_since_restore': 25}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_RLToy-v0_0:\n",
      "  callback_ok: true\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-18_00-41-47\n",
      "  done: false\n",
      "  episode_len_mean: 103.77\n",
      "  episode_reward_max: 1211.0\n",
      "  episode_reward_mean: 46.15\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 737\n",
      "  experiment_id: 52abb92d04254715928ae3620e2b94a8\n",
      "  hostname: mlstaff04\n",
      "  info:\n",
      "    grad_time_ms: 2.298\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 6.25000029685907e-05\n",
      "        max_q: 48.27271270751953\n",
      "        mean_q: 43.93482208251953\n",
      "        mean_td_error: -1.295918345451355\n",
      "        min_q: 0.2549872100353241\n",
      "        model: {}\n",
      "    max_exploration: 0.010000000000000009\n",
      "    min_exploration: 0.010000000000000009\n",
      "    num_steps_sampled: 12780\n",
      "    num_steps_trained: 86240\n",
      "    num_target_updates: 152\n",
      "    opt_peak_throughput: 13923.145\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.642\n",
      "    sample_time_ms: 3.986\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 10.5.150.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 515\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.061955663468460195\n",
      "    mean_inference_ms: 0.5692637810498621\n",
      "    mean_processing_ms: 0.2084582271798344\n",
      "  time_since_restore: 26.137765645980835\n",
      "  time_this_iter_s: 1.003462553024292\n",
      "  time_total_s: 26.137765645980835\n",
      "  timestamp: 1566081707\n",
      "  timesteps_since_restore: 12780\n",
      "  timesteps_this_iter: 412\n",
      "  timesteps_total: 12780\n",
      "  training_iteration: 26\n",
      "  trial_id: 1f264556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.2/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_RLToy-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=515], 26 s, 26 iter, 12780 ts, 46.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 46.15, 'episode_len_mean': 103.77, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061955663468460195, 'mean_processing_ms': 0.2084582271798344, 'mean_inference_ms': 0.5692637810498621}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 152, 'num_steps_trained': 86240, 'num_steps_sampled': 12780, 'sample_time_ms': 3.986, 'replay_time_ms': 2.642, 'grad_time_ms': 2.298, 'update_time_ms': 0.003, 'opt_peak_throughput': 13923.145, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 43.934822, 'min_q': 0.2549872, 'max_q': 48.272713, 'mean_td_error': -1.2959183, 'model': {}}}}, 'timesteps_this_iter': 412, 'done': False, 'timesteps_total': 12780, 'episodes_total': 737, 'training_iteration': 26, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-47', 'timestamp': 1566081707, 'time_this_iter_s': 1.003462553024292, 'time_total_s': 26.137765645980835, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 26.137765645980835, 'timesteps_since_restore': 12780, 'iterations_since_restore': 26}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 46.15, 'episode_len_mean': 103.77, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061955663468460195, 'mean_processing_ms': 0.20845822717983442, 'mean_inference_ms': 0.5692637810498621}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 156, 'num_steps_trained': 89312, 'num_steps_sampled': 13164, 'sample_time_ms': 3.524, 'replay_time_ms': 2.61, 'grad_time_ms': 2.179, 'update_time_ms': 0.003, 'opt_peak_throughput': 14687.867, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 43.149612, 'min_q': 1.0956076, 'max_q': 49.05456, 'mean_td_error': 0.0605174, 'model': {}}}}, 'timesteps_this_iter': 384, 'done': False, 'timesteps_total': 13164, 'episodes_total': 737, 'training_iteration': 27, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-48', 'timestamp': 1566081708, 'time_this_iter_s': 1.0020272731781006, 'time_total_s': 27.139792919158936, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 27.139792919158936, 'timesteps_since_restore': 13164, 'iterations_since_restore': 27}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 50.34, 'episode_len_mean': 112.8, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06196631145034155, 'mean_processing_ms': 0.20780366111882106, 'mean_inference_ms': 0.569050887177934}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 162, 'num_steps_trained': 93056, 'num_steps_sampled': 13632, 'sample_time_ms': 3.451, 'replay_time_ms': 2.793, 'grad_time_ms': 2.067, 'update_time_ms': 0.002, 'opt_peak_throughput': 15483.921, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 45.086327, 'min_q': 0.08855906, 'max_q': 51.16265, 'mean_td_error': -0.5517187, 'model': {}}}}, 'timesteps_this_iter': 468, 'done': False, 'timesteps_total': 13632, 'episodes_total': 738, 'training_iteration': 28, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-49', 'timestamp': 1566081709, 'time_this_iter_s': 1.0047922134399414, 'time_total_s': 28.144585132598877, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 28.144585132598877, 'timesteps_since_restore': 13632, 'iterations_since_restore': 28}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 2 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 52.7, 'episode_len_mean': 117.82, 'episodes_this_iter': 2, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06198441708233432, 'mean_processing_ms': 0.2064795453708982, 'mean_inference_ms': 0.5685859700247551}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 168, 'num_steps_trained': 96896, 'num_steps_sampled': 14112, 'sample_time_ms': 3.569, 'replay_time_ms': 2.774, 'grad_time_ms': 2.113, 'update_time_ms': 0.003, 'opt_peak_throughput': 15142.063, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 48.943993, 'min_q': 16.981941, 'max_q': 53.209854, 'mean_td_error': -0.53433496, 'model': {}}}}, 'timesteps_this_iter': 480, 'done': False, 'timesteps_total': 14112, 'episodes_total': 740, 'training_iteration': 29, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-50', 'timestamp': 1566081710, 'time_this_iter_s': 1.0068392753601074, 'time_total_s': 29.151424407958984, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 29.151424407958984, 'timesteps_since_restore': 14112, 'iterations_since_restore': 29}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 55.95, 'episode_len_mean': 125.22, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061992421845797875, 'mean_processing_ms': 0.20581015801649016, 'mean_inference_ms': 0.5683366393119538}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 173, 'num_steps_trained': 100704, 'num_steps_sampled': 14588, 'sample_time_ms': 3.429, 'replay_time_ms': 2.747, 'grad_time_ms': 2.029, 'update_time_ms': 0.003, 'opt_peak_throughput': 15772.507, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 51.933098, 'min_q': 21.510439, 'max_q': 54.582596, 'mean_td_error': 0.45665205, 'model': {}}}}, 'timesteps_this_iter': 476, 'done': False, 'timesteps_total': 14588, 'episodes_total': 741, 'training_iteration': 30, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-51', 'timestamp': 1566081711, 'time_this_iter_s': 1.009751558303833, 'time_total_s': 30.161175966262817, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 30.161175966262817, 'timesteps_since_restore': 14588, 'iterations_since_restore': 30}}\n",
      "Result for DQN_RLToy-v0_0:\n",
      "  callback_ok: true\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-18_00-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 125.22\n",
      "  episode_reward_max: 1211.0\n",
      "  episode_reward_mean: 55.95\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 741\n",
      "  experiment_id: 52abb92d04254715928ae3620e2b94a8\n",
      "  hostname: mlstaff04\n",
      "  info:\n",
      "    grad_time_ms: 2.108\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 6.25000029685907e-05\n",
      "        max_q: 55.66559600830078\n",
      "        mean_q: 53.35639953613281\n",
      "        mean_td_error: 0.15652042627334595\n",
      "        min_q: 1.476754903793335\n",
      "        model: {}\n",
      "    max_exploration: 0.010000000000000009\n",
      "    min_exploration: 0.010000000000000009\n",
      "    num_steps_sampled: 15052\n",
      "    num_steps_trained: 104416\n",
      "    num_target_updates: 179\n",
      "    opt_peak_throughput: 15178.022\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.82\n",
      "    sample_time_ms: 3.571\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 10.5.150.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 515\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.061992421845797875\n",
      "    mean_inference_ms: 0.5683366393119538\n",
      "    mean_processing_ms: 0.20581015801649014\n",
      "  time_since_restore: 31.16727924346924\n",
      "  time_this_iter_s: 1.006103277206421\n",
      "  time_total_s: 31.16727924346924\n",
      "  timestamp: 1566081712\n",
      "  timesteps_since_restore: 15052\n",
      "  timesteps_this_iter: 464\n",
      "  timesteps_total: 15052\n",
      "  training_iteration: 31\n",
      "  trial_id: 1f264556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.3/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_RLToy-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=515], 31 s, 31 iter, 15052 ts, 56 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 55.95, 'episode_len_mean': 125.22, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061992421845797875, 'mean_processing_ms': 0.20581015801649014, 'mean_inference_ms': 0.5683366393119538}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 179, 'num_steps_trained': 104416, 'num_steps_sampled': 15052, 'sample_time_ms': 3.571, 'replay_time_ms': 2.82, 'grad_time_ms': 2.108, 'update_time_ms': 0.003, 'opt_peak_throughput': 15178.022, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 53.3564, 'min_q': 1.4767549, 'max_q': 55.665596, 'mean_td_error': 0.15652043, 'model': {}}}}, 'timesteps_this_iter': 464, 'done': False, 'timesteps_total': 15052, 'episodes_total': 741, 'training_iteration': 31, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-52', 'timestamp': 1566081712, 'time_this_iter_s': 1.006103277206421, 'time_total_s': 31.16727924346924, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 31.16727924346924, 'timesteps_since_restore': 15052, 'iterations_since_restore': 31}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 55.95, 'episode_len_mean': 125.22, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.061992421845797875, 'mean_processing_ms': 0.20581015801649014, 'mean_inference_ms': 0.5683366393119538}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 184, 'num_steps_trained': 108192, 'num_steps_sampled': 15524, 'sample_time_ms': 3.518, 'replay_time_ms': 2.781, 'grad_time_ms': 2.079, 'update_time_ms': 0.003, 'opt_peak_throughput': 15390.529, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 52.188484, 'min_q': 0.3808405, 'max_q': 57.172047, 'mean_td_error': -0.31741145, 'model': {}}}}, 'timesteps_this_iter': 472, 'done': False, 'timesteps_total': 15524, 'episodes_total': 741, 'training_iteration': 32, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-53', 'timestamp': 1566081713, 'time_this_iter_s': 1.0053684711456299, 'time_total_s': 32.17264771461487, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 32.17264771461487, 'timesteps_since_restore': 15524, 'iterations_since_restore': 32}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 3 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 62.53, 'episode_len_mean': 139.58, 'episodes_this_iter': 3, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.062007593303885285, 'mean_processing_ms': 0.20375051878870937, 'mean_inference_ms': 0.5674902497074341}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 190, 'num_steps_trained': 111968, 'num_steps_sampled': 15996, 'sample_time_ms': 3.524, 'replay_time_ms': 2.743, 'grad_time_ms': 2.039, 'update_time_ms': 0.002, 'opt_peak_throughput': 15692.474, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 52.386703, 'min_q': -0.00039315596, 'max_q': 57.618374, 'mean_td_error': -0.32881355, 'model': {}}}}, 'timesteps_this_iter': 472, 'done': False, 'timesteps_total': 15996, 'episodes_total': 744, 'training_iteration': 33, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-54', 'timestamp': 1566081714, 'time_this_iter_s': 1.0018301010131836, 'time_total_s': 33.17447781562805, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 33.17447781562805, 'timesteps_since_restore': 15996, 'iterations_since_restore': 33}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 62.53, 'episode_len_mean': 139.58, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06200759330388528, 'mean_processing_ms': 0.20375051878870937, 'mean_inference_ms': 0.567490249707434}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 195, 'num_steps_trained': 115328, 'num_steps_sampled': 16416, 'sample_time_ms': 4.867, 'replay_time_ms': 2.947, 'grad_time_ms': 2.716, 'update_time_ms': 0.003, 'opt_peak_throughput': 11784.132, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 58.38085, 'min_q': 40.159985, 'max_q': 59.92613, 'mean_td_error': 1.7559521, 'model': {}}}}, 'timesteps_this_iter': 420, 'done': False, 'timesteps_total': 16416, 'episodes_total': 744, 'training_iteration': 34, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-55', 'timestamp': 1566081715, 'time_this_iter_s': 1.0068252086639404, 'time_total_s': 34.18130302429199, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 34.18130302429199, 'timesteps_since_restore': 16416, 'iterations_since_restore': 34}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 62.53, 'episode_len_mean': 139.58, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06200759330388528, 'mean_processing_ms': 0.20375051878870937, 'mean_inference_ms': 0.567490249707434}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 200, 'num_steps_trained': 118400, 'num_steps_sampled': 16800, 'sample_time_ms': 4.761, 'replay_time_ms': 2.72, 'grad_time_ms': 2.924, 'update_time_ms': 0.003, 'opt_peak_throughput': 10942.614, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 56.203426, 'min_q': 11.097846, 'max_q': 61.034027, 'mean_td_error': 1.8125069, 'model': {}}}}, 'timesteps_this_iter': 384, 'done': False, 'timesteps_total': 16800, 'episodes_total': 744, 'training_iteration': 35, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-56', 'timestamp': 1566081716, 'time_this_iter_s': 1.00846266746521, 'time_total_s': 35.1897656917572, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 35.1897656917572, 'timesteps_since_restore': 16800, 'iterations_since_restore': 35}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_RLToy-v0_0:\n",
      "  callback_ok: true\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-18_00-41-57\n",
      "  done: false\n",
      "  episode_len_mean: 139.58\n",
      "  episode_reward_max: 1211.0\n",
      "  episode_reward_mean: 62.53\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 744\n",
      "  experiment_id: 52abb92d04254715928ae3620e2b94a8\n",
      "  hostname: mlstaff04\n",
      "  info:\n",
      "    grad_time_ms: 2.237\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 6.25000029685907e-05\n",
      "        max_q: 61.372074127197266\n",
      "        mean_q: 58.63371658325195\n",
      "        mean_td_error: -0.23699043691158295\n",
      "        min_q: 6.222259998321533\n",
      "        model: {}\n",
      "    max_exploration: 0.010000000000000009\n",
      "    min_exploration: 0.010000000000000009\n",
      "    num_steps_sampled: 17176\n",
      "    num_steps_trained: 121408\n",
      "    num_target_updates: 204\n",
      "    opt_peak_throughput: 14303.437\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 3.376\n",
      "    sample_time_ms: 3.666\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 10.5.150.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 515\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06200759330388528\n",
      "    mean_inference_ms: 0.567490249707434\n",
      "    mean_processing_ms: 0.20375051878870937\n",
      "  time_since_restore: 36.200878620147705\n",
      "  time_this_iter_s: 1.011112928390503\n",
      "  time_total_s: 36.200878620147705\n",
      "  timestamp: 1566081717\n",
      "  timesteps_since_restore: 17176\n",
      "  timesteps_this_iter: 376\n",
      "  timesteps_total: 17176\n",
      "  training_iteration: 36\n",
      "  trial_id: 1f264556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.3/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_RLToy-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=515], 36 s, 36 iter, 17176 ts, 62.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 62.53, 'episode_len_mean': 139.58, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06200759330388528, 'mean_processing_ms': 0.20375051878870937, 'mean_inference_ms': 0.567490249707434}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 204, 'num_steps_trained': 121408, 'num_steps_sampled': 17176, 'sample_time_ms': 3.666, 'replay_time_ms': 3.376, 'grad_time_ms': 2.237, 'update_time_ms': 0.002, 'opt_peak_throughput': 14303.437, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 58.633717, 'min_q': 6.22226, 'max_q': 61.372074, 'mean_td_error': -0.23699044, 'model': {}}}}, 'timesteps_this_iter': 376, 'done': False, 'timesteps_total': 17176, 'episodes_total': 744, 'training_iteration': 36, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-57', 'timestamp': 1566081717, 'time_this_iter_s': 1.011112928390503, 'time_total_s': 36.200878620147705, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 36.200878620147705, 'timesteps_since_restore': 17176, 'iterations_since_restore': 36}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 69.4, 'episode_len_mean': 155.49, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06202096157155924, 'mean_processing_ms': 0.20306629581507907, 'mean_inference_ms': 0.567294372462027}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 209, 'num_steps_trained': 125088, 'num_steps_sampled': 17636, 'sample_time_ms': 3.633, 'replay_time_ms': 2.957, 'grad_time_ms': 2.155, 'update_time_ms': 0.002, 'opt_peak_throughput': 14851.694, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 57.49745, 'min_q': 4.7322474, 'max_q': 63.773376, 'mean_td_error': 0.6140807, 'model': {}}}}, 'timesteps_this_iter': 460, 'done': False, 'timesteps_total': 17636, 'episodes_total': 745, 'training_iteration': 37, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-58', 'timestamp': 1566081718, 'time_this_iter_s': 1.002152681350708, 'time_total_s': 37.20303130149841, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 37.20303130149841, 'timesteps_since_restore': 17636, 'iterations_since_restore': 37}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 70.12, 'episode_len_mean': 157.12, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.062033557148359, 'mean_processing_ms': 0.20237771370318175, 'mean_inference_ms': 0.5670882314107144}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 215, 'num_steps_trained': 128800, 'num_steps_sampled': 18100, 'sample_time_ms': 3.619, 'replay_time_ms': 2.939, 'grad_time_ms': 2.145, 'update_time_ms': 0.002, 'opt_peak_throughput': 14918.385, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 64.10534, 'min_q': 61.91525, 'max_q': 64.90978, 'mean_td_error': 0.17749667, 'model': {}}}}, 'timesteps_this_iter': 464, 'done': False, 'timesteps_total': 18100, 'episodes_total': 746, 'training_iteration': 38, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-41-59', 'timestamp': 1566081719, 'time_this_iter_s': 1.0062835216522217, 'time_total_s': 38.209314823150635, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 38.209314823150635, 'timesteps_since_restore': 18100, 'iterations_since_restore': 38}}\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 72.7, 'episode_len_mean': 163.15, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.062044891471392895, 'mean_processing_ms': 0.20168397268716354, 'mean_inference_ms': 0.5668711526235306}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 221, 'num_steps_trained': 132576, 'num_steps_sampled': 18572, 'sample_time_ms': 3.473, 'replay_time_ms': 2.814, 'grad_time_ms': 2.143, 'update_time_ms': 0.003, 'opt_peak_throughput': 14933.822, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 62.26305, 'min_q': 0.9711316, 'max_q': 65.58695, 'mean_td_error': 1.6142721, 'model': {}}}}, 'timesteps_this_iter': 472, 'done': False, 'timesteps_total': 18572, 'episodes_total': 747, 'training_iteration': 39, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-42-00', 'timestamp': 1566081720, 'time_this_iter_s': 1.0085299015045166, 'time_total_s': 39.21784472465515, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 39.21784472465515, 'timesteps_since_restore': 18572, 'iterations_since_restore': 39}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 72.7, 'episode_len_mean': 163.15, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.062044891471392895, 'mean_processing_ms': 0.20168397268716354, 'mean_inference_ms': 0.5668711526235305}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 226, 'num_steps_trained': 136416, 'num_steps_sampled': 19052, 'sample_time_ms': 3.481, 'replay_time_ms': 2.797, 'grad_time_ms': 2.147, 'update_time_ms': 0.002, 'opt_peak_throughput': 14903.642, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 58.53141, 'min_q': 0.98029727, 'max_q': 66.55881, 'mean_td_error': -2.9727938, 'model': {}}}}, 'timesteps_this_iter': 480, 'done': False, 'timesteps_total': 19052, 'episodes_total': 747, 'training_iteration': 40, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-42-01', 'timestamp': 1566081721, 'time_this_iter_s': 1.0030910968780518, 'time_total_s': 40.2209358215332, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 40.2209358215332, 'timesteps_since_restore': 19052, 'iterations_since_restore': 40}}\n",
      "Result for DQN_RLToy-v0_0:\n",
      "  callback_ok: true\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-18_00-42-02\n",
      "  done: false\n",
      "  episode_len_mean: 163.15\n",
      "  episode_reward_max: 1211.0\n",
      "  episode_reward_mean: 72.7\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 747\n",
      "  experiment_id: 52abb92d04254715928ae3620e2b94a8\n",
      "  hostname: mlstaff04\n",
      "  info:\n",
      "    grad_time_ms: 2.034\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 6.25000029685907e-05\n",
      "        max_q: 67.5573501586914\n",
      "        mean_q: 65.99849700927734\n",
      "        mean_td_error: -0.2770270109176636\n",
      "        min_q: 42.91172790527344\n",
      "        model: {}\n",
      "    max_exploration: 0.010000000000000009\n",
      "    min_exploration: 0.010000000000000009\n",
      "    num_steps_sampled: 19532\n",
      "    num_steps_trained: 140256\n",
      "    num_target_updates: 232\n",
      "    opt_peak_throughput: 15731.282\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.688\n",
      "    sample_time_ms: 3.444\n",
      "    update_time_ms: 0.002\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 10.5.150.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 515\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.062044891471392895\n",
      "    mean_inference_ms: 0.5668711526235305\n",
      "    mean_processing_ms: 0.20168397268716354\n",
      "  time_since_restore: 41.223050117492676\n",
      "  time_this_iter_s: 1.0021142959594727\n",
      "  time_total_s: 41.223050117492676\n",
      "  timestamp: 1566081722\n",
      "  timesteps_since_restore: 19532\n",
      "  timesteps_this_iter: 480\n",
      "  timesteps_total: 19532\n",
      "  training_iteration: 41\n",
      "  trial_id: 1f264556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.3/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_RLToy-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=515], 41 s, 41 iter, 19532 ts, 72.7 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 0 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 72.7, 'episode_len_mean': 163.15, 'episodes_this_iter': 0, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.062044891471392895, 'mean_processing_ms': 0.20168397268716354, 'mean_inference_ms': 0.5668711526235305}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 232, 'num_steps_trained': 140256, 'num_steps_sampled': 19532, 'sample_time_ms': 3.444, 'replay_time_ms': 2.688, 'grad_time_ms': 2.034, 'update_time_ms': 0.002, 'opt_peak_throughput': 15731.282, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 65.9985, 'min_q': 42.911728, 'max_q': 67.55735, 'mean_td_error': -0.277027, 'model': {}}}}, 'timesteps_this_iter': 480, 'done': False, 'timesteps_total': 19532, 'episodes_total': 747, 'training_iteration': 41, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-42-02', 'timestamp': 1566081722, 'time_this_iter_s': 1.0021142959594727, 'time_total_s': 41.223050117492676, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 41.223050117492676, 'timesteps_since_restore': 19532, 'iterations_since_restore': 41}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_RLToy-v0_0:\n",
      "  callback_ok: true\n",
      "  custom_metrics: {}\n",
      "  date: 2019-08-18_00-42-03\n",
      "  done: true\n",
      "  episode_len_mean: 175.43\n",
      "  episode_reward_max: 1211.0\n",
      "  episode_reward_mean: 78.49\n",
      "  episode_reward_min: 0.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 748\n",
      "  experiment_id: 52abb92d04254715928ae3620e2b94a8\n",
      "  hostname: mlstaff04\n",
      "  info:\n",
      "    grad_time_ms: 2.046\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 6.25000029685907e-05\n",
      "        max_q: 69.28202819824219\n",
      "        mean_q: 61.954063415527344\n",
      "        mean_td_error: 0.09763866662979126\n",
      "        min_q: -0.3660565912723541\n",
      "        model: {}\n",
      "    max_exploration: 0.010000000000000009\n",
      "    min_exploration: 0.010000000000000009\n",
      "    num_steps_sampled: 20008\n",
      "    num_steps_trained: 144064\n",
      "    num_target_updates: 238\n",
      "    opt_peak_throughput: 15639.993\n",
      "    opt_samples: 32.0\n",
      "    replay_time_ms: 2.711\n",
      "    sample_time_ms: 3.511\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 10.5.150.104\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  pid: 515\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06205245643800219\n",
      "    mean_inference_ms: 0.5666116418198061\n",
      "    mean_processing_ms: 0.20097573154931517\n",
      "  time_since_restore: 42.22515058517456\n",
      "  time_this_iter_s: 1.0021004676818848\n",
      "  time_total_s: 42.22515058517456\n",
      "  timestamp: 1566081723\n",
      "  timesteps_since_restore: 20008\n",
      "  timesteps_this_iter: 476\n",
      "  timesteps_total: 20008\n",
      "  training_iteration: 42\n",
      "  trial_id: 1f264556\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 17.3/33.6 GB\n",
      "Result logdir: /home/rajanr/ray_results/DQN\n",
      "Number of trials: 1 ({'TERMINATED': 1})\n",
      "TERMINATED trials:\n",
      " - DQN_RLToy-v0_0:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=515], 42 s, 42 iter, 20008 ts, 78.5 rew\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f51bd808828>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=515)\u001b[0m #############trainer.train() result: <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978> -> 1 episodes {'trainer': <ray.rllib.agents.trainer_template.DQN object at 0x7f23b2207978>, 'result': {'episode_reward_max': 1211.0, 'episode_reward_min': 0.0, 'episode_reward_mean': 78.49, 'episode_len_mean': 175.43, 'episodes_this_iter': 1, 'policy_reward_mean': {}, 'custom_metrics': {}, 'sampler_perf': {'mean_env_wait_ms': 0.06205245643800219, 'mean_processing_ms': 0.20097573154931517, 'mean_inference_ms': 0.5666116418198061}, 'off_policy_estimator': {}, 'info': {'min_exploration': 0.010000000000000009, 'max_exploration': 0.010000000000000009, 'num_target_updates': 238, 'num_steps_trained': 144064, 'num_steps_sampled': 20008, 'sample_time_ms': 3.511, 'replay_time_ms': 2.711, 'grad_time_ms': 2.046, 'update_time_ms': 0.003, 'opt_peak_throughput': 15639.993, 'opt_samples': 32.0, 'learner': {'default_policy': {'cur_lr': 6.25000029685907e-05, 'mean_q': 61.954063, 'min_q': -0.3660566, 'max_q': 69.28203, 'mean_td_error': 0.09763867, 'model': {}}}}, 'timesteps_this_iter': 476, 'done': False, 'timesteps_total': 20008, 'episodes_total': 748, 'training_iteration': 42, 'experiment_id': '52abb92d04254715928ae3620e2b94a8', 'date': '2019-08-18_00-42-03', 'timestamp': 1566081723, 'time_this_iter_s': 1.0021004676818848, 'time_total_s': 42.22515058517456, 'pid': 515, 'hostname': 'mlstaff04', 'node_ip': '10.5.150.104', 'config': {'monitor': False, 'log_level': 'INFO', 'callbacks': {'on_episode_start': None, 'on_episode_step': None, 'on_episode_end': None, 'on_sample_end': None, 'on_train_result': tune.function(<function on_train_result at 0x7f23b21a5ae8>), 'on_postprocess_traj': None}, 'ignore_worker_failures': False, 'log_sys_usage': True, 'model': {'conv_filters': None, 'conv_activation': 'relu', 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action_reward': False, 'state_shape': None, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_preprocessor': None, 'custom_model': None, 'custom_options': {}}, 'optimizer': {}, 'gamma': 0.99, 'horizon': None, 'soft_horizon': False, 'env_config': {'state_space_type': 'discrete', 'action_space_type': 'discrete', 'state_space_size': 16, 'action_space_size': 16, 'generate_random_mdp': True, 'delay': 6, 'sequence_length': 1, 'reward_density': 0.25, 'terminal_state_density': 0.25}, 'env': 'RLToy-v0', 'clip_rewards': None, 'clip_actions': True, 'preprocessor_pref': 'deepmind', 'lr': 6.25e-05, 'evaluation_interval': None, 'evaluation_num_episodes': 10, 'evaluation_config': {'exploration_fraction': 0, 'exploration_final_eps': 0}, 'num_workers': 0, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, 'custom_resources_per_worker': {}, 'num_cpus_for_driver': 1, 'num_envs_per_worker': 1, 'sample_batch_size': 4, 'train_batch_size': 32, 'batch_mode': 'truncate_episodes', 'sample_async': False, 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'compress_observations': True, 'collect_metrics_timeout': 180, 'metrics_smoothing_episodes': 100, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'min_iter_time_s': 1, 'timesteps_per_iteration': 100, 'seed': None, 'input': 'sampler', 'input_evaluation': ['is', 'wis'], 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'multiagent': {'policies': {}, 'policy_mapping_fn': None, 'policies_to_train': None}, 'num_atoms': 1, 'v_min': -10.0, 'v_max': 10.0, 'noisy': False, 'sigma0': 0.5, 'dueling': False, 'double_q': False, 'hiddens': [256], 'n_step': 1, 'schedule_max_timesteps': 20000, 'exploration_fraction': 0.1, 'exploration_final_eps': 0.01, 'target_network_update_freq': 80, 'soft_q': False, 'softmax_temp': 1.0, 'parameter_noise': False, 'buffer_size': 1000000, 'prioritized_replay': False, 'prioritized_replay_alpha': 0.5, 'prioritized_replay_beta': 0.4, 'beta_annealing_fraction': 1.0, 'final_prioritized_replay_beta': 1.0, 'prioritized_replay_eps': 1e-06, 'lr_schedule': None, 'adam_epsilon': 0.00015, 'grad_norm_clipping': 40, 'learning_starts': 2000, 'per_worker_exploration': False, 'worker_side_prioritization': False}, 'time_since_restore': 42.22515058517456, 'timesteps_since_restore': 20008, 'iterations_since_restore': 42}}\n"
     ]
    }
   ],
   "source": [
    "# stats = {}\n",
    "# aaaa = 3\n",
    "\n",
    "# fout = open('rl_stats_temp.csv', 'a') #hardcoded\n",
    "# fout.write('# basename, n_points, n_features, n_trees ')\n",
    "\n",
    "\n",
    "def on_train_result(info):\n",
    "    print(\"#############trainer.train() result: {} -> {} episodes\".format(\n",
    "        info[\"trainer\"], info[\"result\"][\"episodes_this_iter\"]), info)\n",
    "    # you can mutate the result dict to add new fields to return\n",
    "    stats['episode_len_mean'] = info['result']['episode_len_mean']\n",
    "#     print(\"++++++++\", aaaa, stats)\n",
    "    state_space_size = info[\"result\"][\"config\"][\"env_config\"][\"state_space_size\"]\n",
    "#     action_space_size = \n",
    "    fout = open('rl_stats_temp.csv', 'a') #hardcoded\n",
    "    fout.write('# basename, n_points, n_features, n_trees ' + str(state_space_size) + ' ' + str() + ' ' + str() + ' ' + str() + '\\n')\n",
    "    fout.close()\n",
    "\n",
    "    info[\"result\"][\"callback_ok\"] = True\n",
    "    \n",
    "\n",
    "# tune.run(\n",
    "#     RandomAgent,\n",
    "#     stop={\n",
    "#         \"timesteps_total\": 20000,\n",
    "#           },\n",
    "#     config={\n",
    "#       \"rollouts_per_iteration\": 10,\n",
    "#       \"env\": \"RLToy-v0\",\n",
    "#       \"env_config\": {\n",
    "#         'state_space_type': 'discrete',\n",
    "#         'action_space_type': 'discrete',\n",
    "#         'state_space_size': 16,\n",
    "#         'action_space_size': 16,\n",
    "#         'generate_random_mdp': True,\n",
    "#         'delay': 6,\n",
    "#         'sequence_length': 1,\n",
    "#         'reward_density': 0.25,\n",
    "#         'terminal_state_density': 0.25\n",
    "#         },\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# tune.run(\n",
    "#     VIAgent,\n",
    "#     stop={\n",
    "#         \"timesteps_total\": 20000,\n",
    "#           },\n",
    "#     config={\n",
    "#         \"tolerance\": 0.01,\n",
    "#         \"discount_factor\": 0.99,\n",
    "#         \"rollouts_per_iteration\": 10,\n",
    "#       \"env\": \"RLToy-v0\",\n",
    "#       \"env_config\": {\n",
    "#         'state_space_type': 'discrete',\n",
    "#         'action_space_type': 'discrete',\n",
    "#         'state_space_size': 10,\n",
    "#         'action_space_size': 10,\n",
    "#         'generate_random_mdp': True,\n",
    "#         'delay': 0,\n",
    "#         'sequence_length': 1,\n",
    "#         'reward_density': 0.25,\n",
    "#         'terminal_state_density': 0.25\n",
    "#         },\n",
    "#     },\n",
    "# )\n",
    "\n",
    "\n",
    "tune.run(\n",
    "    \"DQN\",\n",
    "    stop={\n",
    "        \"timesteps_total\": 20000,\n",
    "          },\n",
    "    config={\n",
    "      \"adam_epsilon\": 0.00015,\n",
    "      \"beta_annealing_fraction\": 1.0,\n",
    "      \"buffer_size\": 1000000,\n",
    "      \"double_q\": False,\n",
    "      \"dueling\": False,\n",
    "      \"env\": \"RLToy-v0\",\n",
    "      \"env_config\": {\n",
    "        'state_space_type': 'discrete',\n",
    "        'action_space_type': 'discrete',\n",
    "        'state_space_size': 16,\n",
    "        'action_space_size': 16,\n",
    "        'generate_random_mdp': True,\n",
    "        'delay': 6,\n",
    "        'sequence_length': 1,\n",
    "        'reward_density': 0.25,\n",
    "        'terminal_state_density': 0.25\n",
    "        },\n",
    "      \"exploration_final_eps\": 0.01,\n",
    "      \"exploration_fraction\": 0.1,\n",
    "      \"final_prioritized_replay_beta\": 1.0,\n",
    "      \"hiddens\": [\n",
    "        256\n",
    "      ],\n",
    "      \"learning_starts\": 2000,\n",
    "      \"lr\": 6.25e-05, # \"lr\": grid_search([1e-2, 1e-4, 1e-6]),\n",
    "      \"n_step\": 1,\n",
    "      \"noisy\": False,\n",
    "      \"num_atoms\": 1,\n",
    "      \"prioritized_replay\": False,\n",
    "      \"prioritized_replay_alpha\": 0.5,\n",
    "      \"sample_batch_size\": 4,\n",
    "      \"schedule_max_timesteps\": 20000,\n",
    "      \"target_network_update_freq\": 80,\n",
    "      \"timesteps_per_iteration\": 100,\n",
    "      \"train_batch_size\": 32,\n",
    "        \n",
    "              \"callbacks\": {\n",
    "#                 \"on_episode_start\": tune.function(on_episode_start),\n",
    "#                 \"on_episode_step\": tune.function(on_episode_step),\n",
    "#                 \"on_episode_end\": tune.function(on_episode_end),\n",
    "#                 \"on_sample_end\": tune.function(on_sample_end),\n",
    "                \"on_train_result\": tune.function(on_train_result),\n",
    "#                 \"on_postprocess_traj\": tune.function(on_postprocess_traj),\n",
    "            },\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 8, 16, 32] [2, 4, 8, 16, 32] [0, 1, 2, 4, 8, 16] [1, 2, 3, 4, 5] [0.   0.25 0.5  0.75 1.  ] [0.1   0.325 0.55  0.775 1.   ]\n"
     ]
    }
   ],
   "source": [
    "state_space_sizes = [2**i for i in range(1,6)]\n",
    "action_space_sizes = [2**i for i in range(1,6)]\n",
    "delays = [0] + [2**i for i in range(5)]\n",
    "sequence_lengths = [i for i in range(1,6)]\n",
    "reward_densities = np.linspace(0.0, 1.0, num=5)\n",
    "# make_reward_dense = [True, False]\n",
    "terminal_state_densities = np.linspace(0.1, 1.0, num=5)\n",
    "\n",
    "print(state_space_sizes, action_space_sizes, delays, sequence_lengths, reward_densities, terminal_state_densities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats\n",
    "fout = open('rl_stats_temp.csv', 'a') #hardcoded\n",
    "fout.write('# basename, n_points, n_features, n_trees ')\n",
    "\n",
    "fout.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
