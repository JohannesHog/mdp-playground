{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to analyse an MDP Playground experiment\n",
    "from mdp_playground.analysis import MDPP_Analysis\n",
    "# Set dir_name to the location where the CSV files from running an experiment were saved\n",
    "dir_name = '/home/rajanr/mdpp_8733940'\n",
    "# Set exp_name to the name that was given to the experiment when running it\n",
    "exp_name = 'sac_reacher_action_max'\n",
    "# Set the following to True to save PDFs of plots that you generate below\n",
    "save_fig = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "mdpp_analysis = MDPP_Analysis()\n",
    "train_stats, eval_stats, train_curves, eval_curves = mdpp_analysis.load_data(dir_name, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-D: Plots showing reward after 20k timesteps when varying a single meta-feature\n",
    "# Plots across 10 runs: Training: with std dev across the runs\n",
    "mdpp_analysis.plot_1d_dimensions(train_stats, save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots across 10 runs: Evaluation: with std dev across the runs\n",
    "mdpp_analysis.plot_1d_dimensions(eval_stats, save_fig, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This and the next cell do the same as the previous 2 cells but plot episode mean lengths instead of episode reward\n",
    "mdpp_analysis.plot_1d_dimensions(train_stats, save_fig, metric_num=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdpp_analysis.plot_1d_dimensions(eval_stats, save_fig, train=False, metric_num=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-D heatmap plots across 10 runs: Training runs: with std dev across the runs\n",
    "# There seems to be a bug with matplotlib - x and y axes tick labels are not correctly set even though we pass them. Please feel free to look into the code and suggest a correction if you find it.\n",
    "mdpp_analysis.plot_2d_heatmap(train_stats, save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2-D heatmap plots across 10 runs: Evaluation runs: with std dev across the runs\n",
    "mdpp_analysis.plot_2d_heatmap(eval_stats, save_fig, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves: Training: Each curve corresponds to a different seed for the agent\n",
    "mdpp_analysis.plot_learning_curves(train_curves, save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves: Evaluation: Each curve corresponds to a different seed for the agent\n",
    "mdpp_analysis.plot_learning_curves(eval_curves, save_fig, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "frc = [-1] + mdpp_analysis.final_rows_for_a_config\n",
    "print(len(frc), frc, type(train_curves))\n",
    "j = 15\n",
    "for i in range(5):\n",
    "    plt.plot(train_curves[frc[j+i]+1:frc[j+i+1], -2])\n",
    "plt.ylim([-100, 0])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data = train_stats\n",
    "metric_num = -2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "mean_data_ = np.mean(stats_data[..., metric_num], axis=-1) # the slice sub-selects the metric written in position metric_num from the \"last axis of diff. metrics that were written\" and then the axis of #seeds becomes axis=-1 ( before slice it was -2).\n",
    "to_plot_ = np.squeeze(mean_data_)\n",
    "std_dev_ = np.std(stats_data[..., metric_num], axis=-1) #seed\n",
    "to_plot_std_ = np.squeeze(std_dev_)\n",
    "\n",
    "#fig_width = len(self.tick_labels[0])\n",
    "fig_width = 5\n",
    "# plt.figure()\n",
    "plt.figure(figsize=(fig_width, 1.5))\n",
    "\n",
    "print(to_plot_.shape)\n",
    "plt.bar([i for i in range(to_plot_.shape[0])], to_plot_, yerr=to_plot_std_)\n",
    "plt.ylim([-5, 0])\n",
    "plt.grid()\n",
    "# plt.bar(self.tick_labels[0], to_plot_[:, 0], yerr=to_plot_std_[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
