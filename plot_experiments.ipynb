{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to analyse an MDP Playground experiment\n",
    "from mdp_playground.analysis import MDPP_Analysis\n",
    "\n",
    "# Set the following to True to save PDFs of plots that you generate below\n",
    "save_fig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data loading\n",
    "mdpp_analysis = MDPP_Analysis()\n",
    "\n",
    "# load multiple experiments data\n",
    "\n",
    "# experiments = { <exp_name_1>: <dir_name_1>, ... }\n",
    "# For each experiment (also works in case of a single experiment):\n",
    "# Set dir_name to the location where the CSV files from running an experiment were saved\n",
    "# Set exp_name to the name that was given to the experiment when running it, i.e., with the -e option\n",
    "experiments = {\n",
    "#     \"dqn_p_r_noises\" : \"/home/rajanr/spider_plots_data_discrete/\",\n",
    "#     \"a3c_p_r_noises\" : \"/home/rajanr/spider_plots_data_discrete/\",\n",
    "#                \"dqn_qbert_del\" :\"/home/rajanr/mdpp_8780992/\",\n",
    "#                 \"rainbow_qbert_del\": \"/home/rajanr/mdpp_8815604\",\n",
    "#                     \"a3c_qbert_del\": \"/home/rajanr/mdpp_3214031\",\n",
    "    \"rainbow_hydra_0\": \"/home/rajanr/mdpp_9485031_temp/\",\n",
    "}\n",
    "\n",
    "# Remember to set load_eval=False in case evaluation stats were not recorded and only training stats were recorded, otherwise there will be errors in loading the data in this cell.\n",
    "list_exp_data = mdpp_analysis.load_data(experiments, load_eval=True, exp_type='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-D: Plots showing reward after 20k timesteps when varying a single meta-feature\n",
    "# Plots across runs: Training: with std dev across the runs\n",
    "mdpp_analysis.plot_1d_dimensions(list_exp_data, save_fig, plot_type = \"agent\")\n",
    "mdpp_analysis.plot_1d_dimensions(list_exp_data, save_fig, use_aucs=True, plot_type = \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots across runs: Evaluation: with std dev across the runs\n",
    "mdpp_analysis.plot_1d_dimensions(list_exp_data, save_fig, train=False, plot_type = \"agent\")\n",
    "mdpp_analysis.plot_1d_dimensions(list_exp_data, save_fig, train=False, use_aucs=True, plot_type = \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-D: Plots showing reward after 20k timesteps when varying a single meta-feature\n",
    "# Plots across runs: Training: with std dev across the runs\n",
    "mdpp_analysis.plot_1d_dimensions(list_exp_data, save_fig, plot_type = \"metric\")\n",
    "mdpp_analysis.plot_1d_dimensions(list_exp_data, save_fig, use_aucs=True, plot_type = \"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots across runs: Evaluation: with std dev across the runs\n",
    "mdpp_analysis.plot_1d_dimensions(list_exp_data, save_fig, train=False, plot_type = \"metric\")\n",
    "mdpp_analysis.plot_1d_dimensions(list_exp_data, save_fig, use_aucs=True, train=False, plot_type = \"metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This and the next cell do the same as cells 3 and 4 but plot episode mean lengths instead of episode reward\n",
    "mdpp_analysis.plot_1d_dimensions(list_exp_data, save_fig, metric_num=-1, plot_type = \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdpp_analysis.plot_1d_dimensions(list_exp_data, save_fig, train=False, metric_num=-1, plot_type = \"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-D heatmap plots across 10 runs: Training runs: with std dev across the runs\n",
    "# There seems to be a bug with matplotlib - x and y axes tick labels are not correctly set even though we pass them. Please feel free to look into the code and suggest a correction if you find it.\n",
    "mdpp_analysis.plot_2d_heatmap(list_exp_data, save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2-D heatmap plots across 10 runs: Evaluation runs: with std dev across the runs\n",
    "mdpp_analysis.plot_2d_heatmap(list_exp_data, save_fig, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves: Training: Each curve corresponds to a different seed for the agent\n",
    "mdpp_analysis.plot_learning_curves(list_exp_data, save_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves: Evaluation: Each curve corresponds to a different seed for the agent\n",
    "mdpp_analysis.plot_learning_curves(list_exp_data, save_fig, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weights = {}\n",
    "# for continuous_experiments\n",
    "weights['reward_noise'] = [.2, .2, .2, .2, .2, .0, .0, .0]\n",
    "weights['action_loss_weight'] = [.33, .33, .33, .0, .0, .0]\n",
    "\n",
    "# Plot radar(spider) plot: Training: across different meta-features\n",
    "mdpp_analysis.plot_radar(list_exp_data, save_fig=save_fig, weights=weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot radar(spider) plot: Evaluation: across different meta-features\n",
    "mdpp_analysis.plot_radar(list_exp_data, save_fig=save_fig, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = list_exp_data[0]\n",
    "train_stats = exp_data['train_stats']\n",
    "eval_stats = exp_data['eval_stats']\n",
    "train_curves = exp_data['train_curves']\n",
    "eval_curves = exp_data['eval_curves']\n",
    "train_aucs = exp_data['train_aucs']\n",
    "eval_aucs = exp_data['eval_aucs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "frc = [-1] + mdpp_analysis.final_rows_for_a_config\n",
    "print(len(frc), frc, type(train_curves))\n",
    "j = 20\n",
    "for i in range(5):\n",
    "    plt.plot(train_curves[frc[j+i]+1:frc[j+i+1], -2])\n",
    "# plt.ylim([200, 250])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_data = train_stats\n",
    "metric_num = -2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "mean_data_ = np.mean(stats_data[..., metric_num], axis=-1) # the slice sub-selects the metric written in position metric_num from the \"last axis of diff. metrics that were written\" and then the axis of #seeds becomes axis=-1 ( before slice it was -2).\n",
    "to_plot_ = np.squeeze(mean_data_)\n",
    "std_dev_ = np.std(stats_data[..., metric_num], axis=-1) #seed\n",
    "to_plot_std_ = np.squeeze(std_dev_)\n",
    "\n",
    "#fig_width = len(self.tick_labels[0])\n",
    "fig_width = 5\n",
    "# plt.figure()\n",
    "plt.figure(figsize=(fig_width, 1.5))\n",
    "\n",
    "print(to_plot_.shape)\n",
    "plt.bar([i for i in range(to_plot_.shape[0])], to_plot_, yerr=to_plot_std_)\n",
    "plt.ylim([-5, 0])\n",
    "plt.grid()\n",
    "# plt.bar(self.tick_labels[0], to_plot_[:, 0], yerr=to_plot_std_[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_stats.shape, eval_stats.shape, train_curves.shape, eval_curves.shape)\n",
    "print(train_stats[:,0:4,:,:])\n",
    "ts_copy = train_stats.copy()\n",
    "ts_copy[:,1:,:,:] = train_stats[:,:-1,:,:]\n",
    "ts_copy[:,0,:,:] = train_stats[:,4,:,:]\n",
    "\n",
    "tc_copy = train_curves.copy()\n",
    "tc_copy[14955:,:] = train_curves[:-14955,:]\n",
    "tc_copy[:14955,:] = train_curves[-14955:,:]\n",
    "# 14955\n",
    "# mdpp_analysis.tick_labels[0][1:5], mdpp_analysis.tick_labels[0][0] = mdpp_analysis.tick_labels[0][0:4], mdpp_analysis.tick_labels[0][4]\n",
    "# mdpp_analysis.dims_values[1][1:5], mdpp_analysis.dims_values[1][0] = mdpp_analysis.dims_values[1][0:4], mdpp_analysis.dims_values[1][4]\n",
    "print(ts_copy)\n",
    "print(dir(mdpp_analysis))\n",
    "print(mdpp_analysis.metric_names, mdpp_analysis.tick_labels)\n",
    "print(mdpp_analysis.dims_values[1], mdpp_analysis.config_names, mdpp_analysis.dims_varied)\n",
    "print(train_curves[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some more analysis (for tune HPs)\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr as spm\n",
    "from scipy.stats import pearsonr as prs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dir_name_config = 'experiments/'\n",
    "file_ = dir_name_config + exp_name\n",
    "\n",
    "config_file_path = os.path.abspath('/'.join(file_.split('/')[:-1]))\n",
    "# print(file_.split('/')[:-1])\n",
    "print(\"config_file_path:\", config_file_path)\n",
    "sys.path.insert(1, config_file_path) #hack\n",
    "import importlib\n",
    "config = importlib.import_module(file_.split('/')[-1], package=None)\n",
    "print(\"Number of seeds for environment:\", config.num_seeds)\n",
    "\n",
    "value_tuples = []\n",
    "for config_type, config_dict in config.var_configs.items():\n",
    "    for key in config_dict:\n",
    "        if 'seed' in key:\n",
    "            print(\"Found seed axis:\", key)\n",
    "            pass\n",
    "        else:\n",
    "            assert type(config.var_configs[config_type][key]) == list, \"var_config should be a dict of dicts with lists as the leaf values to allow each configuration option to take multiple possible values\"\n",
    "            value_tuples.append(config.var_configs[config_type][key])\n",
    "print(\"value_tuples\", value_tuples)\n",
    "\n",
    "import itertools\n",
    "cartesian_product_configs = list(itertools.product(*value_tuples))\n",
    "print(\"Total number of configs. to run:\", len(cartesian_product_configs))\n",
    "print(\"Varying dims in mdpp_analysis.axis_labels (will have dummy_seed in there as 1st dim and may not show actual last varying dim because mdpp_analysis assumes last varying dim is seed (and ignores last config_name) and here seed is always the 1st dim):\", mdpp_analysis.axis_labels)\n",
    "# import itertools\n",
    "# cartesian_product_configs = list(itertools.product(*config_vals))\n",
    "for i in range(len(train_stats.shape)):\n",
    "    if train_stats.shape[i] > 1:\n",
    "        dummy_seeds_axis = i\n",
    "        break\n",
    "print(\"dummy_seeds_axis, train_stats.shape:\", dummy_seeds_axis, len(train_stats.shape))\n",
    "\n",
    "# dummy_seeds_axis = -1\n",
    "\n",
    "def analysis(train_stats):\n",
    "    mean_data_ = np.mean(train_stats[..., -2], axis=dummy_seeds_axis)\n",
    "    std_data_ = np.std(train_stats[..., -2], axis=dummy_seeds_axis)\n",
    "    print(\"Mean shape (after slice), Sliced shape:\", mean_data_.shape, train_stats[..., -2].shape)\n",
    "    flattened_mean = np.ravel(mean_data_)\n",
    "    flattened_std = np.ravel(std_data_)\n",
    "    ranks = np.argsort(flattened_mean)[::-1]\n",
    "    print('sort of indices:\\n', ranks)\n",
    "    ranks_with_std = np.argsort(flattened_mean - flattened_std)[::-1]\n",
    "    print('sort of indices (with std taken into account):\\n', ranks_with_std)\n",
    "    sorted_vals = np.sort(flattened_mean)[::-1]\n",
    "    print('sort of values:\\n', sorted_vals)\n",
    "    sorted_vals_with_std = np.sort(flattened_mean - flattened_std)[::-1]\n",
    "    print('sort of values (with std taken into account):\\n', sorted_vals_with_std)\n",
    "    print(\"TOP 3 configs (with std taken into account):\")\n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[-1]]) \n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[-2]])\n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[-3]])\n",
    "    print(\"\\nBOTTOM 3 configs (with std taken into account):\")\n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[0]]) \n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[1]])\n",
    "    print(cartesian_product_configs[np.argsort(flattened_mean - flattened_std)[2]])\n",
    "    plt.figure(figsize=(30, 1.5))\n",
    "    plt.bar([i for i in range(len(flattened_mean))], flattened_mean, yerr=flattened_std)\n",
    "    plt.show()\n",
    "    return flattened_mean, flattened_mean - flattened_std\n",
    "\n",
    "sorted_vals_t, sorted_vals_with_std_t = analysis(train_stats)\n",
    "sorted_vals_e, sorted_vals_with_std_e = analysis(eval_stats)\n",
    "print(spm(sorted_vals_t, sorted_vals_e))\n",
    "print(spm(sorted_vals_t, sorted_vals_with_std_t))\n",
    "print(spm(sorted_vals_with_std_t, sorted_vals_with_std_e))\n",
    "print(spm(sorted_vals_e, sorted_vals_with_std_e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.var_configs)\n",
    "print(train_stats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdpp_analysis.config_names, mdpp_analysis.config_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(np.mean(train_stats, axis=2))\n",
    "print(np.mean(train_aucs, axis=2))\n",
    "from scipy import stats\n",
    "print(stats.spearmanr([4,3,2,1,0], [4,3,2,1,0]))\n",
    "print(np.mean(eval_stats, axis=2))\n",
    "print(np.mean(eval_aucs, axis=2))\n",
    "print(train_stats)\n",
    "print(eval_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup to analyse an MDP Playground experiment\n",
    "from mdp_playground.analysis import MDPP_Analysis\n",
    "\n",
    "# Set the following to True to save PDFs of plots that you generate below\n",
    "save_fig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture out1\n",
    "mdpp_analysis = MDPP_Analysis()\n",
    "\n",
    "experiments = {\n",
    "#     \"rainbow_hydra_0\": \"/home/rajanr/mdpp_9485031_temp/\",\n",
    "}\n",
    "\n",
    "num_env_configs = 1000\n",
    "for i in range(num_env_configs):\n",
    "    experiments['rainbow_hydra_' + str(i)] = \"/home/rajanr/mdpp_9485031/\"\n",
    "\n",
    "# Remember to set load_eval=False in case evaluation stats were not recorded and only training stats were recorded, otherwise there will be errors in loading the data in this cell.\n",
    "list_exp_data_reward_scales = mdpp_analysis.load_data(experiments, load_eval=False, exp_type='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(list_exp_data))\n",
    "# del list_exp_data[259]\n",
    "print(list_exp_data_reward_scales[1]['train_stats'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configs in list_exp_data_ (hacky variable name)\n",
    "import pickle\n",
    "pik = \"mdpp_hydra_reward_scales_pickle.dat\"\n",
    "\n",
    "import os.path\n",
    "if not os.path.exists(pik):\n",
    "    with open(pik, \"wb\") as f:\n",
    "        pickle.dump(list_exp_data_, f)\n",
    "    print(\"Saved file.\")\n",
    "else:\n",
    "    print(\"File already exists!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configs in list_exp_data_reward_scales\n",
    "import pickle\n",
    "pik = \"mdpp_hydra_reward_scales_pickle.dat\"\n",
    "\n",
    "import os.path\n",
    "if not os.path.exists(pik):\n",
    "    with open(pik, \"wb\") as f:\n",
    "        pickle.dump(list_exp_data_reward_scales, f)\n",
    "    print(\"Saved file.\")\n",
    "else:\n",
    "    print(\"File already exists!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performance metrics in list_exp_data (usual filename)\n",
    "import pickle\n",
    "pik = \"mdpp_hydra_pickle.dat\"\n",
    "\n",
    "import os.path\n",
    "if not os.path.exists(pik):\n",
    "    with open(pik, \"wb\") as f:\n",
    "        pickle.dump(list_exp_data, f)\n",
    "    print(\"Saved file.\")\n",
    "else:\n",
    "    print(\"File already exists!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load above 3 saved pickles\n",
    "import pickle\n",
    "pik = \"mdpp_hydra_pickle.dat\"\n",
    "# pik = \"mdpp_hydra_pickle_train.dat\"\n",
    "pik_conf = \"mdpp_hydra_configs_pickle.dat\"\n",
    "pik_reward_scales = \"mdpp_hydra_reward_scales_pickle.dat\"\n",
    "\n",
    "\n",
    "with open(pik, \"rb\") as f:\n",
    "    list_exp_data = pickle.load(f)\n",
    "#     print()\n",
    "\n",
    "with open(pik_conf, \"rb\") as f:\n",
    "    list_exp_data_with_configs = pickle.load(f)\n",
    "\n",
    "with open(pik_reward_scales, \"rb\") as f:\n",
    "    list_exp_data_reward_scales = pickle.load(f)\n",
    "\n",
    "# print(list_exp_data)\n",
    "\n",
    "del list_exp_data[259]\n",
    "# del list_exp_data_with_configs[259]\n",
    "del list_exp_data_reward_scales[259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = []\n",
    "for key in list_exp_data[0]:\n",
    "    contents.append(key)\n",
    "print(contents)\n",
    "# print(list_exp_data[0]['train_stats'])\n",
    "print(len(list_exp_data[0]['dims_values']))\n",
    "\n",
    "print(len(list_exp_data_with_configs))\n",
    "# print(list_exp_data_with_configs[0])\n",
    "\n",
    "print(len(list_exp_data_reward_scales))\n",
    "print(list_exp_data_reward_scales[0]['train_stats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scratch\n",
    "# print(list_exp_data_with_configs[0]['train_stats'])\n",
    "i=0\n",
    "print(list_exp_data[i]['train_stats']['episode_reward_mean']/list_exp_data_reward_scales[i]['train_stats'])\n",
    "print(list_exp_data[i]['train_stats']['episode_reward_mean'].to_numpy()/list_exp_data_reward_scales[i]['train_stats'].to_numpy()) # ['reward_scale'] not needed since it's not a DataFrame anymore I think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_env_configs = 999\n",
    "num_agent_configs = 996\n",
    "\n",
    "rows = []\n",
    "cols = []\n",
    "for i in range(num_env_configs):\n",
    "    rows.append(list_exp_data[i]['train_stats'].shape[0])\n",
    "    cols.append(list_exp_data[i]['train_stats'].shape[1])\n",
    "    if rows[-1] != num_agent_configs:\n",
    "        print(i)\n",
    "# print(lens)\n",
    "print(sum(rows)/num_env_configs, sum(cols)/num_env_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lists of top configs for train, eval, etc. below\n",
    "# Collect performances of all agents on all envs for train, eval, etc.\n",
    "# Calculate Pearson corr. coeff. for all combinations of train, eval, etc.\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr as prs\n",
    "from scipy.stats import spearmanr as spm\n",
    "from collections import Counter\n",
    "\n",
    "num_env_configs = 999\n",
    "num_agent_configs = 996\n",
    "normalise_rewards = True\n",
    "\n",
    "perf_sets = ['train', 'eval', 'train_auc', 'eval_auc']\n",
    "top_configs = {}\n",
    "top_configs_mins = {}\n",
    "perfs_all_envs = {}\n",
    "\n",
    "print(\"env x agent grid size:\", num_env_configs, num_agent_configs)\n",
    "for perf_set in perf_sets:\n",
    "    top_configs[perf_set] = []\n",
    "    top_configs_mins[perf_set] = []\n",
    "    perfs_all_envs[perf_set] = np.zeros(shape=(num_env_configs, num_agent_configs))\n",
    "\n",
    "corrs = {}\n",
    "corrs_spm = {}\n",
    "import itertools\n",
    "corr_sets = ['train', 'eval', 'train_auc', 'eval_auc']\n",
    "corr_combos = list(itertools.combinations(corr_sets, 2))\n",
    "\n",
    "# corr_sets = ['train_eval', 'train_auc_eval_auc', 'eval_eval_auc', 'train_eval_auc', 'train_train_auc', 'eval_train_auc']\n",
    "for corr_combo in corr_combos:\n",
    "    corrs[corr_combo[0] + ' and ' + corr_combo[1]] = []\n",
    "    corrs_spm[corr_combo[0] + ' and ' + corr_combo[1]] = []\n",
    "    \n",
    "for i in range(num_env_configs):\n",
    "#     if i == 259:\n",
    "#         continue\n",
    "    perfs = {}\n",
    "    perfs['train'] = list_exp_data[i]['train_stats']['episode_reward_mean'].to_numpy().copy()\n",
    "    perfs['eval'] = list_exp_data[i]['eval_stats']['episode_reward_mean'].to_numpy().copy()\n",
    "    perfs['train_auc'] = list_exp_data[i]['train_aucs']['episode_reward_mean'].to_numpy().copy()\n",
    "    perfs['eval_auc'] = list_exp_data[i]['eval_aucs']['episode_reward_mean'].to_numpy().copy()\n",
    "    \n",
    "    if normalise_rewards:\n",
    "        perfs['train'] /= list_exp_data_reward_scales[i]['train_stats'].to_numpy()\n",
    "        perfs['eval'] /= list_exp_data_reward_scales[i]['train_stats'].to_numpy()\n",
    "        perfs['train_auc'] /= list_exp_data_reward_scales[i]['train_stats'].to_numpy()\n",
    "        perfs['eval_auc'] /= list_exp_data_reward_scales[i]['train_stats'].to_numpy()\n",
    "\n",
    "    for perf_set in perf_sets:\n",
    "        top_configs[perf_set].append(np.argmax(perfs[perf_set]))\n",
    "        top_configs_mins[perf_set].append(np.argmin(perfs[perf_set]))\n",
    "        perfs_all_envs[perf_set][i, :] = perfs[perf_set]\n",
    "#     top_configs['eval'].append(np.argmax(perfs['eval']))\n",
    "#     top_configs['train_auc'].append(np.argmax(perfs['train_auc']))\n",
    "#     top_configs['eval_auc'].append(np.argmax(perfs['eval_auc']))\n",
    "        \n",
    "    for combo in corr_combos:\n",
    "        corr_ = prs(perfs[combo[0]], perfs[combo[1]])[0]\n",
    "        corrs[combo[0] + ' and ' + combo[1]].append(corr_)\n",
    "        \n",
    "        corr_ = spm(perfs[combo[0]], perfs[combo[1]])[0]\n",
    "        corrs_spm[combo[0] + ' and ' + combo[1]].append(corr_)\n",
    "        \n",
    "\n",
    "#     corrs['train_eval']\n",
    "#     corrs['train_auc_eval_auc'].append(prs(perfs['train_auc'], perfs['eval_auc']))\n",
    "#     corrs['eval_eval_auc'].append(prs(perfs['eval'], perfs['eval_auc']))\n",
    "#     corrs['train_eval_auc'].append(prs(perfs['train'], perfs['eval_auc']))\n",
    "#     corrs['train_train_auc'].append(prs(perfs['train'], perfs['train_auc']))\n",
    "#     corrs['eval_train_auc'].append(prs(perfs['eval'], perfs['train_auc']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Hydra portfolios and analyse\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "top_perfs_on_envs = [0.0] * num_env_configs\n",
    "top_norm_perfs_on_envs = [0.0] * num_env_configs\n",
    "\n",
    "sum_over_maxes = {}\n",
    "sum_across_envs = {}\n",
    "hydra_perfs = {}\n",
    "portfolio = {}\n",
    "hydra_perfs_mins = {}\n",
    "portfolio_mins = {}\n",
    "\n",
    "\n",
    "num_portfolio_configs = 10\n",
    "\n",
    "sorted_agents = {}\n",
    "for perf_set in perf_sets:\n",
    "    sorted_agents[perf_set] = -np.sort(-perfs_all_envs[perf_set], axis=1)\n",
    "#     print(perf_set, sorted_agents[perf_set])\n",
    "\n",
    "\n",
    "for perf_set in perf_sets: # ['train']\n",
    "    counts = Counter(top_configs[perf_set])\n",
    "    counts_mins = Counter(top_configs_mins[perf_set])\n",
    "# print(top_configs['train'])\n",
    "# print(type(counts))\n",
    "# counts.items()\n",
    "#     print(\"Top configs on \" + perf_set + \":\", counts.most_common(num_portfolio_configs), len(counts))\n",
    "    print(\"Frequency of being top agent config on an env:\" + perf_set + \":\", counts, len(counts))\n",
    "    # mins:\n",
    "    print(\"\\nFrequency of being bottom agent config on an env:\" + perf_set + \":\", counts_mins, len(counts_mins))\n",
    "\n",
    "    sum_across_envs[perf_set] = np.sum(perfs_all_envs[perf_set], axis=0)\n",
    "    sum_over_maxes[perf_set] = np.sum(sorted_agents[perf_set], axis=0)\n",
    "    best_conf_index = np.argmax(sum_across_envs[perf_set])\n",
    "    hydra_perfs[perf_set] = [perfs_all_envs[perf_set][:, best_conf_index]]\n",
    "    print(\"\\nFirst portfolio member:\", best_conf_index)\n",
    "    portfolio[perf_set] = [best_conf_index]\n",
    "    port_perfs = [np.sum(hydra_perfs[perf_set])]\n",
    "    num_places_improvementss = [perfs_all_envs[perf_set].size]\n",
    "    print(\"Current portfolio perf.:\", port_perfs[0])\n",
    "    # print(\"hydra_perfs[perf_set]\", len(hydra_perfs[perf_set]), hydra_perfs[perf_set])\n",
    "    # print(\"Sums:\", sum_across_envs[perf_set])\n",
    "    \n",
    "    # Hydra using mins\n",
    "    worst_conf_index = np.argmin(sum_across_envs[perf_set])\n",
    "    hydra_perfs_mins[perf_set] = [perfs_all_envs[perf_set][:, worst_conf_index]]\n",
    "    portfolio_mins[perf_set] = [worst_conf_index]\n",
    "    port_perfs_mins = [np.sum(hydra_perfs_mins[perf_set])]\n",
    "    num_places_improvementss_mins = [perfs_all_envs[perf_set].size]\n",
    "    \n",
    "    for i in range(num_portfolio_configs - 1):\n",
    "    #     print(\"\\nIteration counter, i + 2:\", i + 2)\n",
    "        curr_portfolio_perfs = np.array(hydra_perfs[perf_set]).reshape(-1, 1)\n",
    "        improvements = perfs_all_envs[perf_set] - curr_portfolio_perfs\n",
    "    #     print(improvements)\n",
    "        improvements = np.clip(improvements, a_min=0.0, a_max=None) # Only improve where current portfolio is not better\n",
    "        improvements_per_agent = np.sum(improvements, axis=0)\n",
    "        num_places_improvements = np.count_nonzero(improvements)\n",
    "    #     print(\"Improvements in:\", num_places_improvements, \"out of num_env_configs x num_agent_configs =\", improvements.size)\n",
    "        num_places_improvementss.append(num_places_improvements)\n",
    "    #     print(improvements, improvements.shape)\n",
    "        best_conf_index = np.argmax(improvements_per_agent)\n",
    "        port_perfs.append(np.max(improvements_per_agent))\n",
    "    #     print(\"Next portfolio member:\", best_conf_index)\n",
    "        portfolio[perf_set].append(best_conf_index)\n",
    "        hydra_perfs[perf_set] = hydra_perfs[perf_set] + improvements[:, best_conf_index]\n",
    "    #     print(\"Current portfolio perf.:\", np.sum(hydra_perfs[perf_set]), \"Improvement:\", port_perfs[-1])\n",
    "    \n",
    "        # Hydra using mins\n",
    "        curr_portfolio_perfs_mins = np.array(hydra_perfs_mins[perf_set]).reshape(-1, 1)\n",
    "        improvements_mins = perfs_all_envs[perf_set] - curr_portfolio_perfs_mins\n",
    "    #     print(improvements)\n",
    "        improvements_mins = np.clip(improvements_mins, a_min=None, a_max=0.0) # Only improve where current portfolio is not better\n",
    "        improvements_per_agent_mins = np.sum(improvements_mins, axis=0)\n",
    "        num_places_improvements_mins = np.count_nonzero(improvements_mins)\n",
    "    #     print(\"Improvements in:\", num_places_improvements, \"out of num_env_configs x num_agent_configs =\", improvements.size)\n",
    "        num_places_improvementss_mins.append(num_places_improvements_mins)\n",
    "    #     print(improvements, improvements.shape)\n",
    "        worst_conf_index = np.argmin(improvements_per_agent_mins)\n",
    "        port_perfs_mins.append(np.min(improvements_per_agent_mins))\n",
    "    #     print(\"Next portfolio member:\", best_conf_index)\n",
    "        portfolio_mins[perf_set].append(worst_conf_index)\n",
    "        hydra_perfs_mins[perf_set] = hydra_perfs_mins[perf_set] + improvements_mins[:, worst_conf_index]\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Final portfolio:\", portfolio[perf_set])\n",
    "    print(\"Final portfolio perf.:\", np.sum(hydra_perfs[perf_set]))\n",
    "    print(\"Oracle perf.:\", sum_over_maxes[perf_set][0])\n",
    "    print(\"Final portfolio mins:\", portfolio_mins[perf_set])\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.plot(port_perfs, label=\"Perf. improvements\")\n",
    "    # plt.show()\n",
    "\n",
    "    plt.plot(num_places_improvementss, label=\"No. of config improvements\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Portfolio building iter.')\n",
    "    plt.ylabel('Reward or number of configs.')\n",
    "    plt.yscale('log')\n",
    "    plt.grid(which='both')\n",
    "    plt.show()\n",
    "\n",
    "#     print(port_perfs_mins, sum(port_perfs_mins))\n",
    "#     print(num_places_improvementss_mins)\n",
    "#     plt.plot(port_perfs_mins, label=\"Perf. improvements min\")\n",
    "#     plt.plot(num_places_improvementss_mins, label=\"No. of config improvements min\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "    \n",
    "#     print(\"Max over sums\")\n",
    "    max_over_sums = -np.sort(-sum_across_envs[perf_set])\n",
    "    plt.plot(sum_over_maxes[perf_set], label=\"Sum over maxes\")\n",
    "    plt.plot(max_over_sums, label=\"Max over sums\")\n",
    "    plt.legend()\n",
    "    plt.grid(which='both')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# for i in range(num_portfolio_configs):\n",
    "    \n",
    "#     for j in range(1, num_agent_configs):\n",
    "#         for k in range(num_env_configs):\n",
    "#             if hydra_perfs[perf_set][k] < perfs_all_envs[perf_set][k, j]:\n",
    "                \n",
    "    \n",
    "    \n",
    "# print(np.max(perfs_train_all_envs, axis=1))\n",
    "# print(perfs_all_envs)\n",
    "for combo in corr_combos:\n",
    "#     print(\"Corr. on \" + str(combo[0] + ' and ' + combo[1]), corrs[combo[0] + ' and ' + combo[1]])\n",
    "    print(\"Max (across envs) corr. on \" + str(combo[0] + ' and ' + combo[1]), max(corrs[combo[0] + ' and ' + combo[1]]))\n",
    "    print(\"Min corr. on \" + str(combo[0] + ' and ' + combo[1]), min(corrs[combo[0] + ' and ' + combo[1]]))\n",
    "    print(\"Max spm corr. on \" + str(combo[0] + ' and ' + combo[1]), max(corrs_spm[combo[0] + ' and ' + combo[1]]))\n",
    "    print(\"Min spm corr. on \" + str(combo[0] + ' and ' + combo[1]), min(corrs_spm[combo[0] + ' and ' + combo[1]]))\n",
    "    \n",
    "#     corrs[combo[0] + ' and ' + combo[1]]\n",
    "\n",
    "# for i in range(num_env_configs):\n",
    "#     corrs[combo[0] + ' and ' + combo[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman correlation of agent configs on 100 random pairs of envs\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "# From https://stackoverflow.com/a/48581219/11063709\n",
    "n = 1000\n",
    "A = list(range(n))\n",
    "k = 2\n",
    "m = 100\n",
    "\n",
    "samples = set()\n",
    "tries = 0\n",
    "while len(samples) < m:\n",
    "    samples.add(tuple(sorted(random.sample(A, k))))\n",
    "    tries += 1\n",
    "\n",
    "samples = list(samples)\n",
    "# print(samples)\n",
    "# print(tries)\n",
    "\n",
    "corrs_spm_agents_on_envs = {}\n",
    "for perf_set in perf_sets:\n",
    "    corrs_spm_agents_on_envs[perf_set] = []\n",
    "\n",
    "print(\"Spearman correlation of agent configs on 100 random pairs of envs:\")\n",
    "print(\"Mean, std, max, min\")\n",
    "for perf_set in perf_sets:    \n",
    "    for i in range(len(samples)):\n",
    "#         print(perfs[perf_set])\n",
    "        env_0_perfs = perfs_all_envs[perf_set][samples[i][0], :]\n",
    "        env_1_perfs = perfs_all_envs[perf_set][samples[i][1], :]\n",
    "    \n",
    "        corr_spm = spm(env_0_perfs, env_1_perfs)[0]\n",
    "        corrs_spm_agents_on_envs[perf_set].append(corr_spm)\n",
    "        \n",
    "#     print(corrs_spm_agents_on_envs[perf_set])\n",
    "\n",
    "    print(perf_set, np.mean(corrs_spm_agents_on_envs[perf_set]), np.std(corrs_spm_agents_on_envs[perf_set]), np.max(corrs_spm_agents_on_envs[perf_set]), np.min(corrs_spm_agents_on_envs[perf_set]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "configs = list_exp_data_with_configs[0]['train_stats']\n",
    "\n",
    "# Not needed since it's only on env 0:\n",
    "# if normalise_rewards:\n",
    "#     configs['episode_reward_mean'] /= list_exp_data_with_configs[0]['train_stats']['reward_scale'].to_numpy()\n",
    "\n",
    "num_cols_b = -11 # agent config begins at -11\n",
    "num_cols_e = -3 # agent config ends at -3\n",
    "\n",
    "portfolio_eval_auc_normalised = [634, 287, 488, 542, 570, 123, 471, 177, 465, 848]\n",
    "portfolio_eval_auc_normalised_mins = [149, 864, 869, 478, 470, 306, 218, 735, 113, 349]\n",
    "\n",
    "agent_conf_eval_auc_normalised = configs.iloc[portfolio_eval_auc_normalised, num_cols_b:num_cols_e]\n",
    "print(\"portfolio_eval_auc_normalised\\n\", agent_conf_eval_auc_normalised)\n",
    "print(\"\\nMean:\", agent_conf_eval_auc_normalised.mean(axis=0))\n",
    "\n",
    "agent_conf_eval_auc_normalised_mins = configs.iloc[portfolio_eval_auc_normalised_mins, num_cols_b:num_cols_e]\n",
    "# print(\"portfolio_eval_auc_normalised_mins\\n\", agent_conf_eval_auc_normalised_mins)\n",
    "print(\"\\nMean:\", agent_conf_eval_auc_normalised_mins.mean(axis=0))\n",
    "\n",
    "for perf_set in perf_sets: # ['train']\n",
    "    print(\"\\nperf_set:\", perf_set)\n",
    "    agent_conf = configs.iloc[portfolio[perf_set], num_cols_b:num_cols_e]\n",
    "    print(agent_conf)\n",
    "    print(\"\\nMean:\", agent_conf.mean(axis=0))\n",
    "#     print(\"\\nCorr.:\", agent_conf.corr(method='pearson'))\n",
    "#     print(portfolio[perf_set])\n",
    "print(type(agent_conf['buffer_size']), agent_conf['buffer_size'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save top and bottom Hydra configs to disk\n",
    "agent_confs_top = agent_conf_eval_auc_normalised.to_dict(orient='records')\n",
    "agent_confs_bottom = agent_conf_eval_auc_normalised_mins.to_dict(orient='records')\n",
    "\n",
    "# print(agent_confs_top)\n",
    "# print(agent_confs_bottom)\n",
    "\n",
    "import pickle\n",
    "pik = \"mdpp_hydra_agent_configs_pickle.dat\"\n",
    "\n",
    "import os.path\n",
    "if not os.path.exists(pik):\n",
    "    with open(pik, \"wb\") as f:\n",
    "        pickle.dump([agent_confs_top, agent_confs_bottom], f)\n",
    "    print(\"Saved file.\")\n",
    "else:\n",
    "    print(\"File already exists!\")\n",
    "\n",
    "agent_conf_eval_auc_normalised_mins.to_numpy().tolist()\n",
    "agent_conf_eval_auc_normalised_mins.to_dict()\n",
    "# agent_conf_eval_auc_normalised.to_records(index=False).tolist() + \\\n",
    "# agent_conf_eval_auc_normalised_mins.to_records(index=False).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pik = \"mdpp_hydra_agent_configs_pickle.dat\"\n",
    "\n",
    "with open(pik, \"rb\") as f:\n",
    "    agent_confs_top, agent_confs_bottom = pickle.load(f)\n",
    "\n",
    "print(agent_confs_top, agent_confs_bottom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "perfs_train = list_exp_data[0]['train_stats']['episode_reward_mean'].to_numpy()\n",
    "perfs_eval = list_exp_data[0]['eval_stats']['episode_reward_mean'].to_numpy()\n",
    "perfs_train_auc = list_exp_data[0]['train_aucs']['episode_reward_mean'].to_numpy()\n",
    "perfs_eval_auc = list_exp_data[0]['eval_aucs']['episode_reward_mean'].to_numpy()\n",
    "\n",
    "from scipy.stats import pearsonr as prs\n",
    "from scipy.stats import spearmanr as spm\n",
    "print(\"Corr.:\", prs(perfs_train, perfs_eval))\n",
    "print(\"Corr.:\", prs(perfs_train, perfs_train_auc))\n",
    "print(\"Corr.:\", prs(perfs_eval, perfs_eval_auc))\n",
    "print(\"Corr.:\", prs(perfs_train, perfs_eval_auc))\n",
    "print(\"Corr.:\", prs(perfs_train_auc, perfs_eval_auc))\n",
    "print(\"Corr.:\", prs(perfs_eval, perfs_train_auc))\n",
    "\n",
    "# print(perfs)\n",
    "# print(perfs_eval)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.plot(perfs_train)\n",
    "# plt.show()\n",
    "plt.plot(perfs_eval)\n",
    "plt.show()\n",
    "\n",
    "top_10_train = np.argsort(perfs_train)[-10:-1]\n",
    "top_10_eval = np.argsort(perfs_eval)[-10:-1]\n",
    "print(top_10_train)\n",
    "print(perfs[top_10_train])\n",
    "print(top_10_eval)\n",
    "print(perfs_eval[top_10_eval])\n",
    "print(list_exp_data[0]['train_stats'].iloc[top_10_train, :])\n",
    "print(list_exp_data[0]['eval_stats'].iloc[top_10_eval, :])\n",
    "\n",
    "\n",
    "print(type(list_exp_data[0]['train_stats']))\n",
    "print(perfs_train.shape)\n",
    "print(perfs_eval.shape)\n",
    "print(list_exp_data[0]['train_stats'].columns)\n",
    "# list_exp_data[0]['train_stats'].groupby(['col_a', 'col_b']).ngroups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
