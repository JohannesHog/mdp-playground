# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 1000 4.44e-01 4.03e+00
2 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 2000 3.80e+00 7.76e+00
3 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 3000 1.22e+01 1.66e+01
4 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 4000 2.10e+01 2.55e+01
5 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 5000 3.08e+01 3.52e+01
6 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 6000 3.92e+01 4.31e+01
7 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 7000 4.89e+01 5.24e+01
8 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 8000 5.87e+01 6.19e+01
9 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 9000 6.85e+01 7.14e+01
10 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 10000 7.77e+01 8.04e+01
11 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 11000 8.73e+01 8.97e+01
12 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 12000 8.95e+01 9.14e+01
13 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 13000 9.11e+01 9.25e+01
14 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 14000 8.94e+01 9.07e+01
15 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 15000 9.08e+01 9.20e+01
16 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 16000 8.94e+01 9.05e+01
17 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 17000 8.89e+01 8.99e+01
18 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 18000 8.81e+01 8.91e+01
19 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 19000 8.58e+01 8.73e+01
20 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 0 20000 8.54e+01 8.70e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 1000 4.79e-01 3.85e+00
2 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 2000 3.63e+00 7.19e+00
3 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 3000 1.19e+01 1.48e+01
4 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 4000 2.22e+01 2.49e+01
5 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 5000 3.16e+01 3.41e+01
6 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 6000 4.13e+01 4.36e+01
7 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 7000 5.08e+01 5.28e+01
8 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 8000 6.03e+01 6.21e+01
9 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 9000 6.93e+01 7.08e+01
10 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 10000 7.88e+01 8.01e+01
11 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 11000 8.78e+01 8.89e+01
12 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 12000 8.74e+01 8.85e+01
13 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 13000 8.80e+01 8.92e+01
14 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 14000 8.91e+01 9.01e+01
15 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 15000 8.82e+01 8.92e+01
16 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 16000 9.06e+01 9.16e+01
17 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 17000 8.99e+01 9.13e+01
18 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 18000 8.98e+01 9.12e+01
19 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 19000 9.08e+01 9.22e+01
20 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 1 20000 9.07e+01 9.21e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 1000 4.98e-01 4.21e+00
2 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 2000 3.72e+00 7.58e+00
3 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 3000 1.27e+01 1.62e+01
4 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 4000 2.24e+01 2.57e+01
5 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 5000 3.18e+01 3.49e+01
6 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 6000 4.07e+01 4.37e+01
7 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 7000 5.07e+01 5.33e+01
8 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 8000 6.02e+01 6.24e+01
9 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 9000 6.90e+01 7.10e+01
10 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 10000 7.86e+01 8.02e+01
11 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 11000 8.62e+01 8.76e+01
12 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 12000 8.64e+01 8.76e+01
13 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 13000 8.77e+01 8.90e+01
14 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 14000 8.76e+01 8.88e+01
15 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 15000 8.75e+01 8.89e+01
16 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 16000 8.66e+01 8.79e+01
17 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 17000 8.76e+01 8.89e+01
18 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 18000 8.89e+01 9.01e+01
19 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 19000 8.88e+01 9.00e+01
20 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 2 20000 8.76e+01 8.88e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 1000 4.33e-01 3.91e+00
2 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 2000 3.72e+00 7.87e+00
3 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 3000 1.23e+01 1.62e+01
4 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 4000 2.24e+01 2.60e+01
5 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 5000 3.11e+01 3.46e+01
6 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 6000 4.08e+01 4.41e+01
7 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 7000 4.96e+01 5.25e+01
8 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 8000 5.96e+01 6.24e+01
9 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 9000 6.85e+01 7.06e+01
10 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 10000 7.83e+01 8.02e+01
11 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 11000 8.15e+01 8.32e+01
12 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 12000 8.08e+01 8.26e+01
13 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 13000 8.19e+01 8.36e+01
14 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 14000 8.19e+01 8.35e+01
15 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 15000 8.24e+01 8.40e+01
16 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 16000 8.25e+01 8.43e+01
17 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 17000 8.30e+01 8.49e+01
18 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 18000 8.32e+01 8.52e+01
19 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 19000 8.54e+01 8.74e+01
20 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 3 20000 8.49e+01 8.69e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 1000 5.64e-01 4.39e+00
2 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 2000 3.71e+00 7.60e+00
3 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 3000 1.22e+01 1.57e+01
4 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 4000 2.19e+01 2.52e+01
5 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 5000 3.21e+01 3.50e+01
6 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 6000 4.07e+01 4.32e+01
7 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 7000 5.06e+01 5.29e+01
8 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 8000 5.98e+01 6.18e+01
9 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 9000 6.99e+01 7.17e+01
10 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 10000 7.91e+01 8.05e+01
11 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 11000 8.83e+01 8.93e+01
12 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 12000 8.90e+01 8.99e+01
13 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 13000 8.85e+01 8.94e+01
14 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 14000 8.98e+01 9.08e+01
15 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 15000 8.76e+01 8.86e+01
16 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 16000 8.58e+01 8.68e+01
17 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 17000 8.57e+01 8.67e+01
18 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 18000 8.61e+01 8.71e+01
19 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 19000 8.62e+01 8.72e+01
20 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 4 20000 8.54e+01 8.65e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 1000 6.10e-01 4.48e+00
2 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 2000 3.17e+00 6.99e+00
3 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 3000 1.24e+01 1.61e+01
4 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 4000 2.13e+01 2.46e+01
5 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 5000 3.06e+01 3.36e+01
6 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 6000 4.01e+01 4.29e+01
7 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 7000 4.95e+01 5.19e+01
8 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 8000 5.94e+01 6.14e+01
9 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 9000 6.82e+01 6.98e+01
10 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 10000 7.86e+01 8.00e+01
11 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 11000 8.42e+01 8.55e+01
12 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 12000 8.60e+01 8.73e+01
13 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 13000 8.64e+01 8.76e+01
14 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 14000 8.76e+01 8.87e+01
15 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 15000 8.60e+01 8.71e+01
16 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 16000 8.63e+01 8.75e+01
17 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 17000 8.54e+01 8.70e+01
18 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 18000 8.75e+01 8.90e+01
19 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 19000 8.70e+01 8.85e+01
20 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 5 20000 8.77e+01 8.92e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 1000 5.61e-01 4.18e+00
2 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 2000 4.04e+00 7.22e+00
3 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 3000 1.34e+01 1.64e+01
4 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 4000 2.28e+01 2.56e+01
5 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 5000 3.20e+01 3.44e+01
6 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 6000 4.15e+01 4.37e+01
7 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 7000 5.03e+01 5.22e+01
8 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 8000 6.03e+01 6.20e+01
9 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 9000 6.98e+01 7.14e+01
10 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 10000 7.89e+01 8.02e+01
11 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 11000 8.83e+01 8.93e+01
12 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 12000 8.88e+01 8.99e+01
13 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 13000 8.86e+01 8.97e+01
14 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 14000 8.89e+01 9.01e+01
15 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 15000 8.68e+01 8.78e+01
16 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 16000 8.68e+01 8.78e+01
17 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 17000 8.70e+01 8.81e+01
18 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 18000 8.92e+01 9.03e+01
19 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 19000 8.80e+01 8.91e+01
20 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 6 20000 8.71e+01 8.81e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 1000 6.37e-01 4.19e+00
2 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 2000 4.11e+00 8.18e+00
3 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 3000 1.29e+01 1.68e+01
4 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 4000 2.23e+01 2.60e+01
5 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 5000 3.15e+01 3.48e+01
6 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 6000 4.07e+01 4.36e+01
7 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 7000 4.95e+01 5.22e+01
8 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 8000 6.00e+01 6.22e+01
9 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 9000 6.93e+01 7.14e+01
10 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 10000 7.85e+01 8.02e+01
11 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 11000 8.60e+01 8.75e+01
12 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 12000 8.56e+01 8.71e+01
13 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 13000 8.67e+01 8.81e+01
14 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 14000 8.64e+01 8.77e+01
15 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 15000 8.62e+01 8.75e+01
16 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 16000 8.76e+01 8.89e+01
17 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 17000 8.60e+01 8.74e+01
18 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 18000 8.50e+01 8.63e+01
19 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 19000 8.63e+01 8.77e+01
20 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 7 20000 8.62e+01 8.77e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 1000 4.69e-01 3.88e+00
2 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 2000 3.61e+00 7.54e+00
3 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 3000 1.30e+01 1.69e+01
4 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 4000 2.28e+01 2.65e+01
5 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 5000 3.16e+01 3.50e+01
6 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 6000 4.14e+01 4.45e+01
7 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 7000 5.07e+01 5.35e+01
8 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 8000 6.04e+01 6.23e+01
9 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 9000 7.03e+01 7.20e+01
10 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 10000 7.88e+01 8.00e+01
11 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 11000 8.53e+01 8.63e+01
12 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 12000 8.37e+01 8.46e+01
13 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 13000 8.27e+01 8.36e+01
14 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 14000 8.19e+01 8.28e+01
15 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 15000 8.22e+01 8.32e+01
16 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 16000 8.40e+01 8.51e+01
17 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 17000 8.33e+01 8.44e+01
18 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 18000 8.30e+01 8.40e+01
19 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 19000 8.30e+01 8.40e+01
20 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 8 20000 8.35e+01 8.44e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 1000 4.77e-01 3.79e+00
2 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 2000 4.39e+00 8.55e+00
3 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 3000 1.27e+01 1.68e+01
4 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 4000 2.16e+01 2.55e+01
5 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 5000 3.20e+01 3.57e+01
6 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 6000 4.09e+01 4.44e+01
7 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 7000 5.01e+01 5.32e+01
8 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 8000 5.95e+01 6.22e+01
9 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 9000 6.93e+01 7.16e+01
10 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 10000 7.90e+01 8.09e+01
11 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 11000 8.83e+01 8.98e+01
12 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 12000 9.00e+01 9.11e+01
13 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 13000 9.06e+01 9.14e+01
14 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 14000 9.02e+01 9.11e+01
15 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 15000 9.13e+01 9.22e+01
16 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 16000 9.03e+01 9.12e+01
17 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 17000 9.06e+01 9.15e+01
18 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 18000 9.08e+01 9.17e+01
19 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 19000 9.03e+01 9.12e+01
20 DQN 8 8 0 1 2.50e-01 False 2.50e-01 0 0 9 20000 9.06e+01 9.18e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 1000 2.46e-01 4.08e+00
2 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 2000 1.47e+00 7.59e+00
3 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 3000 4.87e+00 1.69e+01
4 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 4000 8.56e+00 2.59e+01
5 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 5000 1.25e+01 3.52e+01
6 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 6000 1.61e+01 4.40e+01
7 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 7000 1.99e+01 5.24e+01
8 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 8000 2.36e+01 6.12e+01
9 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 9000 2.80e+01 7.11e+01
10 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 10000 3.11e+01 8.05e+01
11 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 11000 3.51e+01 8.96e+01
12 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 12000 3.55e+01 9.04e+01
13 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 13000 3.52e+01 9.02e+01
14 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 14000 3.38e+01 8.70e+01
15 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 15000 3.28e+01 8.62e+01
16 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 16000 3.25e+01 8.51e+01
17 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 17000 3.12e+01 8.30e+01
18 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 18000 3.14e+01 8.42e+01
19 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 19000 3.04e+01 8.17e+01
20 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 0 20000 3.08e+01 8.25e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 1000 2.53e-01 3.83e+00
2 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 2000 1.54e+00 7.47e+00
3 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 3000 6.35e+00 1.68e+01
4 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 4000 1.08e+01 2.57e+01
5 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 5000 1.55e+01 3.45e+01
6 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 6000 2.05e+01 4.43e+01
7 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 7000 2.49e+01 5.25e+01
8 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 8000 2.95e+01 6.13e+01
9 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 9000 3.42e+01 7.06e+01
10 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 10000 3.89e+01 7.97e+01
11 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 11000 4.25e+01 8.68e+01
12 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 12000 4.23e+01 8.63e+01
13 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 13000 4.26e+01 8.70e+01
14 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 14000 4.19e+01 8.58e+01
15 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 15000 4.25e+01 8.67e+01
16 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 16000 4.31e+01 8.79e+01
17 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 17000 4.28e+01 8.71e+01
18 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 18000 4.12e+01 8.41e+01
19 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 19000 4.12e+01 8.40e+01
20 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 1 20000 4.07e+01 8.29e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 1000 1.89e-01 3.77e+00
2 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 2000 1.68e+00 7.24e+00
3 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 3000 6.34e+00 1.66e+01
4 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 4000 1.13e+01 2.61e+01
5 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 5000 1.59e+01 3.50e+01
6 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 6000 2.05e+01 4.41e+01
7 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 7000 2.52e+01 5.36e+01
8 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 8000 2.99e+01 6.28e+01
9 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 9000 3.41e+01 7.09e+01
10 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 10000 3.90e+01 8.02e+01
11 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 11000 4.31e+01 8.82e+01
12 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 12000 4.22e+01 8.62e+01
13 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 13000 4.29e+01 8.76e+01
14 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 14000 4.24e+01 8.65e+01
15 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 15000 4.21e+01 8.55e+01
16 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 16000 4.17e+01 8.48e+01
17 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 17000 4.18e+01 8.47e+01
18 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 18000 4.14e+01 8.42e+01
19 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 19000 4.23e+01 8.59e+01
20 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 2 20000 4.16e+01 8.44e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 1000 2.19e-01 4.13e+00
2 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 2000 1.76e+00 7.69e+00
3 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 3000 5.85e+00 1.61e+01
4 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 4000 1.05e+01 2.51e+01
5 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 5000 1.50e+01 3.40e+01
6 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 6000 1.95e+01 4.28e+01
7 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 7000 2.39e+01 5.20e+01
8 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 8000 2.85e+01 6.18e+01
9 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 9000 3.33e+01 7.12e+01
10 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 10000 3.82e+01 8.05e+01
11 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 11000 4.22e+01 8.78e+01
12 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 12000 4.28e+01 8.86e+01
13 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 13000 4.15e+01 8.62e+01
14 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 14000 4.29e+01 8.87e+01
15 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 15000 4.41e+01 9.12e+01
16 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 16000 4.42e+01 9.07e+01
17 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 17000 4.47e+01 9.08e+01
18 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 18000 4.45e+01 9.07e+01
19 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 19000 4.49e+01 9.15e+01
20 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 3 20000 4.50e+01 9.18e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 1000 2.69e-01 4.00e+00
2 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 2000 1.60e+00 7.48e+00
3 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 3000 6.14e+00 1.62e+01
4 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 4000 1.06e+01 2.48e+01
5 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 5000 1.58e+01 3.48e+01
6 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 6000 2.03e+01 4.40e+01
7 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 7000 2.48e+01 5.29e+01
8 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 8000 2.96e+01 6.22e+01
9 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 9000 3.43e+01 7.14e+01
10 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 10000 3.89e+01 8.03e+01
11 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 11000 4.35e+01 8.92e+01
12 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 12000 4.33e+01 8.88e+01
13 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 13000 4.39e+01 9.00e+01
14 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 14000 4.39e+01 9.09e+01
15 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 15000 4.40e+01 9.07e+01
16 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 16000 4.42e+01 9.11e+01
17 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 17000 4.48e+01 9.21e+01
18 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 18000 4.47e+01 9.19e+01
19 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 19000 4.39e+01 9.05e+01
20 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 4 20000 4.33e+01 8.93e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 1000 1.92e-01 3.76e+00
2 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 2000 1.67e+00 7.33e+00
3 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 3000 6.40e+00 1.64e+01
4 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 4000 1.08e+01 2.52e+01
5 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 5000 1.57e+01 3.46e+01
6 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 6000 2.03e+01 4.37e+01
7 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 7000 2.48e+01 5.29e+01
8 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 8000 2.95e+01 6.18e+01
9 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 9000 3.42e+01 7.08e+01
10 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 10000 3.90e+01 8.01e+01
11 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 11000 4.32e+01 8.83e+01
12 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 12000 4.23e+01 8.64e+01
13 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 13000 4.32e+01 8.82e+01
14 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 14000 4.30e+01 8.79e+01
15 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 15000 4.36e+01 8.85e+01
16 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 16000 4.35e+01 8.83e+01
17 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 17000 4.28e+01 8.67e+01
18 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 18000 4.21e+01 8.55e+01
19 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 19000 4.20e+01 8.51e+01
20 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 5 20000 4.16e+01 8.44e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 1000 2.44e-01 3.76e+00
2 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 2000 1.20e+00 6.59e+00
3 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 3000 4.30e+00 1.60e+01
4 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 4000 6.98e+00 2.47e+01
5 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 5000 1.05e+01 3.47e+01
6 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 6000 1.37e+01 4.38e+01
7 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 7000 1.68e+01 5.27e+01
8 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 8000 1.99e+01 6.19e+01
9 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 9000 2.33e+01 7.15e+01
10 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 10000 2.64e+01 8.02e+01
11 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 11000 2.99e+01 8.97e+01
12 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 12000 3.13e+01 9.32e+01
13 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 13000 3.14e+01 9.13e+01
14 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 14000 3.17e+01 9.15e+01
15 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 15000 3.09e+01 8.89e+01
16 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 16000 3.10e+01 8.86e+01
17 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 17000 3.16e+01 8.93e+01
18 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 18000 3.22e+01 8.98e+01
19 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 19000 3.27e+01 9.05e+01
20 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 6 20000 3.19e+01 8.76e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 1000 2.67e-01 4.30e+00
2 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 2000 1.50e+00 7.63e+00
3 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 3000 4.94e+00 1.60e+01
4 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 4000 9.25e+00 2.61e+01
5 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 5000 1.32e+01 3.45e+01
6 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 6000 1.73e+01 4.39e+01
7 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 7000 2.14e+01 5.30e+01
8 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 8000 2.58e+01 6.22e+01
9 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 9000 3.05e+01 7.21e+01
10 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 10000 3.55e+01 8.18e+01
11 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 11000 3.93e+01 8.98e+01
12 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 12000 4.05e+01 9.09e+01
13 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 13000 4.10e+01 9.07e+01
14 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 14000 4.17e+01 9.10e+01
15 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 15000 4.28e+01 9.20e+01
16 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 16000 4.31e+01 9.18e+01
17 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 17000 4.30e+01 9.08e+01
18 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 18000 4.45e+01 9.32e+01
19 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 19000 4.40e+01 9.21e+01
20 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 7 20000 4.44e+01 9.20e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 1000 2.43e-01 3.86e+00
2 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 2000 1.20e+00 6.69e+00
3 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 3000 5.06e+00 1.52e+01
4 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 4000 9.78e+00 2.45e+01
5 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 5000 1.44e+01 3.37e+01
6 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 6000 1.92e+01 4.32e+01
7 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 7000 2.39e+01 5.21e+01
8 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 8000 2.86e+01 6.12e+01
9 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 9000 3.36e+01 7.12e+01
10 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 10000 3.80e+01 7.96e+01
11 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 11000 4.21e+01 8.72e+01
12 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 12000 4.24e+01 8.68e+01
13 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 13000 4.34e+01 8.83e+01
14 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 14000 4.31e+01 8.76e+01
15 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 15000 4.32e+01 8.78e+01
16 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 16000 4.38e+01 8.93e+01
17 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 17000 4.38e+01 8.92e+01
18 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 18000 4.36e+01 8.87e+01
19 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 19000 4.35e+01 8.84e+01
20 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 8 20000 4.20e+01 8.54e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 1000 2.41e-01 4.15e+00
2 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 2000 1.37e+00 6.66e+00
3 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 3000 5.36e+00 1.52e+01
4 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 4000 9.88e+00 2.51e+01
5 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 5000 1.39e+01 3.31e+01
6 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 6000 1.82e+01 4.25e+01
7 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 7000 2.31e+01 5.24e+01
8 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 8000 2.76e+01 6.19e+01
9 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 9000 3.23e+01 7.12e+01
10 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 10000 3.66e+01 7.98e+01
11 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 11000 4.10e+01 8.80e+01
12 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 12000 4.07e+01 8.75e+01
13 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 13000 4.12e+01 8.75e+01
14 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 14000 4.14e+01 8.70e+01
15 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 15000 4.18e+01 8.72e+01
16 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 16000 4.02e+01 8.40e+01
17 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 17000 3.89e+01 8.05e+01
18 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 18000 3.98e+01 8.20e+01
19 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 19000 3.85e+01 7.92e+01
20 DQN 8 8 0 2 2.50e-01 False 2.50e-01 0 0 9 20000 3.93e+01 8.04e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 1000 1.45e-01 4.66e+00
2 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 2000 3.64e-01 7.54e+00
3 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 3000 1.43e+00 1.64e+01
4 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 4000 3.58e+00 2.49e+01
5 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 5000 6.03e+00 3.37e+01
6 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 6000 8.72e+00 4.34e+01
7 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 7000 1.11e+01 5.25e+01
8 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 8000 1.36e+01 6.15e+01
9 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 9000 1.61e+01 7.04e+01
10 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 10000 1.82e+01 8.07e+01
11 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 11000 2.06e+01 8.87e+01
12 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 12000 2.16e+01 8.98e+01
13 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 13000 2.20e+01 8.88e+01
14 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 14000 2.16e+01 8.92e+01
15 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 15000 2.15e+01 8.89e+01
16 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 16000 2.10e+01 8.88e+01
17 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 17000 2.04e+01 8.66e+01
18 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 18000 2.10e+01 8.64e+01
19 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 19000 2.08e+01 8.53e+01
20 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 0 20000 2.03e+01 8.37e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 1000 9.36e-02 3.73e+00
2 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 2000 4.14e-01 8.60e+00
3 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 3000 2.60e+00 1.74e+01
4 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 4000 4.80e+00 2.59e+01
5 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 5000 7.12e+00 3.52e+01
6 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 6000 9.57e+00 4.43e+01
7 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 7000 1.19e+01 5.35e+01
8 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 8000 1.47e+01 6.28e+01
9 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 9000 1.68e+01 7.18e+01
10 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 10000 1.95e+01 8.08e+01
11 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 11000 2.10e+01 8.66e+01
12 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 12000 2.12e+01 8.70e+01
13 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 13000 2.08e+01 8.67e+01
14 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 14000 2.09e+01 8.65e+01
15 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 15000 2.10e+01 8.76e+01
16 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 16000 2.07e+01 8.61e+01
17 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 17000 2.01e+01 8.59e+01
18 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 18000 2.04e+01 8.54e+01
19 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 19000 2.03e+01 8.52e+01
20 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 1 20000 2.11e+01 8.62e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 1000 9.45e-02 3.93e+00
2 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 2000 4.14e-01 6.92e+00
3 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 3000 2.20e+00 1.66e+01
4 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 4000 3.94e+00 2.53e+01
5 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 5000 5.88e+00 3.49e+01
6 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 6000 7.57e+00 4.35e+01
7 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 7000 9.40e+00 5.30e+01
8 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 8000 1.12e+01 6.18e+01
9 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 9000 1.33e+01 7.16e+01
10 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 10000 1.53e+01 8.09e+01
11 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 11000 1.71e+01 8.96e+01
12 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 12000 1.73e+01 8.96e+01
13 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 13000 1.74e+01 8.85e+01
14 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 14000 1.74e+01 8.92e+01
15 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 15000 1.78e+01 8.95e+01
16 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 16000 1.78e+01 8.71e+01
17 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 17000 1.77e+01 8.55e+01
18 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 18000 1.78e+01 8.70e+01
19 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 19000 1.77e+01 8.64e+01
20 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 2 20000 1.84e+01 8.74e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 1000 1.22e-01 4.20e+00
2 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 2000 5.34e-01 8.61e+00
3 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 3000 1.69e+00 1.59e+01
4 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 4000 4.18e+00 2.55e+01
5 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 5000 6.75e+00 3.53e+01
6 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 6000 8.11e+00 4.39e+01
7 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 7000 9.81e+00 5.22e+01
8 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 8000 1.25e+01 6.13e+01
9 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 9000 1.38e+01 7.05e+01
10 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 10000 1.53e+01 8.01e+01
11 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 11000 1.77e+01 8.76e+01
12 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 12000 1.78e+01 8.82e+01
13 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 13000 1.73e+01 8.89e+01
14 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 14000 1.65e+01 8.92e+01
15 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 15000 1.71e+01 8.94e+01
16 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 16000 1.62e+01 8.95e+01
17 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 17000 1.71e+01 9.28e+01
18 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 18000 1.74e+01 9.39e+01
19 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 19000 1.70e+01 9.42e+01
20 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 3 20000 1.68e+01 9.16e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 1000 6.11e-02 3.79e+00
2 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 2000 4.19e-01 7.39e+00
3 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 3000 1.92e+00 1.75e+01
4 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 4000 3.65e+00 2.64e+01
5 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 5000 5.77e+00 3.53e+01
6 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 6000 7.91e+00 4.42e+01
7 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 7000 1.06e+01 5.36e+01
8 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 8000 1.31e+01 6.27e+01
9 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 9000 1.50e+01 7.14e+01
10 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 10000 1.73e+01 8.05e+01
11 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 11000 1.98e+01 8.93e+01
12 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 12000 1.96e+01 8.84e+01
13 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 13000 2.05e+01 8.85e+01
14 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 14000 2.02e+01 8.81e+01
15 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 15000 2.02e+01 8.75e+01
16 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 16000 1.84e+01 8.66e+01
17 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 17000 1.83e+01 8.88e+01
18 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 18000 1.84e+01 8.74e+01
19 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 19000 1.81e+01 8.76e+01
20 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 4 20000 1.75e+01 8.51e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 1000 8.21e-02 3.73e+00
2 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 2000 3.94e-01 7.84e+00
3 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 3000 2.02e+00 1.69e+01
4 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 4000 4.18e+00 2.54e+01
5 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 5000 6.53e+00 3.45e+01
6 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 6000 8.81e+00 4.37e+01
7 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 7000 1.13e+01 5.29e+01
8 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 8000 1.35e+01 6.17e+01
9 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 9000 1.61e+01 7.14e+01
10 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 10000 1.84e+01 8.05e+01
11 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 11000 2.05e+01 8.82e+01
12 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 12000 2.10e+01 8.78e+01
13 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 13000 2.09e+01 8.75e+01
14 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 14000 2.09e+01 8.79e+01
15 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 15000 2.10e+01 8.80e+01
16 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 16000 2.11e+01 8.86e+01
17 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 17000 2.04e+01 8.63e+01
18 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 18000 2.08e+01 8.75e+01
19 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 19000 2.06e+01 8.68e+01
20 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 5 20000 2.07e+01 8.75e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 1000 7.22e-02 3.78e+00
2 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 2000 4.06e-01 7.87e+00
3 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 3000 1.78e+00 1.59e+01
4 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 4000 3.66e+00 2.54e+01
5 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 5000 5.66e+00 3.44e+01
6 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 6000 7.81e+00 4.34e+01
7 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 7000 9.99e+00 5.31e+01
8 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 8000 1.29e+01 6.20e+01
9 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 9000 1.53e+01 7.06e+01
10 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 10000 1.65e+01 7.99e+01
11 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 11000 1.82e+01 8.60e+01
12 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 12000 1.87e+01 8.66e+01
13 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 13000 1.90e+01 8.64e+01
14 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 14000 1.96e+01 8.72e+01
15 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 15000 1.94e+01 8.80e+01
16 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 16000 1.92e+01 8.86e+01
17 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 17000 1.88e+01 8.95e+01
18 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 18000 1.76e+01 8.89e+01
19 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 19000 1.88e+01 9.01e+01
20 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 6 20000 1.93e+01 9.18e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 1000 1.24e-01 4.13e+00
2 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 2000 4.82e-01 8.91e+00
3 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 3000 2.48e+00 1.66e+01
4 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 4000 4.38e+00 2.62e+01
5 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 5000 5.97e+00 3.54e+01
6 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 6000 8.73e+00 4.44e+01
7 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 7000 1.04e+01 5.30e+01
8 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 8000 1.18e+01 6.15e+01
9 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 9000 1.39e+01 7.08e+01
10 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 10000 1.55e+01 8.02e+01
11 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 11000 1.69e+01 8.66e+01
12 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 12000 1.69e+01 8.77e+01
13 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 13000 1.75e+01 8.75e+01
14 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 14000 1.77e+01 8.60e+01
15 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 15000 1.84e+01 8.77e+01
16 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 16000 1.87e+01 8.97e+01
17 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 17000 1.96e+01 9.23e+01
18 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 18000 2.08e+01 9.23e+01
19 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 19000 2.04e+01 9.06e+01
20 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 7 20000 2.03e+01 8.95e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 1000 9.47e-02 4.08e+00
2 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 2000 4.10e-01 7.46e+00
3 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 3000 2.59e+00 1.74e+01
4 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 4000 5.02e+00 2.64e+01
5 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 5000 7.13e+00 3.65e+01
6 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 6000 9.24e+00 4.55e+01
7 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 7000 1.15e+01 5.37e+01
8 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 8000 1.42e+01 6.33e+01
9 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 9000 1.66e+01 7.23e+01
10 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 10000 1.88e+01 8.05e+01
11 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 11000 2.18e+01 8.93e+01
12 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 12000 2.13e+01 8.94e+01
13 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 13000 2.03e+01 8.75e+01
14 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 14000 2.07e+01 8.81e+01
15 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 15000 2.10e+01 8.85e+01
16 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 16000 2.09e+01 8.86e+01
17 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 17000 2.01e+01 8.73e+01
18 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 18000 1.95e+01 8.71e+01
19 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 19000 1.90e+01 8.62e+01
20 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 8 20000 1.87e+01 8.64e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 1000 1.15e-01 3.95e+00
2 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 2000 4.83e-01 8.30e+00
3 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 3000 2.58e+00 1.65e+01
4 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 4000 4.97e+00 2.59e+01
5 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 5000 7.15e+00 3.49e+01
6 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 6000 9.46e+00 4.39e+01
7 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 7000 1.18e+01 5.29e+01
8 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 8000 1.42e+01 6.22e+01
9 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 9000 1.64e+01 7.10e+01
10 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 10000 1.87e+01 8.00e+01
11 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 11000 2.11e+01 9.00e+01
12 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 12000 2.16e+01 9.14e+01
13 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 13000 2.15e+01 9.14e+01
14 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 14000 2.16e+01 9.17e+01
15 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 15000 2.15e+01 9.14e+01
16 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 16000 2.22e+01 9.38e+01
17 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 17000 2.20e+01 9.32e+01
18 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 18000 2.16e+01 9.15e+01
19 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 19000 2.10e+01 8.95e+01
20 DQN 8 8 0 3 2.50e-01 False 2.50e-01 0 0 9 20000 2.09e+01 8.80e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 1000 3.05e-02 3.82e+00
2 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 2000 8.11e-02 5.36e+00
3 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 3000 3.00e-01 1.37e+01
4 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 4000 9.30e-01 2.33e+01
5 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 5000 2.32e+00 3.31e+01
6 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 6000 3.74e+00 4.25e+01
7 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 7000 5.33e+00 5.19e+01
8 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 8000 7.31e+00 6.18e+01
9 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 9000 8.93e+00 7.07e+01
10 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 10000 1.07e+01 7.96e+01
11 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 11000 1.26e+01 8.56e+01
12 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 12000 1.40e+01 8.61e+01
13 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 13000 1.51e+01 8.65e+01
14 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 14000 1.56e+01 8.81e+01
15 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 15000 1.65e+01 9.12e+01
16 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 16000 1.68e+01 9.16e+01
17 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 17000 1.67e+01 9.10e+01
18 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 18000 1.70e+01 9.15e+01
19 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 19000 1.71e+01 9.07e+01
20 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 0 20000 1.71e+01 9.10e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 1000 4.43e-02 3.66e+00
2 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 2000 9.02e-02 8.26e+00
3 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 3000 2.40e-01 1.60e+01
4 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 4000 5.20e-01 2.46e+01
5 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 5000 9.00e-01 3.38e+01
6 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 6000 1.53e+00 4.42e+01
7 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 7000 2.52e+00 5.34e+01
8 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 8000 3.70e+00 6.16e+01
9 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 9000 5.32e+00 7.11e+01
10 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 10000 6.96e+00 7.99e+01
11 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 11000 8.52e+00 8.66e+01
12 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 12000 1.03e+01 8.87e+01
13 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 13000 1.14e+01 9.07e+01
14 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 14000 1.27e+01 9.18e+01
15 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 15000 1.36e+01 9.02e+01
16 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 16000 1.39e+01 8.97e+01
17 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 17000 1.48e+01 8.97e+01
18 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 18000 1.52e+01 9.07e+01
19 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 19000 1.51e+01 8.98e+01
20 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 1 20000 1.44e+01 8.85e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 1000 4.00e-02 4.00e+00
2 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 2000 2.87e-02 5.70e+00
3 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 3000 2.70e-01 1.39e+01
4 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 4000 6.60e-01 2.36e+01
5 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 5000 1.04e+00 3.36e+01
6 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 6000 1.29e+00 4.24e+01
7 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 7000 1.73e+00 5.20e+01
8 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 8000 2.21e+00 6.15e+01
9 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 9000 2.83e+00 7.10e+01
10 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 10000 3.40e+00 7.99e+01
11 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 11000 4.26e+00 8.60e+01
12 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 12000 5.06e+00 8.74e+01
13 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 13000 5.71e+00 8.82e+01
14 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 14000 6.62e+00 8.89e+01
15 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 15000 7.90e+00 8.88e+01
16 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 16000 9.02e+00 8.84e+01
17 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 17000 9.63e+00 8.74e+01
18 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 18000 1.06e+01 8.80e+01
19 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 19000 1.17e+01 8.83e+01
20 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 2 20000 1.26e+01 9.02e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 1000 3.52e-02 3.88e+00
2 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 2000 7.35e-02 4.92e+00
3 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 3000 3.00e-01 1.42e+01
4 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 4000 7.40e-01 2.34e+01
5 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 5000 1.32e+00 3.30e+01
6 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 6000 3.09e+00 4.24e+01
7 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 7000 4.77e+00 5.14e+01
8 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 8000 6.46e+00 6.10e+01
9 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 9000 8.29e+00 6.96e+01
10 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 10000 1.01e+01 7.75e+01
11 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 11000 1.21e+01 8.29e+01
12 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 12000 1.41e+01 8.78e+01
13 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 13000 1.48e+01 8.95e+01
14 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 14000 1.60e+01 8.78e+01
15 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 15000 1.61e+01 8.91e+01
16 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 16000 1.59e+01 8.88e+01
17 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 17000 1.57e+01 8.87e+01
18 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 18000 1.59e+01 8.84e+01
19 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 19000 1.58e+01 8.78e+01
20 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 3 20000 1.48e+01 8.59e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 1000 3.20e-02 3.98e+00
2 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 2000 7.22e-02 5.18e+00
3 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 3000 5.10e-01 1.54e+01
4 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 4000 1.77e+00 2.56e+01
5 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 5000 3.55e+00 3.50e+01
6 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 6000 5.17e+00 4.43e+01
7 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 7000 7.46e+00 5.32e+01
8 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 8000 9.17e+00 6.24e+01
9 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 9000 1.09e+01 7.18e+01
10 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 10000 1.23e+01 8.05e+01
11 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 11000 1.35e+01 8.91e+01
12 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 12000 1.50e+01 9.08e+01
13 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 13000 1.48e+01 9.10e+01
14 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 14000 1.49e+01 9.11e+01
15 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 15000 1.44e+01 9.13e+01
16 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 16000 1.35e+01 9.10e+01
17 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 17000 1.33e+01 8.89e+01
18 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 18000 1.37e+01 8.80e+01
19 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 19000 1.36e+01 8.90e+01
20 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 4 20000 1.45e+01 9.06e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 1000 3.26e-02 3.60e+00
2 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 2000 4.24e-02 4.25e+00
3 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 3000 1.70e-01 1.26e+01
4 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 4000 3.70e-01 2.06e+01
5 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 5000 8.10e-01 3.04e+01
6 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 6000 1.29e+00 3.86e+01
7 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 7000 2.07e+00 4.30e+01
8 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 8000 3.09e+00 4.95e+01
9 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 9000 4.64e+00 5.84e+01
10 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 10000 5.75e+00 6.58e+01
11 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 11000 6.71e+00 7.28e+01
12 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 12000 7.66e+00 8.12e+01
13 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 13000 8.26e+00 8.92e+01
14 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 14000 8.88e+00 8.99e+01
15 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 15000 9.51e+00 9.00e+01
16 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 16000 9.89e+00 8.93e+01
17 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 17000 1.04e+01 9.10e+01
18 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 18000 9.07e+00 8.82e+01
19 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 19000 9.48e+00 8.88e+01
20 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 5 20000 9.64e+00 8.82e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 1000 2.94e-02 4.12e+00
2 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 2000 1.04e-01 6.96e+00
3 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 3000 7.70e-01 1.74e+01
4 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 4000 2.21e+00 2.66e+01
5 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 5000 3.88e+00 3.56e+01
6 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 6000 5.91e+00 4.49e+01
7 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 7000 8.37e+00 5.40e+01
8 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 8000 9.89e+00 6.32e+01
9 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 9000 1.20e+01 7.20e+01
10 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 10000 1.41e+01 8.05e+01
11 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 11000 1.56e+01 8.49e+01
12 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 12000 1.62e+01 8.59e+01
13 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 13000 1.67e+01 8.77e+01
14 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 14000 1.64e+01 8.61e+01
15 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 15000 1.64e+01 8.83e+01
16 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 16000 1.58e+01 8.82e+01
17 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 17000 1.63e+01 8.99e+01
18 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 18000 1.61e+01 9.01e+01
19 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 19000 1.59e+01 8.94e+01
20 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 6 20000 1.59e+01 8.99e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 1000 4.83e-02 3.71e+00
2 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 2000 5.16e-02 6.44e+00
3 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 3000 2.40e-01 1.48e+01
4 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 4000 6.00e-01 2.36e+01
5 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 5000 1.13e+00 3.33e+01
6 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 6000 1.59e+00 4.27e+01
7 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 7000 2.30e+00 5.20e+01
8 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 8000 3.05e+00 6.10e+01
9 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 9000 3.82e+00 6.98e+01
10 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 10000 4.55e+00 7.97e+01
11 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 11000 5.59e+00 8.77e+01
12 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 12000 6.69e+00 8.77e+01
13 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 13000 7.63e+00 8.88e+01
14 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 14000 8.77e+00 8.87e+01
15 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 15000 1.03e+01 9.07e+01
16 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 16000 1.15e+01 9.16e+01
17 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 17000 1.23e+01 9.07e+01
18 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 18000 1.34e+01 9.22e+01
19 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 19000 1.49e+01 9.27e+01
20 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 7 20000 1.56e+01 9.36e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 1000 3.28e-02 4.10e+00
2 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 2000 5.38e-02 5.37e+00
3 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 3000 5.48e-02 4.57e+00
4 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 4000 3.40e-01 1.22e+01
5 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 5000 6.30e-01 2.15e+01
6 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 6000 1.03e+00 3.11e+01
7 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 7000 1.54e+00 4.06e+01
8 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 8000 2.25e+00 4.96e+01
9 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 9000 3.12e+00 5.89e+01
10 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 10000 4.13e+00 6.85e+01
11 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 11000 5.29e+00 7.90e+01
12 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 12000 6.15e+00 8.83e+01
13 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 13000 7.06e+00 9.14e+01
14 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 14000 8.27e+00 9.21e+01
15 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 15000 8.62e+00 9.17e+01
16 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 16000 8.88e+00 9.27e+01
17 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 17000 8.92e+00 9.13e+01
18 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 18000 8.70e+00 8.96e+01
19 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 19000 8.21e+00 8.82e+01
20 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 8 20000 8.41e+00 8.93e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 1000 5.39e-02 4.15e+00
2 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 2000 9.59e-02 6.75e+00
3 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 3000 3.90e-01 1.71e+01
4 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 4000 8.50e-01 2.63e+01
5 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 5000 1.70e+00 3.55e+01
6 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 6000 2.58e+00 4.41e+01
7 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 7000 3.72e+00 5.32e+01
8 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 8000 5.04e+00 6.21e+01
9 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 9000 6.40e+00 7.10e+01
10 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 10000 7.56e+00 7.93e+01
11 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 11000 8.70e+00 8.66e+01
12 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 12000 9.53e+00 8.65e+01
13 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 13000 9.56e+00 8.47e+01
14 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 14000 1.04e+01 8.54e+01
15 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 15000 1.12e+01 8.56e+01
16 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 16000 1.07e+01 8.51e+01
17 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 17000 1.08e+01 8.72e+01
18 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 18000 1.05e+01 8.61e+01
19 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 19000 1.02e+01 8.83e+01
20 DQN 8 8 0 4 2.50e-01 False 2.50e-01 0 0 9 20000 1.03e+01 8.83e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 1000 5.41e-01 4.04e+00
2 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 2000 3.97e+00 7.67e+00
3 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 3000 1.24e+01 1.59e+01
4 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 4000 2.19e+01 2.52e+01
5 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 5000 3.14e+01 3.43e+01
6 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 6000 4.13e+01 4.41e+01
7 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 7000 5.06e+01 5.32e+01
8 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 8000 5.99e+01 6.22e+01
9 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 9000 6.91e+01 7.13e+01
10 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 10000 7.82e+01 8.03e+01
11 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 11000 8.76e+01 8.94e+01
12 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 12000 8.87e+01 9.03e+01
13 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 13000 8.79e+01 8.95e+01
14 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 14000 8.78e+01 8.93e+01
15 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 15000 8.73e+01 8.89e+01
16 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 16000 8.62e+01 8.77e+01
17 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 17000 8.47e+01 8.62e+01
18 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 18000 8.36e+01 8.53e+01
19 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 19000 8.45e+01 8.62e+01
20 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 0 20000 8.32e+01 8.49e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 1000 5.04e-01 4.27e+00
2 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 2000 3.77e+00 7.41e+00
3 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 3000 1.33e+01 1.70e+01
4 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 4000 2.20e+01 2.53e+01
5 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 5000 3.20e+01 3.52e+01
6 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 6000 4.13e+01 4.42e+01
7 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 7000 5.04e+01 5.31e+01
8 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 8000 5.97e+01 6.23e+01
9 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 9000 6.89e+01 7.12e+01
10 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 10000 7.86e+01 8.07e+01
11 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 11000 8.79e+01 8.98e+01
12 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 12000 8.74e+01 8.92e+01
13 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 13000 8.72e+01 8.91e+01
14 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 14000 8.69e+01 8.87e+01
15 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 15000 8.66e+01 8.83e+01
16 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 16000 8.66e+01 8.83e+01
17 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 17000 8.60e+01 8.78e+01
18 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 18000 8.64e+01 8.81e+01
19 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 19000 8.37e+01 8.53e+01
20 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 1 20000 8.43e+01 8.59e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 1000 5.60e-01 4.00e+00
2 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 2000 4.05e+00 8.32e+00
3 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 3000 1.25e+01 1.71e+01
4 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 4000 2.21e+01 2.65e+01
5 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 5000 3.10e+01 3.47e+01
6 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 6000 4.05e+01 4.39e+01
7 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 7000 4.97e+01 5.30e+01
8 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 8000 5.89e+01 6.21e+01
9 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 9000 6.80e+01 7.09e+01
10 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 10000 7.78e+01 8.06e+01
11 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 11000 8.77e+01 9.04e+01
12 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 12000 8.98e+01 9.21e+01
13 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 13000 9.03e+01 9.24e+01
14 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 14000 9.11e+01 9.31e+01
15 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 15000 9.14e+01 9.35e+01
16 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 16000 9.08e+01 9.29e+01
17 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 17000 9.15e+01 9.35e+01
18 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 18000 9.09e+01 9.28e+01
19 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 19000 9.00e+01 9.20e+01
20 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 2 20000 8.95e+01 9.14e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 1000 4.90e-01 3.80e+00
2 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 2000 4.34e+00 8.68e+00
3 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 3000 1.33e+01 1.77e+01
4 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 4000 2.27e+01 2.67e+01
5 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 5000 3.24e+01 3.62e+01
6 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 6000 4.18e+01 4.55e+01
7 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 7000 5.01e+01 5.31e+01
8 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 8000 5.99e+01 6.23e+01
9 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 9000 6.94e+01 7.18e+01
10 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 10000 7.84e+01 8.04e+01
11 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 11000 8.81e+01 8.98e+01
12 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 12000 8.85e+01 9.02e+01
13 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 13000 8.65e+01 8.81e+01
14 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 14000 8.50e+01 8.67e+01
15 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 15000 8.51e+01 8.67e+01
16 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 16000 8.56e+01 8.72e+01
17 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 17000 8.42e+01 8.58e+01
18 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 18000 8.62e+01 8.79e+01
19 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 19000 8.64e+01 8.81e+01
20 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 3 20000 8.48e+01 8.65e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 1000 4.38e-01 3.99e+00
2 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 2000 3.69e+00 7.49e+00
3 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 3000 1.29e+01 1.68e+01
4 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 4000 2.21e+01 2.57e+01
5 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 5000 3.15e+01 3.51e+01
6 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 6000 4.07e+01 4.43e+01
7 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 7000 4.98e+01 5.33e+01
8 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 8000 5.87e+01 6.19e+01
9 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 9000 6.81e+01 7.12e+01
10 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 10000 7.80e+01 8.10e+01
11 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 11000 8.66e+01 8.93e+01
12 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 12000 8.70e+01 8.96e+01
13 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 13000 8.77e+01 9.04e+01
14 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 14000 8.69e+01 8.94e+01
15 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 15000 8.70e+01 8.93e+01
16 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 16000 8.36e+01 8.60e+01
17 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 17000 8.43e+01 8.67e+01
18 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 18000 8.25e+01 8.50e+01
19 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 19000 8.13e+01 8.35e+01
20 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 4 20000 8.17e+01 8.38e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 1000 5.08e-01 4.03e+00
2 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 2000 3.29e+00 6.73e+00
3 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 3000 1.15e+01 1.47e+01
4 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 4000 2.13e+01 2.44e+01
5 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 5000 3.04e+01 3.31e+01
6 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 6000 3.98e+01 4.22e+01
7 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 7000 4.96e+01 5.19e+01
8 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 8000 5.86e+01 6.08e+01
9 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 9000 6.87e+01 7.06e+01
10 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 10000 7.77e+01 7.97e+01
11 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 11000 8.75e+01 8.95e+01
12 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 12000 8.80e+01 8.98e+01
13 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 13000 8.86e+01 9.03e+01
14 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 14000 8.92e+01 9.10e+01
15 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 15000 8.91e+01 9.09e+01
16 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 16000 8.72e+01 8.89e+01
17 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 17000 8.87e+01 9.04e+01
18 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 18000 8.85e+01 9.03e+01
19 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 19000 8.92e+01 9.09e+01
20 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 5 20000 8.75e+01 8.91e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 1000 4.44e-01 4.12e+00
2 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 2000 3.45e+00 7.05e+00
3 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 3000 1.26e+01 1.67e+01
4 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 4000 2.10e+01 2.46e+01
5 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 5000 3.03e+01 3.43e+01
6 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 6000 3.91e+01 4.29e+01
7 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 7000 4.83e+01 5.17e+01
8 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 8000 5.83e+01 6.15e+01
9 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 9000 6.71e+01 7.01e+01
10 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 10000 7.68e+01 7.97e+01
11 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 11000 8.33e+01 8.58e+01
12 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 12000 8.33e+01 8.56e+01
13 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 13000 8.35e+01 8.52e+01
14 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 14000 8.44e+01 8.62e+01
15 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 15000 8.40e+01 8.58e+01
16 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 16000 8.60e+01 8.78e+01
17 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 17000 8.60e+01 8.77e+01
18 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 18000 8.58e+01 8.75e+01
19 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 19000 8.65e+01 8.83e+01
20 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 6 20000 8.61e+01 8.78e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 1000 4.78e-01 3.92e+00
2 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 2000 4.22e+00 8.74e+00
3 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 3000 1.34e+01 1.77e+01
4 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 4000 2.28e+01 2.70e+01
5 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 5000 3.19e+01 3.58e+01
6 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 6000 4.10e+01 4.46e+01
7 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 7000 5.05e+01 5.37e+01
8 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 8000 5.95e+01 6.26e+01
9 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 9000 6.92e+01 7.19e+01
10 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 10000 7.81e+01 8.05e+01
11 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 11000 8.48e+01 8.70e+01
12 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 12000 8.39e+01 8.60e+01
13 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 13000 8.36e+01 8.57e+01
14 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 14000 8.25e+01 8.47e+01
15 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 15000 8.34e+01 8.55e+01
16 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 16000 8.20e+01 8.41e+01
17 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 17000 8.21e+01 8.43e+01
18 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 18000 8.28e+01 8.49e+01
19 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 19000 8.24e+01 8.45e+01
20 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 7 20000 8.39e+01 8.61e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 1000 5.04e-01 4.02e+00
2 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 2000 5.14e+00 9.89e+00
3 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 3000 1.35e+01 1.80e+01
4 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 4000 2.31e+01 2.74e+01
5 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 5000 3.25e+01 3.65e+01
6 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 6000 4.19e+01 4.57e+01
7 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 7000 5.13e+01 5.48e+01
8 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 8000 5.99e+01 6.31e+01
9 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 9000 6.90e+01 7.17e+01
10 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 10000 7.80e+01 8.03e+01
11 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 11000 8.72e+01 8.92e+01
12 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 12000 8.85e+01 9.04e+01
13 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 13000 8.94e+01 9.13e+01
14 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 14000 8.98e+01 9.16e+01
15 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 15000 8.84e+01 9.02e+01
16 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 16000 8.72e+01 8.90e+01
17 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 17000 8.82e+01 9.01e+01
18 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 18000 8.70e+01 8.88e+01
19 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 19000 8.65e+01 8.82e+01
20 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 8 20000 8.62e+01 8.80e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 1000 4.41e-01 3.69e+00
2 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 2000 4.02e+00 8.27e+00
3 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 3000 1.25e+01 1.66e+01
4 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 4000 2.11e+01 2.49e+01
5 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 5000 3.12e+01 3.47e+01
6 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 6000 4.01e+01 4.36e+01
7 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 7000 4.95e+01 5.27e+01
8 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 8000 5.92e+01 6.20e+01
9 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 9000 6.80e+01 7.06e+01
10 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 10000 7.80e+01 8.04e+01
11 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 11000 8.69e+01 8.89e+01
12 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 12000 8.84e+01 9.05e+01
13 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 13000 8.99e+01 9.20e+01
14 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 14000 8.84e+01 9.05e+01
15 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 15000 8.75e+01 8.97e+01
16 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 16000 8.80e+01 9.00e+01
17 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 17000 8.95e+01 9.16e+01
18 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 18000 8.97e+01 9.17e+01
19 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 19000 8.98e+01 9.19e+01
20 DQN 8 8 1 1 2.50e-01 False 2.50e-01 0 0 9 20000 8.82e+01 9.03e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 1000 2.58e-01 4.29e+00
2 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 2000 1.50e+00 8.48e+00
3 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 3000 4.03e+00 1.75e+01
4 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 4000 6.28e+00 2.63e+01
5 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 5000 8.39e+00 3.55e+01
6 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 6000 1.01e+01 4.39e+01
7 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 7000 1.22e+01 5.33e+01
8 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 8000 1.58e+01 6.27e+01
9 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 9000 1.92e+01 7.12e+01
10 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 10000 2.28e+01 8.03e+01
11 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 11000 2.63e+01 8.70e+01
12 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 12000 2.73e+01 8.59e+01
13 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 13000 2.94e+01 8.57e+01
14 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 14000 3.12e+01 8.25e+01
15 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 15000 3.44e+01 8.47e+01
16 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 16000 3.63e+01 8.48e+01
17 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 17000 3.76e+01 8.65e+01
18 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 18000 3.82e+01 8.50e+01
19 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 19000 3.90e+01 8.54e+01
20 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 0 20000 4.02e+01 8.67e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 1000 1.92e-01 3.82e+00
2 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 2000 1.62e+00 8.73e+00
3 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 3000 4.79e+00 1.78e+01
4 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 4000 9.43e+00 2.69e+01
5 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 5000 1.42e+01 3.63e+01
6 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 6000 1.89e+01 4.52e+01
7 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 7000 2.32e+01 5.39e+01
8 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 8000 2.80e+01 6.31e+01
9 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 9000 3.29e+01 7.24e+01
10 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 10000 3.74e+01 8.08e+01
11 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 11000 4.19e+01 8.97e+01
12 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 12000 4.45e+01 9.23e+01
13 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 13000 4.48e+01 9.23e+01
14 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 14000 4.50e+01 9.28e+01
15 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 15000 4.59e+01 9.46e+01
16 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 16000 4.58e+01 9.42e+01
17 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 17000 4.51e+01 9.30e+01
18 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 18000 4.46e+01 9.18e+01
19 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 19000 4.50e+01 9.28e+01
20 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 1 20000 4.41e+01 9.09e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 1000 2.93e-01 4.30e+00
2 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 2000 1.56e+00 8.27e+00
3 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 3000 6.08e+00 1.70e+01
4 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 4000 1.04e+01 2.57e+01
5 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 5000 1.54e+01 3.49e+01
6 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 6000 1.97e+01 4.36e+01
7 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 7000 2.45e+01 5.29e+01
8 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 8000 2.89e+01 6.16e+01
9 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 9000 3.39e+01 7.12e+01
10 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 10000 3.84e+01 8.04e+01
11 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 11000 4.23e+01 8.79e+01
12 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 12000 4.20e+01 8.74e+01
13 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 13000 4.26e+01 8.85e+01
14 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 14000 4.31e+01 8.94e+01
15 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 15000 4.35e+01 9.03e+01
16 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 16000 4.40e+01 9.11e+01
17 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 17000 4.38e+01 9.07e+01
18 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 18000 4.43e+01 9.17e+01
19 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 19000 4.45e+01 9.19e+01
20 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 2 20000 4.48e+01 9.25e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 1000 2.52e-01 3.98e+00
2 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 2000 1.57e+00 6.99e+00
3 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 3000 5.95e+00 1.58e+01
4 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 4000 1.05e+01 2.48e+01
5 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 5000 1.50e+01 3.37e+01
6 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 6000 1.98e+01 4.33e+01
7 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 7000 2.46e+01 5.27e+01
8 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 8000 2.93e+01 6.20e+01
9 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 9000 3.39e+01 7.11e+01
10 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 10000 3.84e+01 8.00e+01
11 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 11000 4.28e+01 8.88e+01
12 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 12000 4.21e+01 8.72e+01
13 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 13000 4.23e+01 8.74e+01
14 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 14000 4.29e+01 8.86e+01
15 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 15000 4.38e+01 9.03e+01
16 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 16000 4.33e+01 8.92e+01
17 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 17000 4.26e+01 8.78e+01
18 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 18000 4.37e+01 9.01e+01
19 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 19000 4.39e+01 9.07e+01
20 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 3 20000 4.40e+01 9.11e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 1000 2.65e-01 4.20e+00
2 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 2000 1.56e+00 7.87e+00
3 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 3000 4.26e+00 1.57e+01
4 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 4000 8.64e+00 2.52e+01
5 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 5000 1.30e+01 3.42e+01
6 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 6000 1.76e+01 4.37e+01
7 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 7000 2.22e+01 5.29e+01
8 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 8000 2.71e+01 6.25e+01
9 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 9000 3.14e+01 7.09e+01
10 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 10000 3.59e+01 8.02e+01
11 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 11000 4.04e+01 8.84e+01
12 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 12000 4.14e+01 8.85e+01
13 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 13000 4.26e+01 9.00e+01
14 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 14000 4.16e+01 8.75e+01
15 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 15000 4.05e+01 8.50e+01
16 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 16000 4.19e+01 8.78e+01
17 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 17000 4.16e+01 8.69e+01
18 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 18000 4.19e+01 8.77e+01
19 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 19000 4.24e+01 8.87e+01
20 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 4 20000 4.25e+01 8.82e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 1000 2.99e-01 4.27e+00
2 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 2000 1.72e+00 8.91e+00
3 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 3000 4.70e+00 1.79e+01
4 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 4000 7.44e+00 2.69e+01
5 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 5000 1.03e+01 3.58e+01
6 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 6000 1.32e+01 4.48e+01
7 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 7000 1.60e+01 5.29e+01
8 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 8000 1.92e+01 6.25e+01
9 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 9000 2.21e+01 7.14e+01
10 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 10000 2.53e+01 8.07e+01
11 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 11000 2.77e+01 8.78e+01
12 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 12000 2.79e+01 8.78e+01
13 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 13000 2.86e+01 8.89e+01
14 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 14000 2.84e+01 8.81e+01
15 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 15000 2.89e+01 8.96e+01
16 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 16000 2.92e+01 9.07e+01
17 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 17000 2.96e+01 9.19e+01
18 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 18000 2.94e+01 9.10e+01
19 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 19000 2.93e+01 9.07e+01
20 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 5 20000 2.89e+01 8.97e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 1000 2.46e-01 3.78e+00
2 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 2000 1.72e+00 7.98e+00
3 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 3000 5.62e+00 1.66e+01
4 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 4000 1.03e+01 2.58e+01
5 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 5000 1.48e+01 3.47e+01
6 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 6000 1.94e+01 4.40e+01
7 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 7000 2.37e+01 5.25e+01
8 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 8000 2.85e+01 6.17e+01
9 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 9000 3.32e+01 7.12e+01
10 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 10000 3.78e+01 8.01e+01
11 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 11000 4.02e+01 8.40e+01
12 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 12000 4.07e+01 8.47e+01
13 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 13000 4.17e+01 8.66e+01
14 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 14000 4.24e+01 8.79e+01
15 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 15000 4.21e+01 8.71e+01
16 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 16000 4.26e+01 8.79e+01
17 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 17000 4.42e+01 9.11e+01
18 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 18000 4.41e+01 9.11e+01
19 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 19000 4.39e+01 9.06e+01
20 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 6 20000 4.37e+01 9.02e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 1000 2.18e-01 4.36e+00
2 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 2000 1.65e+00 7.31e+00
3 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 3000 5.61e+00 1.60e+01
4 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 4000 9.79e+00 2.51e+01
5 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 5000 1.35e+01 3.40e+01
6 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 6000 1.68e+01 4.34e+01
7 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 7000 2.13e+01 5.28e+01
8 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 8000 2.49e+01 6.16e+01
9 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 9000 2.94e+01 7.12e+01
10 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 10000 3.38e+01 8.07e+01
11 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 11000 3.81e+01 8.96e+01
12 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 12000 3.80e+01 8.97e+01
13 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 13000 3.93e+01 9.18e+01
14 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 14000 3.91e+01 9.02e+01
15 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 15000 3.94e+01 8.86e+01
16 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 16000 4.00e+01 8.95e+01
17 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 17000 4.10e+01 9.08e+01
18 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 18000 4.14e+01 9.11e+01
19 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 19000 4.14e+01 9.13e+01
20 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 7 20000 4.10e+01 9.04e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 1000 2.68e-01 4.26e+00
2 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 2000 1.42e+00 7.50e+00
3 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 3000 4.90e+00 1.63e+01
4 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 4000 8.60e+00 2.47e+01
5 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 5000 1.28e+01 3.39e+01
6 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 6000 1.71e+01 4.35e+01
7 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 7000 2.11e+01 5.25e+01
8 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 8000 2.51e+01 6.13e+01
9 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 9000 2.92e+01 7.05e+01
10 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 10000 3.34e+01 7.99e+01
11 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 11000 3.62e+01 8.47e+01
12 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 12000 3.70e+01 8.53e+01
13 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 13000 3.72e+01 8.52e+01
14 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 14000 3.69e+01 8.41e+01
15 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 15000 3.73e+01 8.48e+01
16 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 16000 3.78e+01 8.53e+01
17 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 17000 3.87e+01 8.65e+01
18 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 18000 3.83e+01 8.48e+01
19 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 19000 3.80e+01 8.40e+01
20 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 8 20000 3.75e+01 8.30e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 1000 3.43e-01 4.08e+00
2 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 2000 1.37e+00 8.26e+00
3 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 3000 5.68e+00 1.69e+01
4 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 4000 1.05e+01 2.64e+01
5 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 5000 1.48e+01 3.48e+01
6 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 6000 1.98e+01 4.41e+01
7 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 7000 2.44e+01 5.32e+01
8 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 8000 2.87e+01 6.18e+01
9 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 9000 3.35e+01 7.12e+01
10 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 10000 3.84e+01 8.05e+01
11 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 11000 4.27e+01 8.88e+01
12 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 12000 4.25e+01 8.79e+01
13 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 13000 4.25e+01 8.78e+01
14 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 14000 4.21e+01 8.68e+01
15 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 15000 4.32e+01 8.91e+01
16 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 16000 4.25e+01 8.78e+01
17 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 17000 4.28e+01 8.83e+01
18 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 18000 4.20e+01 8.68e+01
19 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 19000 4.25e+01 8.77e+01
20 DQN 8 8 1 2 2.50e-01 False 2.50e-01 0 0 9 20000 4.29e+01 8.86e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 1000 1.28e-01 4.24e+00
2 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 2000 2.67e-01 6.65e+00
3 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 3000 1.19e+00 1.67e+01
4 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 4000 2.40e+00 2.59e+01
5 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 5000 3.39e+00 3.53e+01
6 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 6000 4.42e+00 4.36e+01
7 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 7000 6.64e+00 5.36e+01
8 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 8000 8.01e+00 6.21e+01
9 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 9000 9.43e+00 7.18e+01
10 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 10000 1.04e+01 8.02e+01
11 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 11000 1.10e+01 9.00e+01
12 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 12000 1.09e+01 8.94e+01
13 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 13000 1.10e+01 8.87e+01
14 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 14000 1.09e+01 8.88e+01
15 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 15000 1.12e+01 8.96e+01
16 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 16000 1.16e+01 9.12e+01
17 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 17000 1.15e+01 9.07e+01
18 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 18000 1.17e+01 9.05e+01
19 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 19000 1.22e+01 9.19e+01
20 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 0 20000 1.34e+01 9.02e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 1000 1.21e-01 3.68e+00
2 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 2000 2.59e-01 7.38e+00
3 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 3000 1.21e+00 1.60e+01
4 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 4000 2.43e+00 2.54e+01
5 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 5000 3.43e+00 3.46e+01
6 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 6000 4.80e+00 4.41e+01
7 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 7000 6.08e+00 5.28e+01
8 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 8000 7.42e+00 6.14e+01
9 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 9000 9.06e+00 7.13e+01
10 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 10000 1.08e+01 7.99e+01
11 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 11000 1.22e+01 8.21e+01
12 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 12000 1.31e+01 8.21e+01
13 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 13000 1.48e+01 8.35e+01
14 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 14000 1.52e+01 8.68e+01
15 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 15000 1.57e+01 8.66e+01
16 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 16000 1.63e+01 8.59e+01
17 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 17000 1.75e+01 8.74e+01
18 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 18000 1.76e+01 8.59e+01
19 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 19000 1.83e+01 8.73e+01
20 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 1 20000 1.89e+01 8.75e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 1000 1.16e-01 4.12e+00
2 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 2000 2.68e-01 7.19e+00
3 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 3000 9.60e-01 1.56e+01
4 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 4000 2.27e+00 2.44e+01
5 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 5000 4.05e+00 3.39e+01
6 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 6000 5.74e+00 4.26e+01
7 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 7000 7.81e+00 5.25e+01
8 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 8000 1.03e+01 6.15e+01
9 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 9000 1.27e+01 7.07e+01
10 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 10000 1.43e+01 7.97e+01
11 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 11000 1.64e+01 8.56e+01
12 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 12000 1.81e+01 8.63e+01
13 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 13000 1.91e+01 8.81e+01
14 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 14000 1.99e+01 8.81e+01
15 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 15000 2.02e+01 8.75e+01
16 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 16000 2.02e+01 8.84e+01
17 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 17000 2.00e+01 8.80e+01
18 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 18000 1.99e+01 8.78e+01
19 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 19000 1.99e+01 8.85e+01
20 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 2 20000 2.08e+01 9.05e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 1000 1.38e-01 4.17e+00
2 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 2000 3.23e-01 7.70e+00
3 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 3000 9.80e-01 1.61e+01
4 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 4000 2.71e+00 2.50e+01
5 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 5000 4.23e+00 3.42e+01
6 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 6000 6.14e+00 4.30e+01
7 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 7000 8.08e+00 5.23e+01
8 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 8000 1.03e+01 6.16e+01
9 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 9000 1.28e+01 7.13e+01
10 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 10000 1.50e+01 8.07e+01
11 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 11000 1.71e+01 8.94e+01
12 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 12000 1.90e+01 9.01e+01
13 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 13000 1.98e+01 9.01e+01
14 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 14000 2.05e+01 8.97e+01
15 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 15000 2.07e+01 9.01e+01
16 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 16000 2.13e+01 8.98e+01
17 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 17000 2.06e+01 8.88e+01
18 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 18000 2.12e+01 8.82e+01
19 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 19000 2.18e+01 8.93e+01
20 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 3 20000 2.21e+01 8.98e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 1000 1.19e-01 4.12e+00
2 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 2000 4.30e-01 7.74e+00
3 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 3000 2.06e+00 1.66e+01
4 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 4000 4.15e+00 2.49e+01
5 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 5000 6.59e+00 3.44e+01
6 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 6000 8.92e+00 4.33e+01
7 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 7000 1.19e+01 5.20e+01
8 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 8000 1.43e+01 6.17e+01
9 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 9000 1.68e+01 7.07e+01
10 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 10000 1.95e+01 8.05e+01
11 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 11000 2.21e+01 8.95e+01
12 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 12000 2.31e+01 8.98e+01
13 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 13000 2.37e+01 9.15e+01
14 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 14000 2.34e+01 9.23e+01
15 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 15000 2.30e+01 9.15e+01
16 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 16000 2.25e+01 9.16e+01
17 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 17000 2.25e+01 9.09e+01
18 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 18000 2.19e+01 9.02e+01
19 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 19000 2.14e+01 9.05e+01
20 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 4 20000 2.20e+01 9.11e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 1000 7.92e-02 3.77e+00
2 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 2000 2.55e-01 7.30e+00
3 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 3000 1.96e+00 1.65e+01
4 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 4000 3.37e+00 2.58e+01
5 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 5000 5.56e+00 3.46e+01
6 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 6000 8.19e+00 4.30e+01
7 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 7000 1.07e+01 5.26e+01
8 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 8000 1.27e+01 6.13e+01
9 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 9000 1.46e+01 7.06e+01
10 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 10000 1.61e+01 8.00e+01
11 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 11000 1.74e+01 8.57e+01
12 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 12000 1.71e+01 8.56e+01
13 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 13000 1.84e+01 8.75e+01
14 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 14000 1.85e+01 8.81e+01
15 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 15000 1.82e+01 8.79e+01
16 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 16000 1.80e+01 8.92e+01
17 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 17000 1.83e+01 9.13e+01
18 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 18000 1.89e+01 9.24e+01
19 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 19000 2.02e+01 9.14e+01
20 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 5 20000 2.13e+01 9.11e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 1000 1.18e-01 4.20e+00
2 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 2000 4.56e-01 7.95e+00
3 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 3000 1.28e+00 1.76e+01
4 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 4000 2.23e+00 2.67e+01
5 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 5000 4.50e+00 3.60e+01
6 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 6000 6.84e+00 4.48e+01
7 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 7000 9.00e+00 5.42e+01
8 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 8000 1.11e+01 6.27e+01
9 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 9000 1.29e+01 7.20e+01
10 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 10000 1.47e+01 8.07e+01
11 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 11000 1.68e+01 8.93e+01
12 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 12000 1.82e+01 9.05e+01
13 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 13000 1.95e+01 9.13e+01
14 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 14000 1.97e+01 9.06e+01
15 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 15000 1.96e+01 8.93e+01
16 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 16000 2.07e+01 8.83e+01
17 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 17000 2.03e+01 8.85e+01
18 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 18000 2.08e+01 8.73e+01
19 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 19000 2.14e+01 8.72e+01
20 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 6 20000 2.10e+01 8.73e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 1000 1.18e-01 3.80e+00
2 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 2000 4.24e-01 7.19e+00
3 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 3000 2.13e+00 1.66e+01
4 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 4000 3.82e+00 2.56e+01
5 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 5000 6.23e+00 3.50e+01
6 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 6000 8.24e+00 4.40e+01
7 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 7000 1.03e+01 5.37e+01
8 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 8000 1.22e+01 6.22e+01
9 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 9000 1.47e+01 7.16e+01
10 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 10000 1.69e+01 7.99e+01
11 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 11000 1.83e+01 8.60e+01
12 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 12000 1.79e+01 8.48e+01
13 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 13000 1.85e+01 8.47e+01
14 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 14000 1.91e+01 8.64e+01
15 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 15000 1.86e+01 8.62e+01
16 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 16000 1.98e+01 8.72e+01
17 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 17000 1.99e+01 8.72e+01
18 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 18000 1.93e+01 8.76e+01
19 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 19000 1.88e+01 8.74e+01
20 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 7 20000 1.90e+01 8.79e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 1000 7.47e-02 3.54e+00
2 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 2000 4.31e-01 7.30e+00
3 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 3000 1.58e+00 1.68e+01
4 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 4000 2.53e+00 2.56e+01
5 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 5000 4.16e+00 3.51e+01
6 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 6000 6.71e+00 4.38e+01
7 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 7000 8.42e+00 5.27e+01
8 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 8000 1.12e+01 6.22e+01
9 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 9000 1.27e+01 7.14e+01
10 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 10000 1.47e+01 8.00e+01
11 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 11000 1.65e+01 8.98e+01
12 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 12000 1.83e+01 9.28e+01
13 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 13000 1.94e+01 9.23e+01
14 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 14000 1.97e+01 8.96e+01
15 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 15000 1.94e+01 8.89e+01
16 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 16000 1.95e+01 8.64e+01
17 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 17000 1.90e+01 8.72e+01
18 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 18000 1.92e+01 8.94e+01
19 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 19000 1.91e+01 8.78e+01
20 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 8 20000 2.02e+01 8.93e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 1000 9.06e-02 3.93e+00
2 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 2000 4.44e-01 7.38e+00
3 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 3000 2.86e+00 1.63e+01
4 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 4000 5.45e+00 2.58e+01
5 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 5000 7.47e+00 3.45e+01
6 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 6000 1.01e+01 4.45e+01
7 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 7000 1.27e+01 5.33e+01
8 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 8000 1.52e+01 6.19e+01
9 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 9000 1.81e+01 7.09e+01
10 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 10000 1.99e+01 8.00e+01
11 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 11000 2.23e+01 8.93e+01
12 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 12000 2.23e+01 9.02e+01
13 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 13000 2.10e+01 8.64e+01
14 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 14000 2.10e+01 8.70e+01
15 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 15000 2.11e+01 8.73e+01
16 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 16000 2.09e+01 8.94e+01
17 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 17000 2.04e+01 8.88e+01
18 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 18000 1.94e+01 8.76e+01
19 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 19000 1.89e+01 8.68e+01
20 DQN 8 8 1 3 2.50e-01 False 2.50e-01 0 0 9 20000 1.83e+01 8.55e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 1000 3.50e-02 3.89e+00
2 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 2000 4.86e-02 6.91e+00
3 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 3000 2.70e-01 1.45e+01
4 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 4000 6.50e-01 2.47e+01
5 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 5000 7.90e-01 3.36e+01
6 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 6000 1.10e+00 4.30e+01
7 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 7000 1.54e+00 5.24e+01
8 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 8000 1.86e+00 6.19e+01
9 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 9000 2.25e+00 7.05e+01
10 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 10000 2.62e+00 7.96e+01
11 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 11000 2.85e+00 8.30e+01
12 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 12000 2.91e+00 8.06e+01
13 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 13000 3.05e+00 7.95e+01
14 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 14000 3.51e+00 8.14e+01
15 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 15000 3.55e+00 8.22e+01
16 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 16000 3.94e+00 8.53e+01
17 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 17000 4.53e+00 8.74e+01
18 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 18000 4.97e+00 8.70e+01
19 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 19000 5.27e+00 8.87e+01
20 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 0 20000 5.48e+00 9.05e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 1000 2.14e-02 3.56e+00
2 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 2000 8.13e-02 6.26e+00
3 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 3000 3.80e-01 1.50e+01
4 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 4000 1.26e+00 2.48e+01
5 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 5000 2.24e+00 3.45e+01
6 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 6000 3.26e+00 4.36e+01
7 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 7000 4.27e+00 5.28e+01
8 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 8000 5.38e+00 6.22e+01
9 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 9000 6.34e+00 7.11e+01
10 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 10000 7.39e+00 8.01e+01
11 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 11000 8.30e+00 8.64e+01
12 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 12000 8.73e+00 8.64e+01
13 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 13000 8.93e+00 8.61e+01
14 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 14000 8.92e+00 8.64e+01
15 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 15000 8.74e+00 8.63e+01
16 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 16000 8.51e+00 8.50e+01
17 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 17000 8.73e+00 8.62e+01
18 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 18000 8.72e+00 8.71e+01
19 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 19000 8.48e+00 8.69e+01
20 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 1 20000 8.46e+00 8.40e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 1000 2.20e-02 4.40e+00
2 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 2000 2.18e-02 4.38e+00
3 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 3000 1.80e-01 1.39e+01
4 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 4000 4.40e-01 2.34e+01
5 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 5000 6.20e-01 3.29e+01
6 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 6000 8.10e-01 4.25e+01
7 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 7000 9.90e-01 5.11e+01
8 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 8000 1.30e+00 6.08e+01
9 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 9000 1.69e+00 7.10e+01
10 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 10000 2.07e+00 8.02e+01
11 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 11000 2.42e+00 8.87e+01
12 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 12000 2.79e+00 8.88e+01
13 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 13000 3.17e+00 9.16e+01
14 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 14000 3.57e+00 9.06e+01
15 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 15000 3.96e+00 9.03e+01
16 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 16000 4.72e+00 9.10e+01
17 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 17000 5.86e+00 8.96e+01
18 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 18000 7.04e+00 8.95e+01
19 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 19000 8.21e+00 8.84e+01
20 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 2 20000 9.20e+00 9.02e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 1000 1.61e-02 4.02e+00
2 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 2000 4.62e-02 5.80e+00
3 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 3000 1.07e-01 8.88e+00
4 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 4000 3.20e-01 1.79e+01
5 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 5000 5.50e-01 2.50e+01
6 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 6000 1.26e+00 3.16e+01
7 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 7000 2.52e+00 4.05e+01
8 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 8000 3.56e+00 4.99e+01
9 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 9000 4.69e+00 5.81e+01
10 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 10000 5.95e+00 6.73e+01
11 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 11000 7.37e+00 7.48e+01
12 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 12000 8.52e+00 7.83e+01
13 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 13000 9.89e+00 8.59e+01
14 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 14000 1.07e+01 8.56e+01
15 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 15000 1.05e+01 8.40e+01
16 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 16000 1.07e+01 8.53e+01
17 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 17000 1.06e+01 8.52e+01
18 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 18000 1.09e+01 8.68e+01
19 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 19000 1.12e+01 8.85e+01
20 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 3 20000 1.10e+01 8.85e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 1000 2.51e-02 4.18e+00
2 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 2000 7.61e-02 5.39e+00
3 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 3000 2.70e-01 1.44e+01
4 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 4000 4.30e-01 2.36e+01
5 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 5000 6.90e-01 3.32e+01
6 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 6000 1.30e+00 4.28e+01
7 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 7000 1.84e+00 5.27e+01
8 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 8000 2.55e+00 6.22e+01
9 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 9000 3.12e+00 7.06e+01
10 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 10000 4.15e+00 7.98e+01
11 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 11000 5.14e+00 8.95e+01
12 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 12000 6.01e+00 8.92e+01
13 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 13000 6.68e+00 8.75e+01
14 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 14000 7.30e+00 8.67e+01
15 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 15000 7.71e+00 8.69e+01
16 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 16000 8.64e+00 8.85e+01
17 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 17000 8.45e+00 8.72e+01
18 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 18000 8.48e+00 8.61e+01
19 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 19000 8.24e+00 8.77e+01
20 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 4 20000 7.89e+00 8.50e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 1000 3.81e-02 4.23e+00
2 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 2000 4.63e-02 3.82e+00
3 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 3000 4.30e-01 1.43e+01
4 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 4000 1.07e+00 2.42e+01
5 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 5000 1.61e+00 3.32e+01
6 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 6000 2.22e+00 4.34e+01
7 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 7000 2.78e+00 5.22e+01
8 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 8000 3.27e+00 6.24e+01
9 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 9000 3.77e+00 7.10e+01
10 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 10000 4.47e+00 7.97e+01
11 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 11000 5.24e+00 8.53e+01
12 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 12000 5.46e+00 8.52e+01
13 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 13000 5.32e+00 8.73e+01
14 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 14000 5.30e+00 8.70e+01
15 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 15000 5.17e+00 8.74e+01
16 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 16000 5.27e+00 8.74e+01
17 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 17000 5.54e+00 8.75e+01
18 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 18000 5.62e+00 8.77e+01
19 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 19000 5.57e+00 8.71e+01
20 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 5 20000 5.39e+00 8.56e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 1000 4.80e-02 4.00e+00
2 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 2000 8.21e-02 7.46e+00
3 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 3000 2.50e-01 1.59e+01
4 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 4000 4.10e-01 2.44e+01
5 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 5000 6.40e-01 3.28e+01
6 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 6000 8.50e-01 4.19e+01
7 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 7000 1.19e+00 5.14e+01
8 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 8000 1.50e+00 6.02e+01
9 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 9000 1.90e+00 6.93e+01
10 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 10000 2.14e+00 7.17e+01
11 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 11000 2.50e+00 7.42e+01
12 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 12000 2.95e+00 8.15e+01
13 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 13000 3.27e+00 8.51e+01
14 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 14000 3.71e+00 8.76e+01
15 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 15000 4.06e+00 8.81e+01
16 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 16000 4.14e+00 8.83e+01
17 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 17000 4.20e+00 8.90e+01
18 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 18000 4.61e+00 8.93e+01
19 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 19000 4.89e+00 8.71e+01
20 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 6 20000 5.33e+00 8.70e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 1000 6.28e-02 4.48e+00
2 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 2000 5.52e-02 6.08e+00
3 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 3000 1.26e-01 9.71e+00
4 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 4000 2.60e-01 1.26e+01
5 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 5000 6.30e-01 2.18e+01
6 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 6000 8.90e-01 3.12e+01
7 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 7000 1.27e+00 4.09e+01
8 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 8000 1.68e+00 4.96e+01
9 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 9000 2.13e+00 5.90e+01
10 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 10000 2.52e+00 6.85e+01
11 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 11000 2.96e+00 7.82e+01
12 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 12000 3.38e+00 8.54e+01
13 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 13000 3.69e+00 8.72e+01
14 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 14000 3.64e+00 8.79e+01
15 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 15000 3.80e+00 8.62e+01
16 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 16000 4.20e+00 8.93e+01
17 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 17000 4.16e+00 8.84e+01
18 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 18000 4.24e+00 8.93e+01
19 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 19000 4.67e+00 8.94e+01
20 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 7 20000 5.02e+00 8.91e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 1000 3.61e-02 4.02e+00
2 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 2000 1.38e-01 7.15e+00
3 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 3000 1.59e+00 1.77e+01
4 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 4000 3.21e+00 2.61e+01
5 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 5000 5.32e+00 3.60e+01
6 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 6000 7.20e+00 4.46e+01
7 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 7000 8.86e+00 5.31e+01
8 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 8000 1.06e+01 6.21e+01
9 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 9000 1.25e+01 7.07e+01
10 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 10000 1.42e+01 8.00e+01
11 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 11000 1.55e+01 8.42e+01
12 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 12000 1.48e+01 8.34e+01
13 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 13000 1.54e+01 8.54e+01
14 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 14000 1.60e+01 8.70e+01
15 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 15000 1.54e+01 8.64e+01
16 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 16000 1.55e+01 8.73e+01
17 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 17000 1.50e+01 8.69e+01
18 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 18000 1.55e+01 8.86e+01
19 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 19000 1.59e+01 8.89e+01
20 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 8 20000 1.60e+01 9.04e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 1000 3.08e-02 4.39e+00
2 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 2000 4.96e-02 7.08e+00
3 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 3000 1.40e-01 1.10e+01
4 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 4000 3.10e-01 2.08e+01
5 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 5000 4.10e-01 2.97e+01
6 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 6000 6.10e-01 3.60e+01
7 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 7000 8.50e-01 4.46e+01
8 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 8000 1.15e+00 5.37e+01
9 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 9000 1.51e+00 6.31e+01
10 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 10000 1.98e+00 7.25e+01
11 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 11000 2.78e+00 8.20e+01
12 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 12000 3.11e+00 8.82e+01
13 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 13000 3.63e+00 8.92e+01
14 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 14000 4.02e+00 9.04e+01
15 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 15000 4.51e+00 9.18e+01
16 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 16000 4.66e+00 8.99e+01
17 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 17000 4.99e+00 8.96e+01
18 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 18000 5.06e+00 8.98e+01
19 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 19000 4.88e+00 8.81e+01
20 DQN 8 8 1 4 2.50e-01 False 2.50e-01 0 0 9 20000 5.18e+00 8.94e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 1000 3.07e-01 3.73e+00
2 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 2000 2.77e+00 8.21e+00
3 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 3000 1.10e+01 1.59e+01
4 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 4000 2.09e+01 2.60e+01
5 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 5000 3.05e+01 3.55e+01
6 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 6000 3.95e+01 4.45e+01
7 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 7000 4.90e+01 5.33e+01
8 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 8000 5.85e+01 6.25e+01
9 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 9000 6.75e+01 7.11e+01
10 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 10000 7.68e+01 8.02e+01
11 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 11000 8.67e+01 8.98e+01
12 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 12000 8.42e+01 8.70e+01
13 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 13000 8.62e+01 8.88e+01
14 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 14000 8.74e+01 9.02e+01
15 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 15000 8.58e+01 8.84e+01
16 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 16000 8.70e+01 8.98e+01
17 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 17000 8.78e+01 9.05e+01
18 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 18000 8.65e+01 8.93e+01
19 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 19000 8.66e+01 8.95e+01
20 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 0 20000 8.56e+01 8.85e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 1000 3.57e-01 3.68e+00
2 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 2000 2.88e+00 7.94e+00
3 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 3000 1.13e+01 1.65e+01
4 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 4000 2.01e+01 2.49e+01
5 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 5000 2.96e+01 3.44e+01
6 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 6000 3.96e+01 4.46e+01
7 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 7000 4.84e+01 5.30e+01
8 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 8000 5.84e+01 6.26e+01
9 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 9000 6.73e+01 7.12e+01
10 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 10000 7.68e+01 8.06e+01
11 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 11000 8.61e+01 8.97e+01
12 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 12000 8.74e+01 9.05e+01
13 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 13000 8.76e+01 9.08e+01
14 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 14000 8.61e+01 8.92e+01
15 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 15000 8.68e+01 9.00e+01
16 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 16000 8.67e+01 8.97e+01
17 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 17000 8.86e+01 9.17e+01
18 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 18000 9.09e+01 9.39e+01
19 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 19000 8.87e+01 9.17e+01
20 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 1 20000 8.73e+01 9.02e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 1000 3.82e-01 4.04e+00
2 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 2000 3.15e+00 8.06e+00
3 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 3000 1.15e+01 1.66e+01
4 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 4000 2.12e+01 2.62e+01
5 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 5000 3.01e+01 3.47e+01
6 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 6000 3.86e+01 4.29e+01
7 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 7000 4.81e+01 5.24e+01
8 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 8000 5.75e+01 6.18e+01
9 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 9000 6.69e+01 7.08e+01
10 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 10000 7.62e+01 7.99e+01
11 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 11000 8.30e+01 8.65e+01
12 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 12000 8.48e+01 8.83e+01
13 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 13000 8.65e+01 8.98e+01
14 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 14000 8.60e+01 8.93e+01
15 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 15000 8.89e+01 9.21e+01
16 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 16000 8.80e+01 9.12e+01
17 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 17000 8.70e+01 9.03e+01
18 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 18000 8.78e+01 9.09e+01
19 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 19000 8.59e+01 8.91e+01
20 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 2 20000 8.50e+01 8.83e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 1000 3.65e-01 3.83e+00
2 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 2000 3.09e+00 7.78e+00
3 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 3000 1.20e+01 1.69e+01
4 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 4000 2.15e+01 2.63e+01
5 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 5000 3.05e+01 3.53e+01
6 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 6000 3.98e+01 4.43e+01
7 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 7000 4.86e+01 5.36e+01
8 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 8000 5.74e+01 6.21e+01
9 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 9000 6.66e+01 7.12e+01
10 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 10000 7.60e+01 8.04e+01
11 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 11000 8.52e+01 8.97e+01
12 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 12000 8.60e+01 9.00e+01
13 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 13000 8.56e+01 8.97e+01
14 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 14000 8.40e+01 8.81e+01
15 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 15000 8.40e+01 8.81e+01
16 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 16000 8.43e+01 8.81e+01
17 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 17000 8.54e+01 8.91e+01
18 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 18000 8.64e+01 9.02e+01
19 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 19000 8.75e+01 9.14e+01
20 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 3 20000 8.82e+01 9.22e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 1000 3.69e-01 3.91e+00
2 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 2000 2.65e+00 7.72e+00
3 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 3000 1.18e+01 1.67e+01
4 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 4000 2.07e+01 2.55e+01
5 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 5000 3.05e+01 3.52e+01
6 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 6000 3.94e+01 4.37e+01
7 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 7000 4.95e+01 5.36e+01
8 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 8000 5.77e+01 6.15e+01
9 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 9000 6.71e+01 7.07e+01
10 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 10000 7.64e+01 7.97e+01
11 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 11000 8.44e+01 8.76e+01
12 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 12000 8.64e+01 8.97e+01
13 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 13000 8.62e+01 8.93e+01
14 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 14000 8.76e+01 9.08e+01
15 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 15000 8.91e+01 9.23e+01
16 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 16000 8.88e+01 9.20e+01
17 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 17000 9.08e+01 9.41e+01
18 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 18000 9.29e+01 9.62e+01
19 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 19000 9.12e+01 9.47e+01
20 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 4 20000 9.01e+01 9.35e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 1000 3.88e-01 4.25e+00
2 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 2000 3.16e+00 8.12e+00
3 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 3000 1.19e+01 1.65e+01
4 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 4000 2.10e+01 2.56e+01
5 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 5000 3.06e+01 3.50e+01
6 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 6000 3.95e+01 4.35e+01
7 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 7000 4.83e+01 5.21e+01
8 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 8000 5.78e+01 6.13e+01
9 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 9000 6.71e+01 7.11e+01
10 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 10000 7.65e+01 8.04e+01
11 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 11000 8.29e+01 8.67e+01
12 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 12000 8.51e+01 8.89e+01
13 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 13000 8.59e+01 8.97e+01
14 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 14000 8.66e+01 9.03e+01
15 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 15000 8.68e+01 9.03e+01
16 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 16000 8.68e+01 9.04e+01
17 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 17000 8.78e+01 9.14e+01
18 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 18000 8.74e+01 9.03e+01
19 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 19000 8.68e+01 8.97e+01
20 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 5 20000 8.89e+01 9.19e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 1000 3.47e-01 4.08e+00
2 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 2000 3.27e+00 8.22e+00
3 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 3000 1.19e+01 1.67e+01
4 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 4000 2.11e+01 2.61e+01
5 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 5000 3.05e+01 3.52e+01
6 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 6000 3.99e+01 4.43e+01
7 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 7000 4.95e+01 5.36e+01
8 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 8000 5.84e+01 6.25e+01
9 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 9000 6.76e+01 7.17e+01
10 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 10000 7.67e+01 8.05e+01
11 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 11000 8.68e+01 9.04e+01
12 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 12000 8.95e+01 9.28e+01
13 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 13000 8.91e+01 9.23e+01
14 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 14000 9.05e+01 9.35e+01
15 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 15000 8.93e+01 9.24e+01
16 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 16000 8.88e+01 9.20e+01
17 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 17000 8.69e+01 9.00e+01
18 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 18000 8.54e+01 8.85e+01
19 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 19000 8.61e+01 8.93e+01
20 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 6 20000 8.48e+01 8.80e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 1000 3.47e-01 3.82e+00
2 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 2000 2.71e+00 7.26e+00
3 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 3000 1.18e+01 1.63e+01
4 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 4000 2.03e+01 2.51e+01
5 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 5000 2.93e+01 3.42e+01
6 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 6000 3.82e+01 4.30e+01
7 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 7000 4.79e+01 5.26e+01
8 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 8000 5.76e+01 6.21e+01
9 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 9000 6.69e+01 7.13e+01
10 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 10000 7.64e+01 8.07e+01
11 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 11000 8.37e+01 8.78e+01
12 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 12000 8.32e+01 8.72e+01
13 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 13000 8.60e+01 8.93e+01
14 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 14000 8.54e+01 8.86e+01
15 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 15000 8.60e+01 8.92e+01
16 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 16000 8.60e+01 8.92e+01
17 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 17000 8.46e+01 8.76e+01
18 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 18000 8.35e+01 8.65e+01
19 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 19000 8.49e+01 8.79e+01
20 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 7 20000 8.64e+01 8.94e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 1000 4.29e-01 4.16e+00
2 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 2000 3.03e+00 7.91e+00
3 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 3000 1.24e+01 1.74e+01
4 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 4000 2.17e+01 2.64e+01
5 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 5000 3.04e+01 3.47e+01
6 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 6000 4.02e+01 4.44e+01
7 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 7000 4.95e+01 5.37e+01
8 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 8000 5.83e+01 6.22e+01
9 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 9000 6.78e+01 7.14e+01
10 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 10000 7.69e+01 8.02e+01
11 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 11000 8.53e+01 8.81e+01
12 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 12000 8.48e+01 8.75e+01
13 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 13000 8.39e+01 8.66e+01
14 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 14000 8.09e+01 8.36e+01
15 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 15000 8.08e+01 8.35e+01
16 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 16000 8.09e+01 8.35e+01
17 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 17000 8.25e+01 8.52e+01
18 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 18000 8.33e+01 8.60e+01
19 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 19000 8.48e+01 8.75e+01
20 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 8 20000 8.40e+01 8.67e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 1000 4.09e-01 3.91e+00
2 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 2000 3.08e+00 8.03e+00
3 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 3000 1.24e+01 1.70e+01
4 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 4000 2.08e+01 2.60e+01
5 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 5000 2.98e+01 3.49e+01
6 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 6000 3.86e+01 4.34e+01
7 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 7000 4.86e+01 5.29e+01
8 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 8000 5.75e+01 6.17e+01
9 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 9000 6.69e+01 7.12e+01
10 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 10000 7.58e+01 8.01e+01
11 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 11000 8.29e+01 8.69e+01
12 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 12000 8.37e+01 8.78e+01
13 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 13000 8.38e+01 8.69e+01
14 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 14000 8.47e+01 8.80e+01
15 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 15000 8.53e+01 8.87e+01
16 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 16000 8.58e+01 8.91e+01
17 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 17000 8.60e+01 8.93e+01
18 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 18000 8.45e+01 8.77e+01
19 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 19000 8.58e+01 8.90e+01
20 DQN 8 8 2 1 2.50e-01 False 2.50e-01 0 0 9 20000 8.54e+01 8.85e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 1000 1.60e-01 3.82e+00
2 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 2000 9.22e-01 7.63e+00
3 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 3000 4.09e+00 1.62e+01
4 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 4000 8.78e+00 2.52e+01
5 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 5000 1.39e+01 3.50e+01
6 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 6000 1.86e+01 4.40e+01
7 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 7000 2.31e+01 5.28e+01
8 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 8000 2.82e+01 6.26e+01
9 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 9000 3.30e+01 7.20e+01
10 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 10000 3.73e+01 8.02e+01
11 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 11000 4.17e+01 8.85e+01
12 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 12000 4.23e+01 8.78e+01
13 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 13000 4.26e+01 8.82e+01
14 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 14000 4.25e+01 8.81e+01
15 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 15000 4.20e+01 8.70e+01
16 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 16000 4.08e+01 8.48e+01
17 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 17000 3.99e+01 8.36e+01
18 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 18000 3.91e+01 8.19e+01
19 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 19000 4.00e+01 8.38e+01
20 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 0 20000 4.02e+01 8.42e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 1000 2.22e-01 4.03e+00
2 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 2000 9.87e-01 6.53e+00
3 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 3000 3.62e+00 1.52e+01
4 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 4000 7.20e+00 2.51e+01
5 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 5000 9.10e+00 3.42e+01
6 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 6000 1.24e+01 4.39e+01
7 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 7000 1.62e+01 5.26e+01
8 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 8000 2.02e+01 6.18e+01
9 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 9000 2.47e+01 7.15e+01
10 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 10000 2.90e+01 8.06e+01
11 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 11000 3.36e+01 8.98e+01
12 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 12000 3.59e+01 9.13e+01
13 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 13000 3.70e+01 9.19e+01
14 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 14000 3.98e+01 9.20e+01
15 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 15000 4.22e+01 9.39e+01
16 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 16000 4.33e+01 9.38e+01
17 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 17000 4.39e+01 9.36e+01
18 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 18000 4.46e+01 9.43e+01
19 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 19000 4.48e+01 9.43e+01
20 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 1 20000 4.46e+01 9.34e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 1000 1.77e-01 3.69e+00
2 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 2000 9.35e-01 8.06e+00
3 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 3000 3.76e+00 1.69e+01
4 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 4000 8.36e+00 2.59e+01
5 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 5000 1.30e+01 3.50e+01
6 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 6000 1.77e+01 4.40e+01
7 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 7000 2.26e+01 5.37e+01
8 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 8000 2.71e+01 6.23e+01
9 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 9000 3.16e+01 7.15e+01
10 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 10000 3.60e+01 7.98e+01
11 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 11000 4.05e+01 8.80e+01
12 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 12000 4.19e+01 8.80e+01
13 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 13000 4.03e+01 8.49e+01
14 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 14000 4.04e+01 8.50e+01
15 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 15000 3.98e+01 8.37e+01
16 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 16000 4.03e+01 8.50e+01
17 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 17000 4.07e+01 8.53e+01
18 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 18000 4.07e+01 8.54e+01
19 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 19000 4.02e+01 8.48e+01
20 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 2 20000 4.08e+01 8.62e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 1000 1.40e-01 3.77e+00
2 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 2000 9.52e-01 7.87e+00
3 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 3000 3.84e+00 1.64e+01
4 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 4000 6.97e+00 2.56e+01
5 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 5000 1.01e+01 3.52e+01
6 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 6000 1.32e+01 4.40e+01
7 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 7000 1.61e+01 5.26e+01
8 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 8000 1.91e+01 6.14e+01
9 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 9000 2.25e+01 7.15e+01
10 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 10000 2.53e+01 7.98e+01
11 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 11000 2.75e+01 8.59e+01
12 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 12000 2.76e+01 8.61e+01
13 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 13000 2.76e+01 8.60e+01
14 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 14000 2.73e+01 8.59e+01
15 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 15000 2.73e+01 8.59e+01
16 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 16000 2.63e+01 8.28e+01
17 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 17000 2.63e+01 8.29e+01
18 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 18000 2.67e+01 8.40e+01
19 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 19000 2.72e+01 8.56e+01
20 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 3 20000 2.71e+01 8.50e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 1000 2.41e-01 4.15e+00
2 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 2000 1.23e+00 7.60e+00
3 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 3000 3.35e+00 1.66e+01
4 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 4000 6.19e+00 2.50e+01
5 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 5000 8.31e+00 3.43e+01
6 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 6000 1.17e+01 4.34e+01
7 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 7000 1.56e+01 5.30e+01
8 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 8000 1.85e+01 6.17e+01
9 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 9000 2.08e+01 7.06e+01
10 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 10000 2.34e+01 8.02e+01
11 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 11000 2.52e+01 8.76e+01
12 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 12000 2.57e+01 8.95e+01
13 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 13000 2.57e+01 8.88e+01
14 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 14000 2.56e+01 8.56e+01
15 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 15000 2.39e+01 8.53e+01
16 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 16000 2.26e+01 8.56e+01
17 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 17000 2.26e+01 8.49e+01
18 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 18000 2.25e+01 8.59e+01
19 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 19000 2.37e+01 8.84e+01
20 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 4 20000 2.48e+01 8.86e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 1000 2.44e-01 4.18e+00
2 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 2000 1.23e+00 7.57e+00
3 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 3000 4.57e+00 1.58e+01
4 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 4000 7.96e+00 2.49e+01
5 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 5000 1.16e+01 3.42e+01
6 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 6000 1.52e+01 4.34e+01
7 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 7000 1.87e+01 5.27e+01
8 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 8000 2.27e+01 6.18e+01
9 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 9000 2.61e+01 7.15e+01
10 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 10000 3.00e+01 8.05e+01
11 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 11000 3.20e+01 8.68e+01
12 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 12000 3.22e+01 8.61e+01
13 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 13000 3.21e+01 8.57e+01
14 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 14000 3.22e+01 8.47e+01
15 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 15000 3.16e+01 8.21e+01
16 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 16000 3.19e+01 8.23e+01
17 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 17000 3.21e+01 8.13e+01
18 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 18000 3.28e+01 8.28e+01
19 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 19000 3.38e+01 8.39e+01
20 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 5 20000 3.44e+01 8.58e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 1000 2.19e-01 4.20e+00
2 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 2000 7.70e-01 6.79e+00
3 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 3000 3.85e+00 1.54e+01
4 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 4000 5.54e+00 2.44e+01
5 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 5000 8.02e+00 3.35e+01
6 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 6000 1.16e+01 4.34e+01
7 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 7000 1.55e+01 5.25e+01
8 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 8000 1.86e+01 6.11e+01
9 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 9000 2.23e+01 7.05e+01
10 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 10000 2.61e+01 7.99e+01
11 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 11000 2.69e+01 7.93e+01
12 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 12000 2.96e+01 8.13e+01
13 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 13000 3.20e+01 8.21e+01
14 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 14000 3.35e+01 8.32e+01
15 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 15000 3.55e+01 8.62e+01
16 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 16000 3.63e+01 8.68e+01
17 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 17000 3.75e+01 8.67e+01
18 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 18000 3.93e+01 8.93e+01
19 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 19000 4.09e+01 9.04e+01
20 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 6 20000 4.28e+01 9.34e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 1000 1.54e-01 3.74e+00
2 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 2000 1.04e+00 8.45e+00
3 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 3000 2.77e+00 1.65e+01
4 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 4000 5.62e+00 2.62e+01
5 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 5000 7.75e+00 3.48e+01
6 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 6000 9.75e+00 4.42e+01
7 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 7000 1.22e+01 5.30e+01
8 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 8000 1.48e+01 6.25e+01
9 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 9000 1.67e+01 7.08e+01
10 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 10000 1.85e+01 7.96e+01
11 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 11000 2.12e+01 8.60e+01
12 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 12000 2.18e+01 8.60e+01
13 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 13000 2.15e+01 8.67e+01
14 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 14000 2.13e+01 8.75e+01
15 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 15000 2.23e+01 8.73e+01
16 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 16000 2.03e+01 8.71e+01
17 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 17000 2.02e+01 8.88e+01
18 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 18000 2.01e+01 9.04e+01
19 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 19000 1.95e+01 9.06e+01
20 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 7 20000 1.79e+01 9.17e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 1000 1.77e-01 4.31e+00
2 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 2000 9.43e-01 8.19e+00
3 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 3000 4.89e+00 1.76e+01
4 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 4000 9.37e+00 2.70e+01
5 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 5000 1.36e+01 3.58e+01
6 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 6000 1.82e+01 4.43e+01
7 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 7000 2.28e+01 5.33e+01
8 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 8000 2.78e+01 6.29e+01
9 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 9000 3.23e+01 7.16e+01
10 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 10000 3.70e+01 8.07e+01
11 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 11000 4.07e+01 8.72e+01
12 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 12000 4.10e+01 8.66e+01
13 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 13000 4.06e+01 8.60e+01
14 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 14000 4.12e+01 8.71e+01
15 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 15000 4.01e+01 8.50e+01
16 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 16000 4.01e+01 8.47e+01
17 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 17000 4.10e+01 8.64e+01
18 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 18000 4.10e+01 8.63e+01
19 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 19000 4.24e+01 8.92e+01
20 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 8 20000 4.24e+01 8.89e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 1000 1.82e-01 4.55e+00
2 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 2000 9.92e-01 8.32e+00
3 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 3000 5.05e+00 1.79e+01
4 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 4000 9.54e+00 2.64e+01
5 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 5000 1.42e+01 3.53e+01
6 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 6000 1.87e+01 4.47e+01
7 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 7000 2.34e+01 5.38e+01
8 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 8000 2.82e+01 6.28e+01
9 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 9000 3.26e+01 7.17e+01
10 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 10000 3.74e+01 8.10e+01
11 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 11000 4.19e+01 8.98e+01
12 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 12000 4.44e+01 9.33e+01
13 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 13000 4.47e+01 9.42e+01
14 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 14000 4.38e+01 9.26e+01
15 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 15000 4.35e+01 9.13e+01
16 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 16000 4.33e+01 9.10e+01
17 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 17000 4.32e+01 9.10e+01
18 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 18000 4.30e+01 9.03e+01
19 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 19000 4.24e+01 8.90e+01
20 DQN 8 8 2 2 2.50e-01 False 2.50e-01 0 0 9 20000 4.24e+01 8.91e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 1000 7.11e-02 3.95e+00
2 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 2000 3.20e-01 7.98e+00
3 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 3000 1.13e+00 1.66e+01
4 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 4000 2.09e+00 2.58e+01
5 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 5000 3.18e+00 3.51e+01
6 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 6000 4.57e+00 4.46e+01
7 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 7000 6.64e+00 5.33e+01
8 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 8000 8.86e+00 6.26e+01
9 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 9000 1.05e+01 7.14e+01
10 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 10000 1.19e+01 7.98e+01
11 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 11000 1.39e+01 8.91e+01
12 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 12000 1.46e+01 8.77e+01
13 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 13000 1.56e+01 8.80e+01
14 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 14000 1.67e+01 8.85e+01
15 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 15000 1.73e+01 8.80e+01
16 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 16000 1.70e+01 8.77e+01
17 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 17000 1.75e+01 8.90e+01
18 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 18000 1.80e+01 8.88e+01
19 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 19000 1.87e+01 8.92e+01
20 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 0 20000 1.95e+01 8.92e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 1000 1.20e-01 3.98e+00
2 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 2000 3.69e-01 8.19e+00
3 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 3000 8.30e-01 1.77e+01
4 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 4000 1.52e+00 2.58e+01
5 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 5000 2.81e+00 3.50e+01
6 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 6000 4.00e+00 4.39e+01
7 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 7000 5.12e+00 5.35e+01
8 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 8000 6.64e+00 6.25e+01
9 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 9000 8.43e+00 7.14e+01
10 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 10000 1.04e+01 8.07e+01
11 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 11000 1.16e+01 8.70e+01
12 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 12000 1.25e+01 8.67e+01
13 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 13000 1.34e+01 8.78e+01
14 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 14000 1.34e+01 8.75e+01
15 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 15000 1.41e+01 8.90e+01
16 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 16000 1.47e+01 8.94e+01
17 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 17000 1.44e+01 8.94e+01
18 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 18000 1.38e+01 8.93e+01
19 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 19000 1.34e+01 9.08e+01
20 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 1 20000 1.38e+01 8.87e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 1000 7.72e-02 4.06e+00
2 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 2000 3.17e-01 7.20e+00
3 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 3000 9.80e-01 1.58e+01
4 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 4000 1.68e+00 2.57e+01
5 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 5000 3.24e+00 3.42e+01
6 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 6000 5.47e+00 4.39e+01
7 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 7000 7.53e+00 5.23e+01
8 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 8000 9.78e+00 6.17e+01
9 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 9000 1.16e+01 7.11e+01
10 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 10000 1.32e+01 8.01e+01
11 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 11000 1.51e+01 8.80e+01
12 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 12000 1.64e+01 8.76e+01
13 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 13000 1.79e+01 9.05e+01
14 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 14000 1.71e+01 8.77e+01
15 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 15000 1.72e+01 8.90e+01
16 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 16000 1.68e+01 8.95e+01
17 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 17000 1.64e+01 8.85e+01
18 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 18000 1.73e+01 9.07e+01
19 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 19000 1.74e+01 9.08e+01
20 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 2 20000 1.64e+01 8.78e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 1000 1.01e-01 3.86e+00
2 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 2000 3.66e-01 8.14e+00
3 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 3000 1.41e+00 1.75e+01
4 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 4000 2.31e+00 2.62e+01
5 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 5000 3.35e+00 3.53e+01
6 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 6000 4.53e+00 4.39e+01
7 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 7000 5.73e+00 5.31e+01
8 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 8000 6.84e+00 6.22e+01
9 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 9000 8.49e+00 7.17e+01
10 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 10000 9.91e+00 8.03e+01
11 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 11000 1.16e+01 8.85e+01
12 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 12000 1.19e+01 8.97e+01
13 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 13000 1.25e+01 8.92e+01
14 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 14000 1.25e+01 8.80e+01
15 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 15000 1.25e+01 8.74e+01
16 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 16000 1.30e+01 8.83e+01
17 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 17000 1.31e+01 8.80e+01
18 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 18000 1.34e+01 8.92e+01
19 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 19000 1.34e+01 8.93e+01
20 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 3 20000 1.34e+01 8.91e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 1000 4.20e-02 3.81e+00
2 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 2000 2.83e-01 7.21e+00
3 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 3000 8.60e-01 1.62e+01
4 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 4000 1.54e+00 2.56e+01
5 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 5000 3.12e+00 3.52e+01
6 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 6000 4.28e+00 4.42e+01
7 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 7000 5.09e+00 5.32e+01
8 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 8000 6.40e+00 6.14e+01
9 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 9000 7.01e+00 7.08e+01
10 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 10000 8.19e+00 7.92e+01
11 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 11000 9.30e+00 8.36e+01
12 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 12000 1.03e+01 8.51e+01
13 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 13000 1.02e+01 8.31e+01
14 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 14000 1.01e+01 8.29e+01
15 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 15000 1.05e+01 8.45e+01
16 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 16000 1.08e+01 8.53e+01
17 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 17000 1.12e+01 8.82e+01
18 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 18000 1.12e+01 8.62e+01
19 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 19000 1.14e+01 8.61e+01
20 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 4 20000 1.09e+01 8.59e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 1000 6.64e-02 3.87e+00
2 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 2000 1.71e-01 6.23e+00
3 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 3000 5.00e-01 1.49e+01
4 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 4000 2.02e+00 2.41e+01
5 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 5000 4.13e+00 3.36e+01
6 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 6000 6.40e+00 4.31e+01
7 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 7000 8.68e+00 5.26e+01
8 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 8000 1.10e+01 6.19e+01
9 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 9000 1.31e+01 7.06e+01
10 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 10000 1.52e+01 7.98e+01
11 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 11000 1.77e+01 8.67e+01
12 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 12000 1.96e+01 8.87e+01
13 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 13000 1.94e+01 8.54e+01
14 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 14000 1.97e+01 8.71e+01
15 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 15000 2.00e+01 8.81e+01
16 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 16000 1.96e+01 8.59e+01
17 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 17000 1.98e+01 8.81e+01
18 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 18000 2.03e+01 8.96e+01
19 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 19000 1.97e+01 8.75e+01
20 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 5 20000 1.91e+01 8.72e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 1000 7.05e-02 4.11e+00
2 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 2000 2.00e-01 6.96e+00
3 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 3000 1.29e+00 1.59e+01
4 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 4000 2.84e+00 2.46e+01
5 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 5000 5.01e+00 3.42e+01
6 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 6000 7.19e+00 4.32e+01
7 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 7000 9.40e+00 5.25e+01
8 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 8000 1.16e+01 6.14e+01
9 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 9000 1.38e+01 7.00e+01
10 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 10000 1.61e+01 8.03e+01
11 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 11000 1.84e+01 8.92e+01
12 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 12000 1.98e+01 9.00e+01
13 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 13000 2.02e+01 8.96e+01
14 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 14000 2.06e+01 9.01e+01
15 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 15000 2.08e+01 9.06e+01
16 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 16000 2.10e+01 9.11e+01
17 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 17000 2.12e+01 9.12e+01
18 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 18000 2.16e+01 9.26e+01
19 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 19000 2.18e+01 9.29e+01
20 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 6 20000 2.20e+01 9.39e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 1000 5.34e-02 3.56e+00
2 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 2000 1.61e-01 6.37e+00
3 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 3000 1.70e+00 1.62e+01
4 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 4000 4.14e+00 2.60e+01
5 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 5000 6.34e+00 3.52e+01
6 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 6000 8.58e+00 4.39e+01
7 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 7000 1.05e+01 5.31e+01
8 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 8000 1.24e+01 6.16e+01
9 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 9000 1.49e+01 7.08e+01
10 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 10000 1.60e+01 7.95e+01
11 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 11000 1.80e+01 8.52e+01
12 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 12000 1.75e+01 8.62e+01
13 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 13000 1.74e+01 8.72e+01
14 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 14000 1.79e+01 8.64e+01
15 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 15000 1.83e+01 8.66e+01
16 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 16000 1.82e+01 8.63e+01
17 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 17000 1.66e+01 8.64e+01
18 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 18000 1.69e+01 8.60e+01
19 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 19000 1.72e+01 8.71e+01
20 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 7 20000 1.76e+01 8.80e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 1000 1.13e-01 4.30e+00
2 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 2000 2.88e-01 8.50e+00
3 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 3000 1.47e+00 1.73e+01
4 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 4000 3.54e+00 2.61e+01
5 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 5000 5.67e+00 3.52e+01
6 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 6000 7.78e+00 4.41e+01
7 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 7000 9.64e+00 5.32e+01
8 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 8000 1.15e+01 6.19e+01
9 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 9000 1.33e+01 7.04e+01
10 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 10000 1.54e+01 7.96e+01
11 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 11000 1.73e+01 8.52e+01
12 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 12000 1.75e+01 8.54e+01
13 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 13000 1.74e+01 8.56e+01
14 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 14000 1.76e+01 8.61e+01
15 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 15000 1.76e+01 8.57e+01
16 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 16000 1.82e+01 8.67e+01
17 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 17000 1.85e+01 8.78e+01
18 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 18000 1.83e+01 8.74e+01
19 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 19000 1.80e+01 8.74e+01
20 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 8 20000 1.83e+01 8.79e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 1000 6.15e-02 4.08e+00
2 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 2000 2.46e-01 7.91e+00
3 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 3000 1.23e+00 1.74e+01
4 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 4000 3.63e+00 2.59e+01
5 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 5000 6.40e+00 3.44e+01
6 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 6000 8.35e+00 4.39e+01
7 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 7000 1.03e+01 5.28e+01
8 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 8000 1.23e+01 6.24e+01
9 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 9000 1.33e+01 7.10e+01
10 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 10000 1.57e+01 8.04e+01
11 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 11000 1.75e+01 8.78e+01
12 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 12000 1.81e+01 8.92e+01
13 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 13000 1.77e+01 8.98e+01
14 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 14000 1.76e+01 9.05e+01
15 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 15000 1.80e+01 9.11e+01
16 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 16000 1.85e+01 9.01e+01
17 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 17000 1.82e+01 8.79e+01
18 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 18000 1.90e+01 8.71e+01
19 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 19000 1.86e+01 8.83e+01
20 DQN 8 8 2 3 2.50e-01 False 2.50e-01 0 0 9 20000 1.94e+01 8.89e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 1000 4.56e-02 4.12e+00
2 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 2000 5.62e-02 5.65e+00
3 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 3000 2.20e-01 1.41e+01
4 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 4000 4.30e-01 2.43e+01
5 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 5000 6.80e-01 3.33e+01
6 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 6000 8.30e-01 4.30e+01
7 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 7000 9.50e-01 5.26e+01
8 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 8000 1.23e+00 6.12e+01
9 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 9000 1.37e+00 7.04e+01
10 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 10000 1.58e+00 7.96e+01
11 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 11000 1.73e+00 8.25e+01
12 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 12000 1.78e+00 8.52e+01
13 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 13000 1.74e+00 8.55e+01
14 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 14000 1.82e+00 8.55e+01
15 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 15000 1.96e+00 8.63e+01
16 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 16000 1.99e+00 9.00e+01
17 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 17000 2.00e+00 9.07e+01
18 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 18000 2.04e+00 9.27e+01
19 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 19000 2.01e+00 9.31e+01
20 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 0 20000 2.00e+00 9.41e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 1000 3.38e-02 4.22e+00
2 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 2000 3.09e-02 5.15e+00
3 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 3000 9.65e-02 8.66e+00
4 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 4000 3.80e-01 1.77e+01
5 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 5000 6.70e-01 2.73e+01
6 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 6000 9.80e-01 3.68e+01
7 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 7000 1.48e+00 4.67e+01
8 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 8000 1.84e+00 5.55e+01
9 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 9000 2.12e+00 6.45e+01
10 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 10000 2.46e+00 7.19e+01
11 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 11000 2.94e+00 8.02e+01
12 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 12000 3.24e+00 8.76e+01
13 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 13000 3.38e+00 8.92e+01
14 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 14000 3.44e+00 8.97e+01
15 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 15000 3.57e+00 8.97e+01
16 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 16000 3.51e+00 9.14e+01
17 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 17000 3.56e+00 9.18e+01
18 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 18000 3.67e+00 9.18e+01
19 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 19000 3.60e+00 9.15e+01
20 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 1 20000 3.37e+00 9.05e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 1000 2.64e-02 3.77e+00
2 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 2000 5.92e-02 6.59e+00
3 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 3000 1.80e-01 1.16e+01
4 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 4000 3.10e-01 1.97e+01
5 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 5000 3.60e-01 2.89e+01
6 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 6000 4.60e-01 3.86e+01
7 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 7000 5.70e-01 4.81e+01
8 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 8000 6.30e-01 5.62e+01
9 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 9000 6.30e-01 6.15e+01
10 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 10000 7.00e-01 6.77e+01
11 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 11000 7.50e-01 7.28e+01
12 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 12000 9.50e-01 8.21e+01
13 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 13000 1.17e+00 8.87e+01
14 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 14000 1.47e+00 9.28e+01
15 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 15000 1.61e+00 9.27e+01
16 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 16000 1.86e+00 9.06e+01
17 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 17000 1.98e+00 9.16e+01
18 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 18000 2.12e+00 9.19e+01
19 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 19000 2.51e+00 9.20e+01
20 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 2 20000 2.78e+00 9.29e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 1000 2.07e-02 4.14e+00
2 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 2000 3.11e-02 4.45e+00
3 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 3000 9.00e-02 1.06e+01
4 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 4000 2.20e-01 2.02e+01
5 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 5000 3.60e-01 2.95e+01
6 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 6000 6.50e-01 3.83e+01
7 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 7000 9.10e-01 4.55e+01
8 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 8000 1.17e+00 5.39e+01
9 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 9000 1.41e+00 6.22e+01
10 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 10000 1.57e+00 7.12e+01
11 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 11000 1.83e+00 7.47e+01
12 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 12000 2.00e+00 7.79e+01
13 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 13000 2.71e+00 8.21e+01
14 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 14000 4.17e+00 8.60e+01
15 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 15000 4.97e+00 8.98e+01
16 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 16000 5.86e+00 8.95e+01
17 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 17000 7.18e+00 8.78e+01
18 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 18000 7.99e+00 8.72e+01
19 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 19000 9.20e+00 8.73e+01
20 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 3 20000 1.06e+01 8.76e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 1000 2.81e-02 4.00e+00
2 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 2000 2.11e-02 4.23e+00
3 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 3000 7.00e-02 1.20e+01
4 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 4000 1.90e-01 2.14e+01
5 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 5000 3.50e-01 3.09e+01
6 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 6000 4.90e-01 4.03e+01
7 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 7000 6.50e-01 4.94e+01
8 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 8000 8.70e-01 5.81e+01
9 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 9000 1.23e+00 6.70e+01
10 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 10000 1.74e+00 7.60e+01
11 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 11000 2.66e+00 8.39e+01
12 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 12000 3.58e+00 8.58e+01
13 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 13000 4.12e+00 8.93e+01
14 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 14000 4.56e+00 8.97e+01
15 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 15000 5.31e+00 9.00e+01
16 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 16000 5.99e+00 8.87e+01
17 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 17000 6.87e+00 8.93e+01
18 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 18000 7.13e+00 8.97e+01
19 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 19000 7.09e+00 8.85e+01
20 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 4 20000 6.60e+00 8.71e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 1000 3.70e-02 4.08e+00
2 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 2000 2.28e-02 4.60e+00
3 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 3000 2.60e-01 1.20e+01
4 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 4000 4.60e-01 2.18e+01
5 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 5000 7.10e-01 3.14e+01
6 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 6000 1.00e+00 4.08e+01
7 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 7000 1.26e+00 5.01e+01
8 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 8000 1.39e+00 5.98e+01
9 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 9000 1.63e+00 6.97e+01
10 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 10000 2.07e+00 7.92e+01
11 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 11000 2.40e+00 8.37e+01
12 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 12000 2.47e+00 8.43e+01
13 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 13000 2.51e+00 8.60e+01
14 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 14000 2.59e+00 8.68e+01
15 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 15000 2.64e+00 8.79e+01
16 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 16000 2.64e+00 8.80e+01
17 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 17000 2.78e+00 8.78e+01
18 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 18000 2.74e+00 8.99e+01
19 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 19000 2.52e+00 8.94e+01
20 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 5 20000 2.33e+00 8.85e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 1000 2.34e-02 3.89e+00
2 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 2000 1.67e-02 4.15e+00
3 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 3000 1.60e-01 1.19e+01
4 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 4000 3.08e-02 3.95e+00
5 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 5000 2.50e-01 1.33e+01
6 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 6000 4.80e-01 2.25e+01
7 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 7000 6.30e-01 3.20e+01
8 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 8000 8.20e-01 4.25e+01
9 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 9000 1.04e+00 5.15e+01
10 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 10000 1.17e+00 6.15e+01
11 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 11000 1.39e+00 7.12e+01
12 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 12000 1.49e+00 8.03e+01
13 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 13000 1.56e+00 8.64e+01
14 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 14000 1.45e+00 8.85e+01
15 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 15000 1.42e+00 8.91e+01
16 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 16000 1.34e+00 8.80e+01
17 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 17000 1.39e+00 8.92e+01
18 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 18000 1.20e+00 8.72e+01
19 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 19000 1.37e+00 8.85e+01
20 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 6 20000 1.31e+00 9.11e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 1000 2.37e-02 3.94e+00
2 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 2000 3.93e-02 5.61e+00
3 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 3000 8.87e-02 8.10e+00
4 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 4000 9.95e-03 2.49e+00
5 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 5000 2.60e-01 1.09e+01
6 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 6000 4.10e-01 2.12e+01
7 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 7000 5.00e-01 3.06e+01
8 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 8000 6.80e-01 4.05e+01
9 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 9000 8.20e-01 5.03e+01
10 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 10000 1.09e+00 5.98e+01
11 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 11000 1.29e+00 6.92e+01
12 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 12000 1.48e+00 7.72e+01
13 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 13000 1.64e+00 8.32e+01
14 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 14000 1.70e+00 8.61e+01
15 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 15000 1.74e+00 8.70e+01
16 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 16000 1.86e+00 8.85e+01
17 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 17000 1.92e+00 8.94e+01
18 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 18000 1.95e+00 8.87e+01
19 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 19000 1.98e+00 9.05e+01
20 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 7 20000 2.02e+00 9.21e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 1000 1.54e-02 3.80e+00
2 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 2000 4.19e-02 5.29e+00
3 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 3000 4.40e-02 5.17e+00
4 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 4000 4.21e-02 5.57e+00
5 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 5000 1.53e-02 3.54e+00
6 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 6000 2.10e-01 1.44e+01
7 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 7000 8.10e-01 2.43e+01
8 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 8000 1.69e+00 3.35e+01
9 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 9000 2.35e+00 4.29e+01
10 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 10000 3.10e+00 5.26e+01
11 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 11000 3.95e+00 6.30e+01
12 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 12000 4.20e+00 7.23e+01
13 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 13000 5.33e+00 7.87e+01
14 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 14000 6.03e+00 8.47e+01
15 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 15000 7.18e+00 9.19e+01
16 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 16000 7.33e+00 9.20e+01
17 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 17000 8.08e+00 9.21e+01
18 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 18000 8.41e+00 9.23e+01
19 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 19000 8.17e+00 9.13e+01
20 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 8 20000 7.87e+00 9.05e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 1000 2.86e-02 4.07e+00
2 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 2000 6.15e-02 5.58e+00
3 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 3000 1.50e-01 1.51e+01
4 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 4000 3.50e-01 2.04e+01
5 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 5000 1.90e-01 1.31e+01
6 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 6000 5.50e-01 2.21e+01
7 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 7000 1.07e+00 3.16e+01
8 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 8000 1.83e+00 4.12e+01
9 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 9000 2.45e+00 5.07e+01
10 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 10000 3.37e+00 6.09e+01
11 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 11000 4.32e+00 6.92e+01
12 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 12000 5.21e+00 7.74e+01
13 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 13000 6.11e+00 8.59e+01
14 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 14000 7.37e+00 9.16e+01
15 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 15000 8.36e+00 9.06e+01
16 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 16000 9.06e+00 8.97e+01
17 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 17000 9.70e+00 8.97e+01
18 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 18000 1.03e+01 9.00e+01
19 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 19000 1.03e+01 9.05e+01
20 DQN 8 8 2 4 2.50e-01 False 2.50e-01 0 0 9 20000 1.09e+01 9.10e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 1000 1.93e-01 3.72e+00
2 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 2000 1.08e+00 7.93e+00
3 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 3000 4.34e+00 1.69e+01
4 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 4000 8.49e+00 2.59e+01
5 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 5000 1.28e+01 3.50e+01
6 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 6000 1.72e+01 4.44e+01
7 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 7000 2.19e+01 5.36e+01
8 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 8000 2.61e+01 6.20e+01
9 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 9000 3.07e+01 7.15e+01
10 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 10000 3.54e+01 8.05e+01
11 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 11000 4.02e+01 9.01e+01
12 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 12000 4.21e+01 9.17e+01
13 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 13000 4.03e+01 8.68e+01
14 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 14000 4.00e+01 8.54e+01
15 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 15000 4.01e+01 8.53e+01
16 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 16000 4.01e+01 8.53e+01
17 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 17000 4.03e+01 8.58e+01
18 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 18000 4.01e+01 8.53e+01
19 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 19000 4.08e+01 8.66e+01
20 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 0 20000 4.03e+01 8.57e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 1000 2.02e-01 3.96e+00
2 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 2000 6.83e-01 7.86e+00
3 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 3000 3.62e+00 1.63e+01
4 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 4000 8.23e+00 2.53e+01
5 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 5000 1.28e+01 3.42e+01
6 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 6000 1.75e+01 4.40e+01
7 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 7000 2.22e+01 5.36e+01
8 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 8000 2.67e+01 6.22e+01
9 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 9000 3.15e+01 7.16e+01
10 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 10000 3.59e+01 8.03e+01
11 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 11000 4.04e+01 8.93e+01
12 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 12000 4.24e+01 8.98e+01
13 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 13000 4.21e+01 8.92e+01
14 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 14000 4.16e+01 8.80e+01
15 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 15000 4.22e+01 8.89e+01
16 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 16000 4.21e+01 8.88e+01
17 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 17000 4.34e+01 9.14e+01
18 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 18000 4.28e+01 9.02e+01
19 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 19000 4.26e+01 8.95e+01
20 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 1 20000 4.36e+01 9.14e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 1000 2.39e-01 4.18e+00
2 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 2000 1.00e+00 8.66e+00
3 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 3000 5.16e+00 1.75e+01
4 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 4000 9.56e+00 2.64e+01
5 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 5000 1.44e+01 3.56e+01
6 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 6000 1.89e+01 4.45e+01
7 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 7000 2.32e+01 5.29e+01
8 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 8000 2.79e+01 6.20e+01
9 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 9000 3.28e+01 7.18e+01
10 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 10000 3.72e+01 8.06e+01
11 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 11000 4.18e+01 8.95e+01
12 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 12000 4.26e+01 9.02e+01
13 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 13000 4.30e+01 9.08e+01
14 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 14000 4.21e+01 8.90e+01
15 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 15000 4.31e+01 9.11e+01
16 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 16000 4.27e+01 9.02e+01
17 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 17000 4.31e+01 9.10e+01
18 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 18000 4.25e+01 8.99e+01
19 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 19000 4.28e+01 9.03e+01
20 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 2 20000 4.14e+01 8.75e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 1000 2.12e-01 4.24e+00
2 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 2000 8.77e-01 7.57e+00
3 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 3000 9.38e+00 1.57e+01
4 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 4000 1.86e+01 2.50e+01
5 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 5000 2.83e+01 3.45e+01
6 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 6000 3.74e+01 4.33e+01
7 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 7000 4.71e+01 5.27e+01
8 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 8000 5.62e+01 6.18e+01
9 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 9000 6.54e+01 7.12e+01
10 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 10000 7.49e+01 8.05e+01
11 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 11000 8.04e+01 8.58e+01
12 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 12000 8.14e+01 8.69e+01
13 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 13000 8.22e+01 8.75e+01
14 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 14000 8.21e+01 8.72e+01
15 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 15000 8.17e+01 8.68e+01
16 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 16000 8.17e+01 8.67e+01
17 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 17000 8.39e+01 8.88e+01
18 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 18000 8.37e+01 8.85e+01
19 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 19000 8.37e+01 8.85e+01
20 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 3 20000 8.47e+01 8.95e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 1000 1.65e-01 3.66e+00
2 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 2000 6.50e-01 8.08e+00
3 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 3000 4.74e+00 1.74e+01
4 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 4000 9.31e+00 2.64e+01
5 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 5000 1.37e+01 3.47e+01
6 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 6000 1.83e+01 4.39e+01
7 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 7000 2.27e+01 5.26e+01
8 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 8000 2.76e+01 6.24e+01
9 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 9000 3.23e+01 7.15e+01
10 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 10000 3.69e+01 8.05e+01
11 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 11000 4.00e+01 8.53e+01
12 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 12000 3.94e+01 8.40e+01
13 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 13000 3.95e+01 8.40e+01
14 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 14000 3.98e+01 8.44e+01
15 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 15000 4.04e+01 8.55e+01
16 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 16000 4.15e+01 8.78e+01
17 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 17000 4.11e+01 8.71e+01
18 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 18000 4.09e+01 8.65e+01
19 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 19000 4.19e+01 8.86e+01
20 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 4 20000 4.29e+01 9.07e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 1000 1.43e-01 3.86e+00
2 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 2000 6.69e-01 6.36e+00
3 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 3000 8.54e+00 1.53e+01
4 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 4000 1.77e+01 2.46e+01
5 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 5000 2.70e+01 3.37e+01
6 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 6000 3.61e+01 4.31e+01
7 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 7000 4.55e+01 5.21e+01
8 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 8000 5.54e+01 6.23e+01
9 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 9000 6.45e+01 7.17e+01
10 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 10000 7.28e+01 8.01e+01
11 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 11000 8.23e+01 8.94e+01
12 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 12000 8.38e+01 8.97e+01
13 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 13000 8.32e+01 9.02e+01
14 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 14000 8.32e+01 9.03e+01
15 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 15000 8.49e+01 9.21e+01
16 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 16000 8.45e+01 9.18e+01
17 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 17000 8.38e+01 9.12e+01
18 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 18000 8.43e+01 9.15e+01
19 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 19000 8.32e+01 9.03e+01
20 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 5 20000 8.59e+01 9.28e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 1000 2.02e-01 4.10e+00
2 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 2000 1.25e+00 8.57e+00
3 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 3000 1.01e+01 1.77e+01
4 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 4000 1.86e+01 2.57e+01
5 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 5000 2.81e+01 3.49e+01
6 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 6000 3.77e+01 4.44e+01
7 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 7000 4.61e+01 5.35e+01
8 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 8000 5.51e+01 6.21e+01
9 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 9000 6.50e+01 7.16e+01
10 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 10000 7.35e+01 7.99e+01
11 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 11000 8.24e+01 8.85e+01
12 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 12000 8.28e+01 8.87e+01
13 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 13000 8.37e+01 8.96e+01
14 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 14000 8.37e+01 8.93e+01
15 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 15000 8.26e+01 8.82e+01
16 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 16000 8.39e+01 8.90e+01
17 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 17000 8.42e+01 8.93e+01
18 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 18000 8.38e+01 8.89e+01
19 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 19000 8.56e+01 9.07e+01
20 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 6 20000 8.47e+01 8.98e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 1000 1.97e-01 3.78e+00
2 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 2000 1.92e+00 7.69e+00
3 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 3000 1.08e+01 1.69e+01
4 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 4000 1.91e+01 2.53e+01
5 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 5000 2.83e+01 3.45e+01
6 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 6000 3.74e+01 4.35e+01
7 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 7000 4.69e+01 5.30e+01
8 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 8000 5.62e+01 6.23e+01
9 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 9000 6.50e+01 7.11e+01
10 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 10000 7.48e+01 8.08e+01
11 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 11000 8.42e+01 9.00e+01
12 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 12000 8.67e+01 9.22e+01
13 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 13000 8.72e+01 9.23e+01
14 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 14000 8.71e+01 9.23e+01
15 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 15000 8.68e+01 9.20e+01
16 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 16000 8.70e+01 9.22e+01
17 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 17000 8.44e+01 8.95e+01
18 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 18000 8.33e+01 8.84e+01
19 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 19000 8.37e+01 8.89e+01
20 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 7 20000 8.35e+01 8.86e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 1000 1.99e-01 4.06e+00
2 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 2000 1.72e+00 9.17e+00
3 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 3000 5.79e+00 1.74e+01
4 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 4000 1.04e+01 2.68e+01
5 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 5000 1.49e+01 3.56e+01
6 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 6000 1.98e+01 4.53e+01
7 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 7000 2.40e+01 5.36e+01
8 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 8000 2.84e+01 6.24e+01
9 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 9000 3.30e+01 7.14e+01
10 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 10000 3.76e+01 8.04e+01
11 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 11000 4.20e+01 8.92e+01
12 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 12000 4.22e+01 8.93e+01
13 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 13000 4.33e+01 9.14e+01
14 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 14000 4.35e+01 9.22e+01
15 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 15000 4.31e+01 9.16e+01
16 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 16000 4.31e+01 9.15e+01
17 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 17000 4.31e+01 9.15e+01
18 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 18000 4.34e+01 9.22e+01
19 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 19000 4.22e+01 8.99e+01
20 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 8 20000 4.21e+01 8.97e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 1000 1.49e-01 3.82e+00
2 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 2000 1.01e+00 7.15e+00
3 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 3000 5.19e+00 1.63e+01
4 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 4000 9.36e+00 2.47e+01
5 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 5000 1.40e+01 3.41e+01
6 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 6000 1.90e+01 4.38e+01
7 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 7000 2.37e+01 5.32e+01
8 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 8000 2.81e+01 6.20e+01
9 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 9000 3.27e+01 7.12e+01
10 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 10000 3.74e+01 8.04e+01
11 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 11000 4.13e+01 8.73e+01
12 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 12000 4.09e+01 8.64e+01
13 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 13000 4.14e+01 8.72e+01
14 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 14000 4.05e+01 8.54e+01
15 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 15000 4.08e+01 8.59e+01
16 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 16000 4.03e+01 8.49e+01
17 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 17000 4.09e+01 8.61e+01
18 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 18000 4.06e+01 8.58e+01
19 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 19000 4.02e+01 8.49e+01
20 DQN 8 8 4 1 2.50e-01 False 2.50e-01 0 0 9 20000 4.11e+01 8.69e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 1000 1.21e-01 4.02e+00
2 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 2000 3.24e-01 7.40e+00
3 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 3000 8.10e-01 1.54e+01
4 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 4000 1.70e+00 2.55e+01
5 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 5000 3.58e+00 3.43e+01
6 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 6000 5.82e+00 4.35e+01
7 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 7000 8.34e+00 5.22e+01
8 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 8000 1.07e+01 6.17e+01
9 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 9000 1.17e+01 7.00e+01
10 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 10000 1.44e+01 7.96e+01
11 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 11000 1.63e+01 8.52e+01
12 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 12000 1.81e+01 8.70e+01
13 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 13000 1.89e+01 8.48e+01
14 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 14000 1.93e+01 8.49e+01
15 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 15000 1.90e+01 8.51e+01
16 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 16000 1.94e+01 8.77e+01
17 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 17000 2.06e+01 8.85e+01
18 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 18000 2.09e+01 9.03e+01
19 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 19000 2.12e+01 9.10e+01
20 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 0 20000 2.19e+01 9.03e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 1000 1.27e-01 4.21e+00
2 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 2000 6.24e-01 8.00e+00
3 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 3000 3.25e+00 1.74e+01
4 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 4000 6.04e+00 2.56e+01
5 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 5000 9.12e+00 3.47e+01
6 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 6000 1.22e+01 4.41e+01
7 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 7000 1.54e+01 5.33e+01
8 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 8000 1.86e+01 6.27e+01
9 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 9000 2.17e+01 7.18e+01
10 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 10000 2.46e+01 8.02e+01
11 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 11000 2.79e+01 8.99e+01
12 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 12000 2.80e+01 8.87e+01
13 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 13000 2.82e+01 8.95e+01
14 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 14000 2.86e+01 9.02e+01
15 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 15000 2.76e+01 8.73e+01
16 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 16000 2.77e+01 8.75e+01
17 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 17000 2.79e+01 8.80e+01
18 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 18000 2.68e+01 8.49e+01
19 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 19000 2.66e+01 8.45e+01
20 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 1 20000 2.67e+01 8.46e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 1000 1.04e-01 4.16e+00
2 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 2000 2.73e-01 6.62e+00
3 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 3000 2.08e+00 1.62e+01
4 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 4000 4.84e+00 2.46e+01
5 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 5000 7.86e+00 3.40e+01
6 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 6000 9.92e+00 4.30e+01
7 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 7000 1.35e+01 5.26e+01
8 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 8000 1.52e+01 6.17e+01
9 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 9000 1.76e+01 7.06e+01
10 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 10000 2.06e+01 8.00e+01
11 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 11000 2.01e+01 8.14e+01
12 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 12000 2.00e+01 8.27e+01
13 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 13000 1.80e+01 8.22e+01
14 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 14000 1.81e+01 8.16e+01
15 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 15000 1.75e+01 8.22e+01
16 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 16000 1.78e+01 8.31e+01
17 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 17000 1.86e+01 8.45e+01
18 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 18000 1.97e+01 8.68e+01
19 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 19000 2.04e+01 8.90e+01
20 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 2 20000 2.26e+01 9.01e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 1000 9.82e-02 4.36e+00
2 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 2000 3.57e-01 7.15e+00
3 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 3000 1.32e+00 1.61e+01
4 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 4000 2.58e+00 2.47e+01
5 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 5000 4.03e+00 3.38e+01
6 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 6000 5.66e+00 4.36e+01
7 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 7000 7.03e+00 5.20e+01
8 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 8000 8.59e+00 6.11e+01
9 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 9000 1.02e+01 7.10e+01
10 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 10000 1.17e+01 8.02e+01
11 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 11000 1.30e+01 8.58e+01
12 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 12000 1.36e+01 8.80e+01
13 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 13000 1.37e+01 8.75e+01
14 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 14000 1.38e+01 8.82e+01
15 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 15000 1.39e+01 8.84e+01
16 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 16000 1.41e+01 8.93e+01
17 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 17000 1.42e+01 9.02e+01
18 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 18000 1.44e+01 9.15e+01
19 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 19000 1.41e+01 8.95e+01
20 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 3 20000 1.39e+01 8.87e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 1000 1.29e-01 4.29e+00
2 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 2000 5.46e-01 7.68e+00
3 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 3000 1.65e+00 1.61e+01
4 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 4000 3.47e+00 2.56e+01
5 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 5000 5.31e+00 3.47e+01
6 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 6000 8.14e+00 4.41e+01
7 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 7000 1.08e+01 5.27e+01
8 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 8000 1.42e+01 6.16e+01
9 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 9000 1.76e+01 7.15e+01
10 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 10000 2.08e+01 8.03e+01
11 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 11000 2.39e+01 8.93e+01
12 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 12000 2.65e+01 9.25e+01
13 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 13000 2.78e+01 9.19e+01
14 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 14000 2.87e+01 8.95e+01
15 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 15000 2.85e+01 8.72e+01
16 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 16000 2.87e+01 8.53e+01
17 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 17000 2.83e+01 8.49e+01
18 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 18000 2.80e+01 8.32e+01
19 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 19000 2.89e+01 8.51e+01
20 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 4 20000 2.91e+01 8.40e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 1000 1.08e-01 3.97e+00
2 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 2000 5.04e-01 7.74e+00
3 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 3000 1.64e+00 1.66e+01
4 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 4000 3.18e+00 2.61e+01
5 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 5000 4.86e+00 3.63e+01
6 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 6000 6.30e+00 4.54e+01
7 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 7000 7.68e+00 5.40e+01
8 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 8000 9.20e+00 6.29e+01
9 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 9000 1.06e+01 7.13e+01
10 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 10000 1.21e+01 8.05e+01
11 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 11000 1.43e+01 8.65e+01
12 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 12000 1.50e+01 8.57e+01
13 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 13000 1.58e+01 8.54e+01
14 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 14000 1.64e+01 8.55e+01
15 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 15000 1.71e+01 8.70e+01
16 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 16000 1.83e+01 8.90e+01
17 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 17000 1.78e+01 8.75e+01
18 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 18000 1.81e+01 8.98e+01
19 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 19000 1.88e+01 9.06e+01
20 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 5 20000 1.70e+01 8.95e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 1000 7.75e-02 3.84e+00
2 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 2000 5.11e-01 7.13e+00
3 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 3000 1.32e+00 1.58e+01
4 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 4000 2.89e+00 2.57e+01
5 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 5000 4.94e+00 3.50e+01
6 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 6000 7.00e+00 4.39e+01
7 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 7000 9.08e+00 5.27e+01
8 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 8000 1.12e+01 6.20e+01
9 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 9000 1.33e+01 7.10e+01
10 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 10000 1.55e+01 8.03e+01
11 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 11000 1.82e+01 9.04e+01
12 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 12000 2.03e+01 9.53e+01
13 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 13000 2.11e+01 9.53e+01
14 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 14000 2.14e+01 9.52e+01
15 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 15000 2.10e+01 9.30e+01
16 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 16000 2.13e+01 9.40e+01
17 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 17000 2.11e+01 9.33e+01
18 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 18000 2.09e+01 9.37e+01
19 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 19000 2.05e+01 9.22e+01
20 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 6 20000 2.04e+01 9.15e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 1000 1.15e-01 3.97e+00
2 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 2000 2.09e-01 6.48e+00
3 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 3000 7.50e-01 1.62e+01
4 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 4000 2.03e+00 2.60e+01
5 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 5000 4.21e+00 3.52e+01
6 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 6000 6.43e+00 4.39e+01
7 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 7000 8.24e+00 5.26e+01
8 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 8000 1.16e+01 6.24e+01
9 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 9000 1.48e+01 7.17e+01
10 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 10000 1.68e+01 8.03e+01
11 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 11000 1.89e+01 8.97e+01
12 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 12000 2.14e+01 9.02e+01
13 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 13000 2.16e+01 9.12e+01
14 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 14000 2.18e+01 8.93e+01
15 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 15000 2.09e+01 8.92e+01
16 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 16000 2.06e+01 8.80e+01
17 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 17000 1.96e+01 8.79e+01
18 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 18000 1.87e+01 8.79e+01
19 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 19000 1.75e+01 8.78e+01
20 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 7 20000 1.84e+01 9.11e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 1000 9.36e-02 4.26e+00
2 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 2000 4.00e-01 7.94e+00
3 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 3000 7.80e-01 1.53e+01
4 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 4000 1.42e+00 2.44e+01
5 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 5000 2.41e+00 3.35e+01
6 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 6000 3.89e+00 4.31e+01
7 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 7000 5.92e+00 5.27e+01
8 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 8000 8.21e+00 6.20e+01
9 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 9000 1.11e+01 7.09e+01
10 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 10000 1.35e+01 8.05e+01
11 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 11000 1.62e+01 8.89e+01
12 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 12000 1.78e+01 8.87e+01
13 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 13000 1.96e+01 8.98e+01
14 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 14000 2.04e+01 8.97e+01
15 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 15000 2.04e+01 9.00e+01
16 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 16000 2.01e+01 9.07e+01
17 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 17000 2.05e+01 8.95e+01
18 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 18000 1.99e+01 8.93e+01
19 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 19000 2.01e+01 9.08e+01
20 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 8 20000 1.83e+01 8.81e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 1000 1.12e-01 4.00e+00
2 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 2000 2.10e-01 5.99e+00
3 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 3000 6.90e-01 1.51e+01
4 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 4000 2.02e+00 2.46e+01
5 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 5000 3.68e+00 3.34e+01
6 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 6000 6.99e+00 4.24e+01
7 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 7000 1.07e+01 5.19e+01
8 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 8000 1.36e+01 6.10e+01
9 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 9000 1.65e+01 7.04e+01
10 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 10000 1.86e+01 7.97e+01
11 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 11000 2.04e+01 8.55e+01
12 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 12000 2.25e+01 8.48e+01
13 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 13000 2.45e+01 8.40e+01
14 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 14000 2.49e+01 8.44e+01
15 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 15000 2.34e+01 8.19e+01
16 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 16000 2.30e+01 8.22e+01
17 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 17000 2.12e+01 8.07e+01
18 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 18000 2.32e+01 8.25e+01
19 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 19000 2.41e+01 8.29e+01
20 DQN 8 8 4 2 2.50e-01 False 2.50e-01 0 0 9 20000 2.57e+01 8.48e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 1000 7.75e-03 3.87e+00
2 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 2000 3.95e-02 4.35e+00
3 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 3000 2.82e-02 2.84e+00
4 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 4000 2.30e-01 1.11e+01
5 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 5000 4.00e-01 2.12e+01
6 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 6000 7.30e-01 3.03e+01
7 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 7000 9.30e-01 3.83e+01
8 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 8000 1.04e+00 4.56e+01
9 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 9000 1.76e+00 5.36e+01
10 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 10000 3.25e+00 6.21e+01
11 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 11000 5.36e+00 6.48e+01
12 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 12000 6.50e+00 7.38e+01
13 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 13000 7.86e+00 7.70e+01
14 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 14000 9.36e+00 7.97e+01
15 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 15000 1.13e+01 8.15e+01
16 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 16000 1.34e+01 9.11e+01
17 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 17000 1.51e+01 9.18e+01
18 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 18000 1.77e+01 9.41e+01
19 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 19000 1.84e+01 9.30e+01
20 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 0 20000 1.87e+01 9.22e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 1000 4.33e-02 3.93e+00
2 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 2000 1.07e-01 5.93e+00
3 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 3000 4.80e-01 1.61e+01
4 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 4000 7.20e-01 2.52e+01
5 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 5000 1.04e+00 3.46e+01
6 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 6000 1.41e+00 4.36e+01
7 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 7000 1.64e+00 5.31e+01
8 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 8000 2.10e+00 6.20e+01
9 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 9000 2.57e+00 7.09e+01
10 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 10000 2.98e+00 8.02e+01
11 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 11000 3.29e+00 8.37e+01
12 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 12000 3.78e+00 8.76e+01
13 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 13000 4.83e+00 9.08e+01
14 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 14000 5.15e+00 9.06e+01
15 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 15000 5.68e+00 9.18e+01
16 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 16000 6.15e+00 9.18e+01
17 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 17000 6.70e+00 8.95e+01
18 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 18000 7.15e+00 9.05e+01
19 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 19000 7.85e+00 9.02e+01
20 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 1 20000 8.22e+00 9.00e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 1000 3.01e-02 3.73e+00
2 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 2000 1.45e-01 5.81e+00
3 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 3000 5.30e-01 1.37e+01
4 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 4000 8.00e-01 2.32e+01
5 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 5000 9.90e-01 3.23e+01
6 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 6000 1.44e+00 4.14e+01
7 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 7000 2.00e+00 5.06e+01
8 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 8000 2.94e+00 5.99e+01
9 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 9000 4.18e+00 6.89e+01
10 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 10000 5.34e+00 7.87e+01
11 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 11000 7.31e+00 8.82e+01
12 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 12000 8.64e+00 8.74e+01
13 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 13000 1.05e+01 8.74e+01
14 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 14000 1.23e+01 8.70e+01
15 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 15000 1.42e+01 8.72e+01
16 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 16000 1.54e+01 8.84e+01
17 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 17000 1.68e+01 8.78e+01
18 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 18000 1.71e+01 8.67e+01
19 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 19000 1.77e+01 8.77e+01
20 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 2 20000 1.78e+01 8.63e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 1000 4.33e-02 3.91e+00
2 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 2000 1.19e-01 7.50e+00
3 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 3000 3.31e-02 3.31e+00
4 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 4000 2.20e-01 1.26e+01
5 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 5000 4.70e-01 2.16e+01
6 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 6000 8.30e-01 3.13e+01
7 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 7000 1.13e+00 4.09e+01
8 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 8000 1.40e+00 4.90e+01
9 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 9000 2.74e+00 5.75e+01
10 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 10000 4.31e+00 6.61e+01
11 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 11000 6.71e+00 7.56e+01
12 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 12000 8.76e+00 8.45e+01
13 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 13000 1.06e+01 8.87e+01
14 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 14000 1.19e+01 9.03e+01
15 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 15000 1.32e+01 9.09e+01
16 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 16000 1.48e+01 9.09e+01
17 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 17000 1.70e+01 9.15e+01
18 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 18000 1.74e+01 9.29e+01
19 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 19000 1.76e+01 9.21e+01
20 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 3 20000 1.79e+01 9.30e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 1000 3.07e-02 3.82e+00
2 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 2000 6.05e-02 4.65e+00
3 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 3000 1.17e-01 6.77e+00
4 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 4000 3.80e-01 1.71e+01
5 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 5000 6.60e-01 2.59e+01
6 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 6000 1.21e+00 3.56e+01
7 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 7000 1.87e+00 4.47e+01
8 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 8000 2.48e+00 5.51e+01
9 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 9000 2.93e+00 6.45e+01
10 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 10000 3.19e+00 7.31e+01
11 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 11000 3.63e+00 8.24e+01
12 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 12000 4.57e+00 8.67e+01
13 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 13000 4.99e+00 8.76e+01
14 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 14000 5.54e+00 8.73e+01
15 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 15000 6.08e+00 8.56e+01
16 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 16000 5.98e+00 8.60e+01
17 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 17000 6.60e+00 8.89e+01
18 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 18000 6.59e+00 8.91e+01
19 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 19000 6.98e+00 8.87e+01
20 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 4 20000 7.78e+00 8.83e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 1000 3.09e-02 3.85e+00
2 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 2000 9.44e-02 5.57e+00
3 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 3000 4.00e-01 1.51e+01
4 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 4000 8.00e-01 2.49e+01
5 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 5000 1.13e+00 3.32e+01
6 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 6000 1.77e+00 4.32e+01
7 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 7000 2.13e+00 5.14e+01
8 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 8000 2.61e+00 6.07e+01
9 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 9000 3.34e+00 6.99e+01
10 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 10000 3.82e+00 7.82e+01
11 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 11000 4.26e+00 8.43e+01
12 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 12000 4.58e+00 8.59e+01
13 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 13000 4.85e+00 8.77e+01
14 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 14000 5.01e+00 8.91e+01
15 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 15000 4.73e+00 8.83e+01
16 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 16000 4.91e+00 8.85e+01
17 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 17000 5.16e+00 8.75e+01
18 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 18000 5.01e+00 8.70e+01
19 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 19000 5.38e+00 8.68e+01
20 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 5 20000 5.63e+00 8.68e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 1000 4.17e-02 4.17e+00
2 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 2000 1.41e-01 7.37e+00
3 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 3000 3.70e-01 1.67e+01
4 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 4000 9.60e-01 2.65e+01
5 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 5000 1.41e+00 3.59e+01
6 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 6000 1.95e+00 4.47e+01
7 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 7000 2.63e+00 5.35e+01
8 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 8000 3.57e+00 6.17e+01
9 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 9000 4.61e+00 7.07e+01
10 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 10000 5.85e+00 7.99e+01
11 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 11000 6.95e+00 8.93e+01
12 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 12000 7.79e+00 9.27e+01
13 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 13000 8.07e+00 9.30e+01
14 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 14000 9.47e+00 9.26e+01
15 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 15000 1.04e+01 9.11e+01
16 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 16000 1.16e+01 9.27e+01
17 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 17000 1.21e+01 9.20e+01
18 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 18000 1.28e+01 9.23e+01
19 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 19000 1.31e+01 9.23e+01
20 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 6 20000 1.36e+01 9.24e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 1000 3.85e-02 4.23e+00
2 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 2000 6.91e-02 5.37e+00
3 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 3000 9.46e-02 6.53e+00
4 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 4000 3.90e-01 1.79e+01
5 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 5000 5.20e-01 2.72e+01
6 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 6000 1.24e+00 3.69e+01
7 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 7000 2.65e+00 4.64e+01
8 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 8000 4.17e+00 5.50e+01
9 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 9000 5.69e+00 6.52e+01
10 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 10000 7.25e+00 7.38e+01
11 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 11000 9.21e+00 8.28e+01
12 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 12000 1.01e+01 8.81e+01
13 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 13000 1.19e+01 8.89e+01
14 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 14000 1.32e+01 9.08e+01
15 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 15000 1.46e+01 8.99e+01
16 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 16000 1.43e+01 8.96e+01
17 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 17000 1.44e+01 8.98e+01
18 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 18000 1.48e+01 9.01e+01
19 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 19000 1.50e+01 9.12e+01
20 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 7 20000 1.51e+01 8.98e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 1000 3.53e-02 3.92e+00
2 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 2000 1.27e-01 6.25e+00
3 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 3000 2.90e-01 1.51e+01
4 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 4000 4.10e-01 2.39e+01
5 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 5000 7.10e-01 3.38e+01
6 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 6000 1.16e+00 4.27e+01
7 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 7000 1.43e+00 5.17e+01
8 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 8000 1.95e+00 6.16e+01
9 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 9000 2.32e+00 7.05e+01
10 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 10000 2.70e+00 7.99e+01
11 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 11000 3.19e+00 8.53e+01
12 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 12000 3.53e+00 9.26e+01
13 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 13000 3.91e+00 9.05e+01
14 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 14000 4.30e+00 8.90e+01
15 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 15000 4.46e+00 8.90e+01
16 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 16000 4.99e+00 8.89e+01
17 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 17000 5.03e+00 8.91e+01
18 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 18000 5.45e+00 8.90e+01
19 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 19000 5.71e+00 8.87e+01
20 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 8 20000 5.73e+00 8.78e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 1000 1.59e-02 3.92e+00
2 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 2000 9.03e-02 6.52e+00
3 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 3000 8.10e-01 1.51e+01
4 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 4000 1.26e+00 2.41e+01
5 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 5000 2.60e+00 3.33e+01
6 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 6000 3.50e+00 4.23e+01
7 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 7000 5.02e+00 5.21e+01
8 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 8000 6.79e+00 6.09e+01
9 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 9000 7.96e+00 7.05e+01
10 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 10000 9.83e+00 8.03e+01
11 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 11000 1.12e+01 8.63e+01
12 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 12000 1.18e+01 8.77e+01
13 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 13000 1.27e+01 8.82e+01
14 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 14000 1.32e+01 8.93e+01
15 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 15000 1.42e+01 8.90e+01
16 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 16000 1.48e+01 8.92e+01
17 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 17000 1.46e+01 8.93e+01
18 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 18000 1.53e+01 9.02e+01
19 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 19000 1.48e+01 8.97e+01
20 DQN 8 8 4 3 2.50e-01 False 2.50e-01 0 0 9 20000 1.47e+01 8.96e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 1000 1.49e-02 3.73e+00
2 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 2000 2.63e-02 5.25e+00
3 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 3000 2.01e-02 4.02e+00
4 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 4000 9.55e-03 3.10e+00
5 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 5000 5.66e-02 9.60e+00
6 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 6000 1.10e-01 1.50e+01
7 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 7000 2.20e-01 1.92e+01
8 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 8000 1.30e-01 1.16e+01
9 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 9000 3.60e-01 2.08e+01
10 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 10000 5.40e-01 2.94e+01
11 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 11000 8.30e-01 3.91e+01
12 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 12000 1.35e+00 4.76e+01
13 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 13000 1.69e+00 5.61e+01
14 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 14000 2.52e+00 6.58e+01
15 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 15000 3.32e+00 7.35e+01
16 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 16000 4.17e+00 8.22e+01
17 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 17000 5.17e+00 8.78e+01
18 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 18000 6.28e+00 8.83e+01
19 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 19000 7.60e+00 8.85e+01
20 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 0 20000 8.51e+00 8.80e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 1000 1.18e-02 3.91e+00
2 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 2000 1.59e-02 3.98e+00
3 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 3000 5.03e-02 4.95e+00
4 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 4000 2.60e-01 1.51e+01
5 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 5000 4.30e-01 2.52e+01
6 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 6000 5.80e-01 3.40e+01
7 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 7000 7.00e-01 4.36e+01
8 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 8000 1.03e+00 5.06e+01
9 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 9000 1.28e+00 5.72e+01
10 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 10000 2.12e+00 6.44e+01
11 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 11000 2.83e+00 7.27e+01
12 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 12000 3.40e+00 8.16e+01
13 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 13000 4.06e+00 8.61e+01
14 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 14000 4.93e+00 8.70e+01
15 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 15000 5.50e+00 8.70e+01
16 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 16000 6.42e+00 8.68e+01
17 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 17000 6.66e+00 8.63e+01
18 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 18000 6.92e+00 8.61e+01
19 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 19000 7.51e+00 8.82e+01
20 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 1 20000 7.89e+00 8.86e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 1000 1.55e-02 3.88e+00
2 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 2000 2.28e-02 4.54e+00
3 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 3000 1.20e-01 1.25e+01
4 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 4000 3.00e-01 2.18e+01
5 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 5000 4.20e-01 3.04e+01
6 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 6000 5.80e-01 4.09e+01
7 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 7000 7.10e-01 5.03e+01
8 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 8000 8.40e-01 5.99e+01
9 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 9000 9.20e-01 6.86e+01
10 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 10000 8.80e-01 7.20e+01
11 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 11000 1.02e+00 7.96e+01
12 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 12000 1.09e+00 8.67e+01
13 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 13000 1.15e+00 8.82e+01
14 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 14000 1.14e+00 9.00e+01
15 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 15000 1.03e+00 8.95e+01
16 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 16000 9.70e-01 8.98e+01
17 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 17000 9.60e-01 8.90e+01
18 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 18000 1.01e+00 8.72e+01
19 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 19000 1.01e+00 8.72e+01
20 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 2 20000 1.19e+00 8.75e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 1000 1.27e-02 4.21e+00
2 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 2000 7.41e-02 7.40e+00
3 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 3000 3.50e-01 1.67e+01
4 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 4000 5.90e-01 2.60e+01
5 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 5000 7.00e-01 3.45e+01
6 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 6000 7.50e-01 4.37e+01
7 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 7000 9.80e-01 5.32e+01
8 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 8000 1.20e+00 6.25e+01
9 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 9000 1.47e+00 7.19e+01
10 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 10000 1.68e+00 8.11e+01
11 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 11000 1.73e+00 8.82e+01
12 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 12000 1.86e+00 8.92e+01
13 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 13000 1.77e+00 8.96e+01
14 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 14000 2.22e+00 9.03e+01
15 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 15000 2.72e+00 8.98e+01
16 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 16000 2.90e+00 8.95e+01
17 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 17000 3.16e+00 9.01e+01
18 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 18000 3.06e+00 8.85e+01
19 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 19000 3.12e+00 8.92e+01
20 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 3 20000 3.56e+00 8.93e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 1000 1.22e-02 4.06e+00
2 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 2000 8.81e-03 4.33e+00
3 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 3000 2.59e-02 4.38e+00
4 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 4000 2.97e-03 2.93e+00
5 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 5000 1.20e-01 1.23e+01
6 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 6000 3.20e-01 2.16e+01
7 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 7000 4.70e-01 3.08e+01
8 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 8000 6.30e-01 4.02e+01
9 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 9000 9.00e-01 4.90e+01
10 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 10000 1.06e+00 5.60e+01
11 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 11000 1.14e+00 6.39e+01
12 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 12000 1.43e+00 7.12e+01
13 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 13000 1.72e+00 7.99e+01
14 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 14000 1.83e+00 8.60e+01
15 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 15000 1.85e+00 8.64e+01
16 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 16000 1.85e+00 8.79e+01
17 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 17000 1.95e+00 8.76e+01
18 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 18000 1.98e+00 8.78e+01
19 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 19000 1.95e+00 8.80e+01
20 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 4 20000 1.86e+00 8.63e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 1000 1.17e-02 3.88e+00
2 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 2000 4.05e-02 6.76e+00
3 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 3000 1.60e-01 1.53e+01
4 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 4000 2.90e-01 2.41e+01
5 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 5000 3.60e-01 3.29e+01
6 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 6000 4.60e-01 4.16e+01
7 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 7000 5.30e-01 5.04e+01
8 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 8000 6.50e-01 5.81e+01
9 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 9000 6.40e-01 6.26e+01
10 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 10000 7.20e-01 6.68e+01
11 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 11000 7.20e-01 6.68e+01
12 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 12000 6.70e-01 6.64e+01
13 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 13000 8.00e-01 7.26e+01
14 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 14000 9.30e-01 7.83e+01
15 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 15000 1.03e+00 8.39e+01
16 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 16000 1.09e+00 8.43e+01
17 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 17000 1.16e+00 8.51e+01
18 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 18000 1.20e+00 8.77e+01
19 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 19000 1.22e+00 8.80e+01
20 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 5 20000 1.21e+00 8.93e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 1000 1.91e-02 3.79e+00
2 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 2000 4.85e-02 6.10e+00
3 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 3000 1.30e-01 1.63e+01
4 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 4000 2.20e-01 2.55e+01
5 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 5000 2.90e-01 3.31e+01
6 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 6000 3.30e-01 3.68e+01
7 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 7000 4.50e-01 3.63e+01
8 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 8000 5.10e-01 3.75e+01
9 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 9000 6.00e-01 4.50e+01
10 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 10000 7.50e-01 5.16e+01
11 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 11000 8.20e-01 5.64e+01
12 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 12000 9.90e-01 6.20e+01
13 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 13000 1.20e+00 7.11e+01
14 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 14000 1.33e+00 7.99e+01
15 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 15000 1.37e+00 8.03e+01
16 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 16000 1.46e+00 8.05e+01
17 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 17000 1.51e+00 8.48e+01
18 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 18000 1.62e+00 8.46e+01
19 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 19000 1.86e+00 8.63e+01
20 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 6 20000 2.44e+00 8.70e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 1000 1.18e-02 3.90e+00
2 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 2000 9.09e-03 4.48e+00
3 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 3000 2.00e-01 1.26e+01
4 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 4000 4.50e-01 2.24e+01
5 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 5000 7.10e-01 3.22e+01
6 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 6000 8.60e-01 4.21e+01
7 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 7000 1.01e+00 5.12e+01
8 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 8000 1.24e+00 6.11e+01
9 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 9000 1.44e+00 7.14e+01
10 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 10000 1.59e+00 8.07e+01
11 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 11000 1.89e+00 8.82e+01
12 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 12000 1.86e+00 8.84e+01
13 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 13000 1.71e+00 8.68e+01
14 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 14000 1.63e+00 8.72e+01
15 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 15000 1.74e+00 8.71e+01
16 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 16000 1.66e+00 8.82e+01
17 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 17000 1.60e+00 8.77e+01
18 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 18000 1.80e+00 8.76e+01
19 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 19000 1.92e+00 8.76e+01
20 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 7 20000 2.07e+00 8.88e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 1000 1.18e-02 3.89e+00
2 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 2000 9.77e-03 3.29e+00
3 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 3000 7.14e-02 7.14e+00
4 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 4000 2.30e-01 1.01e+01
5 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 5000 5.10e-01 1.93e+01
6 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 6000 6.70e-01 2.82e+01
7 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 7000 7.40e-01 3.72e+01
8 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 8000 8.90e-01 4.64e+01
9 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 9000 1.06e+00 5.61e+01
10 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 10000 1.20e+00 6.57e+01
11 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 11000 1.64e+00 7.60e+01
12 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 12000 1.76e+00 8.10e+01
13 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 13000 1.99e+00 8.80e+01
14 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 14000 2.10e+00 8.72e+01
15 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 15000 2.40e+00 8.88e+01
16 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 16000 2.53e+00 9.01e+01
17 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 17000 2.88e+00 9.00e+01
18 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 18000 3.40e+00 9.08e+01
19 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 19000 3.87e+00 9.21e+01
20 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 8 20000 4.22e+00 9.35e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 1000 7.91e-03 3.95e+00
2 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 2000 2.45e-02 4.91e+00
3 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 3000 2.20e-01 1.44e+01
4 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 4000 4.40e-01 2.37e+01
5 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 5000 6.10e-01 3.33e+01
6 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 6000 8.90e-01 4.27e+01
7 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 7000 1.21e+00 5.25e+01
8 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 8000 1.58e+00 6.19e+01
9 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 9000 1.97e+00 7.05e+01
10 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 10000 2.38e+00 7.95e+01
11 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 11000 2.78e+00 8.68e+01
12 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 12000 3.32e+00 8.83e+01
13 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 13000 3.52e+00 8.76e+01
14 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 14000 3.68e+00 8.86e+01
15 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 15000 3.90e+00 8.92e+01
16 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 16000 4.17e+00 8.75e+01
17 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 17000 4.43e+00 8.90e+01
18 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 18000 4.55e+00 9.00e+01
19 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 19000 4.59e+00 8.85e+01
20 DQN 8 8 4 4 2.50e-01 False 2.50e-01 0 0 9 20000 4.53e+00 8.79e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 1000 5.08e-02 3.89e+00
2 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 2000 4.40e-01 8.55e+00
3 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 3000 1.61e+00 1.69e+01
4 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 4000 4.43e+00 2.62e+01
5 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 5000 7.32e+00 3.49e+01
6 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 6000 1.03e+01 4.43e+01
7 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 7000 1.31e+01 5.30e+01
8 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 8000 1.61e+01 6.22e+01
9 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 9000 1.90e+01 7.13e+01
10 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 10000 2.19e+01 8.02e+01
11 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 11000 2.41e+01 8.23e+01
12 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 12000 2.47e+01 8.35e+01
13 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 13000 2.46e+01 8.34e+01
14 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 14000 2.50e+01 8.46e+01
15 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 15000 2.49e+01 8.42e+01
16 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 16000 2.54e+01 8.55e+01
17 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 17000 2.59e+01 8.69e+01
18 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 18000 2.64e+01 8.82e+01
19 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 19000 2.63e+01 8.81e+01
20 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 0 20000 2.58e+01 8.68e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 1000 8.44e-02 4.22e+00
2 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 2000 2.89e-01 6.67e+00
3 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 3000 1.37e+00 1.56e+01
4 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 4000 5.23e+00 2.41e+01
5 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 5000 1.49e+01 3.43e+01
6 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 6000 2.37e+01 4.36e+01
7 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 7000 3.18e+01 5.22e+01
8 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 8000 4.11e+01 6.21e+01
9 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 9000 4.97e+01 7.10e+01
10 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 10000 5.88e+01 8.02e+01
11 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 11000 6.79e+01 8.98e+01
12 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 12000 7.55e+01 9.26e+01
13 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 13000 8.32e+01 9.33e+01
14 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 14000 8.30e+01 9.29e+01
15 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 15000 8.21e+01 9.21e+01
16 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 16000 8.26e+01 9.24e+01
17 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 17000 8.26e+01 9.24e+01
18 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 18000 8.33e+01 9.30e+01
19 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 19000 8.28e+01 9.24e+01
20 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 1 20000 8.33e+01 9.30e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 1000 6.22e-02 4.13e+00
2 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 2000 2.95e-01 6.66e+00
3 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 3000 2.17e+00 1.69e+01
4 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 4000 4.94e+00 2.55e+01
5 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 5000 7.89e+00 3.50e+01
6 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 6000 1.03e+01 4.33e+01
7 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 7000 1.28e+01 5.21e+01
8 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 8000 1.48e+01 6.18e+01
9 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 9000 1.76e+01 7.11e+01
10 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 10000 2.03e+01 8.03e+01
11 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 11000 2.21e+01 8.44e+01
12 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 12000 2.16e+01 8.36e+01
13 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 13000 2.16e+01 8.57e+01
14 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 14000 2.12e+01 8.66e+01
15 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 15000 2.17e+01 8.80e+01
16 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 16000 2.22e+01 8.99e+01
17 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 17000 2.28e+01 9.03e+01
18 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 18000 2.28e+01 9.03e+01
19 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 19000 2.33e+01 9.17e+01
20 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 2 20000 2.38e+01 9.24e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 1000 7.39e-02 3.89e+00
2 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 2000 4.96e-01 8.43e+00
3 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 3000 2.25e+00 1.76e+01
4 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 4000 4.41e+00 2.68e+01
5 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 5000 6.85e+00 3.56e+01
6 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 6000 8.81e+00 4.47e+01
7 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 7000 1.11e+01 5.31e+01
8 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 8000 1.36e+01 6.32e+01
9 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 9000 1.55e+01 7.17e+01
10 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 10000 1.76e+01 8.02e+01
11 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 11000 2.01e+01 9.00e+01
12 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 12000 2.05e+01 8.90e+01
13 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 13000 2.09e+01 8.80e+01
14 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 14000 2.02e+01 8.66e+01
15 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 15000 2.08e+01 8.66e+01
16 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 16000 2.07e+01 8.56e+01
17 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 17000 2.10e+01 8.57e+01
18 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 18000 2.18e+01 8.62e+01
19 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 19000 2.21e+01 8.63e+01
20 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 3 20000 2.29e+01 8.71e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 1000 5.56e-02 3.94e+00
2 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 2000 5.65e-01 8.70e+00
3 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 3000 6.14e+00 1.75e+01
4 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 4000 1.52e+01 2.70e+01
5 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 5000 2.33e+01 3.51e+01
6 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 6000 3.24e+01 4.44e+01
7 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 7000 4.17e+01 5.36e+01
8 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 8000 5.07e+01 6.31e+01
9 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 9000 5.95e+01 7.16e+01
10 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 10000 6.84e+01 8.06e+01
11 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 11000 7.62e+01 8.82e+01
12 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 12000 7.87e+01 8.83e+01
13 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 13000 7.73e+01 8.67e+01
14 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 14000 7.67e+01 8.61e+01
15 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 15000 7.77e+01 8.71e+01
16 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 16000 7.87e+01 8.80e+01
17 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 17000 7.87e+01 8.80e+01
18 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 18000 7.98e+01 8.91e+01
19 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 19000 8.19e+01 9.12e+01
20 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 4 20000 8.27e+01 9.18e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 1000 6.92e-02 3.83e+00
2 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 2000 3.28e-01 7.33e+00
3 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 3000 2.39e+00 1.54e+01
4 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 4000 5.36e+00 2.48e+01
5 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 5000 8.44e+00 3.44e+01
6 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 6000 1.14e+01 4.38e+01
7 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 7000 1.43e+01 5.25e+01
8 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 8000 1.74e+01 6.18e+01
9 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 9000 2.01e+01 7.02e+01
10 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 10000 2.33e+01 7.99e+01
11 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 11000 2.46e+01 8.25e+01
12 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 12000 2.52e+01 8.38e+01
13 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 13000 2.56e+01 8.50e+01
14 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 14000 2.61e+01 8.65e+01
15 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 15000 2.69e+01 8.91e+01
16 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 16000 2.71e+01 8.93e+01
17 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 17000 2.71e+01 8.94e+01
18 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 18000 2.80e+01 9.24e+01
19 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 19000 2.86e+01 9.43e+01
20 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 5 20000 2.88e+01 9.48e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 1000 1.06e-01 4.07e+00
2 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 2000 4.17e-01 7.60e+00
3 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 3000 2.50e+00 1.64e+01
4 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 4000 6.88e+00 2.56e+01
5 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 5000 1.15e+01 3.49e+01
6 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 6000 1.61e+01 4.45e+01
7 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 7000 2.02e+01 5.29e+01
8 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 8000 2.48e+01 6.26e+01
9 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 9000 2.93e+01 7.17e+01
10 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 10000 3.39e+01 8.11e+01
11 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 11000 3.84e+01 9.00e+01
12 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 12000 4.16e+01 9.25e+01
13 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 13000 4.13e+01 9.12e+01
14 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 14000 4.14e+01 9.15e+01
15 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 15000 4.07e+01 9.01e+01
16 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 16000 4.04e+01 8.95e+01
17 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 17000 4.11e+01 9.08e+01
18 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 18000 4.02e+01 8.92e+01
19 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 19000 3.95e+01 8.79e+01
20 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 6 20000 3.89e+01 8.65e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 1000 4.31e-02 3.91e+00
2 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 2000 3.53e-01 7.53e+00
3 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 3000 7.50e-01 1.74e+01
4 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 4000 1.36e+00 2.65e+01
5 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 5000 1.69e+00 3.50e+01
6 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 6000 2.06e+00 4.49e+01
7 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 7000 2.48e+00 5.39e+01
8 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 8000 3.08e+00 6.32e+01
9 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 9000 3.50e+00 7.13e+01
10 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 10000 4.30e+00 8.03e+01
11 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 11000 5.70e+00 8.87e+01
12 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 12000 7.30e+00 8.91e+01
13 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 13000 9.23e+00 8.63e+01
14 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 14000 1.17e+01 8.61e+01
15 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 15000 1.40e+01 8.60e+01
16 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 16000 1.59e+01 8.54e+01
17 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 17000 1.82e+01 8.69e+01
18 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 18000 2.03e+01 8.83e+01
19 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 19000 2.18e+01 8.97e+01
20 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 7 20000 2.34e+01 9.12e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 1000 1.04e-01 4.50e+00
2 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 2000 7.52e-01 8.79e+00
3 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 3000 7.77e+00 1.78e+01
4 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 4000 1.65e+01 2.71e+01
5 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 5000 2.59e+01 3.66e+01
6 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 6000 3.44e+01 4.48e+01
7 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 7000 4.30e+01 5.34e+01
8 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 8000 5.19e+01 6.25e+01
9 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 9000 6.09e+01 7.16e+01
10 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 10000 7.03e+01 8.12e+01
11 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 11000 7.95e+01 9.04e+01
12 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 12000 8.06e+01 8.95e+01
13 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 13000 8.03e+01 8.91e+01
14 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 14000 7.87e+01 8.74e+01
15 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 15000 7.89e+01 8.76e+01
16 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 16000 7.80e+01 8.68e+01
17 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 17000 8.02e+01 8.90e+01
18 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 18000 7.90e+01 8.76e+01
19 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 19000 7.69e+01 8.55e+01
20 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 8 20000 7.97e+01 8.83e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 1000 7.23e-02 4.25e+00
2 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 2000 6.99e-01 7.00e+00
3 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 3000 8.69e+00 1.55e+01
4 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 4000 1.80e+01 2.53e+01
5 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 5000 2.67e+01 3.43e+01
6 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 6000 3.48e+01 4.27e+01
7 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 7000 4.35e+01 5.17e+01
8 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 8000 5.25e+01 6.10e+01
9 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 9000 6.20e+01 7.06e+01
10 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 10000 6.99e+01 7.90e+01
11 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 11000 7.17e+01 8.05e+01
12 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 12000 7.19e+01 8.06e+01
13 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 13000 7.19e+01 8.06e+01
14 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 14000 7.44e+01 8.31e+01
15 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 15000 7.68e+01 8.57e+01
16 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 16000 7.76e+01 8.65e+01
17 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 17000 7.88e+01 8.78e+01
18 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 18000 8.01e+01 8.91e+01
19 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 19000 8.17e+01 9.07e+01
20 DQN 8 8 8 1 2.50e-01 False 2.50e-01 0 0 9 20000 8.28e+01 9.19e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 1000 2.46e-02 4.08e+00
2 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 2000 7.69e-02 5.06e+00
3 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 3000 6.30e-01 1.44e+01
4 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 4000 2.61e+00 2.48e+01
5 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 5000 4.80e+00 3.37e+01
6 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 6000 7.39e+00 4.36e+01
7 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 7000 9.17e+00 5.24e+01
8 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 8000 1.10e+01 6.17e+01
9 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 9000 1.36e+01 7.15e+01
10 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 10000 1.61e+01 8.08e+01
11 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 11000 1.86e+01 8.97e+01
12 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 12000 1.97e+01 9.44e+01
13 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 13000 1.91e+01 9.19e+01
14 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 14000 1.79e+01 9.12e+01
15 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 15000 1.76e+01 9.19e+01
16 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 16000 1.81e+01 9.14e+01
17 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 17000 1.75e+01 8.96e+01
18 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 18000 1.79e+01 8.99e+01
19 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 19000 1.72e+01 8.97e+01
20 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 0 20000 1.67e+01 8.72e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 1000 2.42e-02 4.03e+00
2 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 2000 1.60e-01 6.00e+00
3 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 3000 1.32e+00 1.59e+01
4 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 4000 4.21e+00 2.56e+01
5 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 5000 7.01e+00 3.45e+01
6 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 6000 9.87e+00 4.39e+01
7 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 7000 1.17e+01 5.32e+01
8 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 8000 1.40e+01 6.29e+01
9 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 9000 1.66e+01 7.20e+01
10 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 10000 1.96e+01 8.11e+01
11 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 11000 2.20e+01 8.85e+01
12 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 12000 2.30e+01 8.88e+01
13 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 13000 2.28e+01 8.93e+01
14 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 14000 2.26e+01 8.84e+01
15 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 15000 2.11e+01 8.74e+01
16 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 16000 2.05e+01 8.61e+01
17 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 17000 1.98e+01 8.65e+01
18 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 18000 1.97e+01 8.71e+01
19 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 19000 2.02e+01 8.79e+01
20 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 1 20000 1.98e+01 8.81e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 1000 1.99e-02 3.95e+00
2 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 2000 2.06e-01 9.36e+00
3 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 3000 4.40e-01 1.82e+01
4 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 4000 6.10e-01 2.66e+01
5 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 5000 8.00e-01 3.56e+01
6 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 6000 1.20e+00 4.52e+01
7 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 7000 1.44e+00 5.42e+01
8 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 8000 2.16e+00 6.37e+01
9 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 9000 2.80e+00 7.26e+01
10 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 10000 3.87e+00 8.09e+01
11 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 11000 5.85e+00 9.01e+01
12 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 12000 7.50e+00 9.36e+01
13 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 13000 9.40e+00 9.25e+01
14 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 14000 1.12e+01 9.23e+01
15 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 15000 1.27e+01 9.16e+01
16 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 16000 1.39e+01 8.97e+01
17 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 17000 1.43e+01 8.94e+01
18 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 18000 1.54e+01 8.71e+01
19 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 19000 1.54e+01 8.53e+01
20 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 2 20000 1.49e+01 8.27e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 1000 4.18e-02 4.18e+00
2 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 2000 1.61e-01 6.97e+00
3 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 3000 7.00e-01 1.60e+01
4 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 4000 1.26e+00 2.57e+01
5 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 5000 3.23e+00 3.51e+01
6 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 6000 5.77e+00 4.44e+01
7 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 7000 8.12e+00 5.24e+01
8 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 8000 1.02e+01 6.24e+01
9 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 9000 1.30e+01 7.08e+01
10 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 10000 1.66e+01 8.04e+01
11 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 11000 1.88e+01 8.94e+01
12 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 12000 2.05e+01 9.04e+01
13 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 13000 2.30e+01 8.99e+01
14 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 14000 2.42e+01 8.98e+01
15 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 15000 2.36e+01 8.89e+01
16 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 16000 2.45e+01 8.79e+01
17 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 17000 2.49e+01 8.88e+01
18 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 18000 2.41e+01 8.72e+01
19 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 19000 2.49e+01 8.66e+01
20 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 3 20000 2.50e+01 8.77e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 1000 1.26e-02 4.18e+00
2 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 2000 7.83e-02 4.50e+00
3 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 3000 5.60e-01 1.34e+01
4 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 4000 1.14e+00 2.32e+01
5 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 5000 1.93e+00 3.26e+01
6 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 6000 3.19e+00 4.17e+01
7 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 7000 4.39e+00 5.18e+01
8 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 8000 5.59e+00 5.99e+01
9 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 9000 6.71e+00 6.82e+01
10 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 10000 8.25e+00 7.74e+01
11 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 11000 9.27e+00 8.42e+01
12 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 12000 1.03e+01 8.53e+01
13 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 13000 1.12e+01 8.68e+01
14 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 14000 1.15e+01 8.72e+01
15 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 15000 1.17e+01 9.03e+01
16 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 16000 1.18e+01 8.89e+01
17 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 17000 1.17e+01 8.85e+01
18 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 18000 1.19e+01 8.92e+01
19 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 19000 1.19e+01 9.02e+01
20 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 4 20000 1.18e+01 8.87e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 1000 1.51e-02 3.75e+00
2 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 2000 1.23e-01 5.62e+00
3 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 3000 9.10e-01 1.37e+01
4 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 4000 1.93e+00 2.27e+01
5 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 5000 3.16e+00 3.24e+01
6 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 6000 4.23e+00 4.25e+01
7 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 7000 5.14e+00 5.15e+01
8 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 8000 6.24e+00 6.02e+01
9 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 9000 7.51e+00 6.99e+01
10 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 10000 8.84e+00 8.00e+01
11 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 11000 9.80e+00 8.76e+01
12 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 12000 1.03e+01 8.90e+01
13 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 13000 1.07e+01 9.05e+01
14 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 14000 1.04e+01 8.79e+01
15 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 15000 1.08e+01 8.91e+01
16 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 16000 1.10e+01 8.93e+01
17 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 17000 1.11e+01 8.88e+01
18 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 18000 1.11e+01 8.91e+01
19 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 19000 1.12e+01 8.76e+01
20 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 5 20000 1.14e+01 8.75e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 1000 3.48e-02 4.32e+00
2 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 2000 3.43e-01 8.81e+00
3 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 3000 1.33e+00 1.76e+01
4 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 4000 2.83e+00 2.72e+01
5 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 5000 4.06e+00 3.57e+01
6 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 6000 5.46e+00 4.43e+01
7 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 7000 7.59e+00 5.36e+01
8 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 8000 8.90e+00 6.25e+01
9 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 9000 1.08e+01 7.15e+01
10 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 10000 1.35e+01 8.00e+01
11 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 11000 1.54e+01 8.16e+01
12 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 12000 1.55e+01 8.04e+01
13 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 13000 1.63e+01 8.05e+01
14 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 14000 1.69e+01 8.21e+01
15 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 15000 1.80e+01 8.27e+01
16 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 16000 1.90e+01 8.32e+01
17 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 17000 2.06e+01 8.57e+01
18 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 18000 2.08e+01 8.70e+01
19 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 19000 2.07e+01 8.76e+01
20 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 6 20000 1.92e+01 8.86e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 1000 1.66e-02 4.15e+00
2 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 2000 1.71e-01 7.14e+00
3 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 3000 5.60e-01 1.55e+01
4 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 4000 1.28e+00 2.54e+01
5 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 5000 2.41e+00 3.47e+01
6 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 6000 4.13e+00 4.35e+01
7 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 7000 6.90e+00 5.25e+01
8 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 8000 9.73e+00 6.20e+01
9 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 9000 1.25e+01 7.08e+01
10 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 10000 1.56e+01 8.04e+01
11 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 11000 1.85e+01 8.98e+01
12 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 12000 2.12e+01 9.39e+01
13 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 13000 2.33e+01 9.20e+01
14 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 14000 2.53e+01 9.16e+01
15 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 15000 2.66e+01 9.06e+01
16 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 16000 2.59e+01 8.75e+01
17 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 17000 2.57e+01 8.67e+01
18 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 18000 2.54e+01 8.56e+01
19 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 19000 2.54e+01 8.55e+01
20 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 7 20000 2.51e+01 8.47e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 1000 3.94e-02 3.93e+00
2 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 2000 1.95e-01 7.47e+00
3 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 3000 4.00e-01 1.55e+01
4 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 4000 7.50e-01 2.45e+01
5 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 5000 1.12e+00 3.40e+01
6 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 6000 2.01e+00 4.28e+01
7 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 7000 3.15e+00 5.23e+01
8 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 8000 4.34e+00 6.14e+01
9 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 9000 5.71e+00 7.12e+01
10 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 10000 7.14e+00 8.01e+01
11 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 11000 8.20e+00 8.19e+01
12 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 12000 9.32e+00 8.52e+01
13 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 13000 1.06e+01 9.12e+01
14 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 14000 1.18e+01 9.15e+01
15 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 15000 1.25e+01 9.28e+01
16 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 16000 1.27e+01 9.27e+01
17 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 17000 1.25e+01 9.15e+01
18 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 18000 1.28e+01 9.24e+01
19 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 19000 1.25e+01 9.08e+01
20 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 8 20000 1.25e+01 9.07e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 1000 2.31e-02 3.83e+00
2 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 2000 1.05e-01 6.14e+00
3 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 3000 1.02e+00 1.49e+01
4 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 4000 3.43e+00 2.45e+01
5 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 5000 6.03e+00 3.37e+01
6 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 6000 8.65e+00 4.32e+01
7 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 7000 1.11e+01 5.17e+01
8 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 8000 1.39e+01 6.14e+01
9 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 9000 1.66e+01 7.06e+01
10 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 10000 1.93e+01 8.01e+01
11 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 11000 2.17e+01 8.51e+01
12 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 12000 2.36e+01 8.93e+01
13 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 13000 2.38e+01 8.89e+01
14 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 14000 2.23e+01 8.38e+01
15 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 15000 2.22e+01 8.31e+01
16 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 16000 2.24e+01 8.38e+01
17 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 17000 2.25e+01 8.40e+01
18 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 18000 2.21e+01 8.27e+01
19 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 19000 2.27e+01 8.42e+01
20 DQN 8 8 8 2 2.50e-01 False 2.50e-01 0 0 9 20000 2.20e+01 8.19e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 1000 1.33e-02 4.44e+00
2 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 2000 5.39e-02 4.89e+00
3 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 3000 3.20e-01 1.50e+01
4 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 4000 5.50e-01 2.40e+01
5 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 5000 7.70e-01 3.35e+01
6 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 6000 9.00e-01 4.13e+01
7 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 7000 1.11e+00 5.16e+01
8 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 8000 1.38e+00 6.09e+01
9 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 9000 1.69e+00 6.96e+01
10 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 10000 1.79e+00 7.11e+01
11 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 11000 2.02e+00 7.88e+01
12 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 12000 2.27e+00 8.56e+01
13 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 13000 2.42e+00 8.82e+01
14 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 14000 2.69e+00 8.91e+01
15 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 15000 3.14e+00 9.04e+01
16 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 16000 3.34e+00 9.07e+01
17 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 17000 3.68e+00 9.11e+01
18 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 18000 4.04e+00 9.34e+01
19 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 19000 4.12e+00 9.33e+01
20 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 0 20000 4.53e+00 9.38e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 1000 2.59e-02 4.30e+00
2 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 2000 1.41e-02 4.69e+00
3 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 3000 5.99e-02 5.98e+00
4 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 4000 2.00e-01 1.03e+01
5 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 5000 4.70e-01 1.89e+01
6 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 6000 1.00e+00 2.81e+01
7 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 7000 1.70e+00 3.73e+01
8 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 8000 1.97e+00 4.61e+01
9 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 9000 2.49e+00 5.36e+01
10 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 10000 2.87e+00 5.68e+01
11 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 11000 3.91e+00 6.66e+01
12 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 12000 4.74e+00 7.58e+01
13 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 13000 5.76e+00 8.44e+01
14 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 14000 6.21e+00 8.97e+01
15 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 15000 6.33e+00 8.99e+01
16 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 16000 6.16e+00 8.89e+01
17 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 17000 6.37e+00 8.88e+01
18 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 18000 6.27e+00 8.94e+01
19 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 19000 6.12e+00 9.15e+01
20 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 1 20000 6.08e+00 9.02e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 1000 0.00e+00 3.88e+00
2 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 2000 2.38e-02 4.75e+00
3 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 3000 2.68e-02 3.35e+00
4 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 4000 7.25e-02 5.12e+00
5 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 5000 4.70e-01 1.31e+01
6 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 6000 7.60e-01 2.27e+01
7 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 7000 1.00e+00 3.25e+01
8 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 8000 1.52e+00 4.18e+01
9 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 9000 2.21e+00 5.18e+01
10 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 10000 3.11e+00 6.12e+01
11 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 11000 3.97e+00 7.09e+01
12 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 12000 4.57e+00 7.68e+01
13 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 13000 6.31e+00 8.28e+01
14 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 14000 7.90e+00 8.68e+01
15 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 15000 9.55e+00 8.72e+01
16 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 16000 1.11e+01 8.76e+01
17 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 17000 1.22e+01 8.48e+01
18 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 18000 1.41e+01 8.68e+01
19 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 19000 1.52e+01 8.57e+01
20 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 2 20000 1.61e+01 8.67e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 1000 4.13e-03 4.13e+00
2 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 2000 3.95e-02 6.58e+00
3 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 3000 3.50e-01 1.42e+01
4 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 4000 8.30e-01 2.41e+01
5 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 5000 1.36e+00 3.25e+01
6 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 6000 2.03e+00 4.11e+01
7 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 7000 2.76e+00 5.06e+01
8 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 8000 3.53e+00 6.01e+01
9 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 9000 4.62e+00 6.97e+01
10 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 10000 5.69e+00 7.99e+01
11 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 11000 6.73e+00 8.84e+01
12 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 12000 7.51e+00 9.21e+01
13 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 13000 8.14e+00 9.13e+01
14 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 14000 8.84e+00 9.20e+01
15 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 15000 9.04e+00 9.25e+01
16 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 16000 9.04e+00 9.13e+01
17 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 17000 9.17e+00 9.02e+01
18 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 18000 9.10e+00 8.94e+01
19 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 19000 8.66e+00 9.03e+01
20 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 3 20000 8.21e+00 8.92e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 1000 8.10e-03 4.04e+00
2 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 2000 3.04e-03 3.04e+00
3 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 3000 2.82e-02 4.04e+00
4 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 4000 7.53e-02 6.45e+00
5 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 5000 3.40e-01 1.84e+01
6 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 6000 4.70e-01 2.78e+01
7 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 7000 6.10e-01 3.71e+01
8 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 8000 8.50e-01 4.62e+01
9 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 9000 1.23e+00 5.57e+01
10 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 10000 1.69e+00 6.52e+01
11 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 11000 2.22e+00 7.19e+01
12 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 12000 2.69e+00 7.63e+01
13 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 13000 3.14e+00 8.35e+01
14 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 14000 3.90e+00 8.71e+01
15 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 15000 4.02e+00 8.94e+01
16 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 16000 4.56e+00 9.09e+01
17 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 17000 5.13e+00 9.23e+01
18 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 18000 5.75e+00 9.23e+01
19 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 19000 5.53e+00 8.99e+01
20 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 4 20000 5.64e+00 8.96e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 1000 1.20e-02 4.02e+00
2 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 2000 3.02e-02 4.97e+00
3 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 3000 7.87e-02 5.68e+00
4 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 4000 4.93e-02 7.03e+00
5 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 5000 7.60e-01 1.69e+01
6 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 6000 1.97e+00 2.64e+01
7 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 7000 3.48e+00 3.65e+01
8 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 8000 4.91e+00 4.53e+01
9 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 9000 6.44e+00 5.49e+01
10 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 10000 8.69e+00 6.40e+01
11 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 11000 1.08e+01 7.35e+01
12 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 12000 1.33e+01 8.03e+01
13 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 13000 1.52e+01 8.74e+01
14 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 14000 1.61e+01 8.85e+01
15 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 15000 1.75e+01 8.80e+01
16 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 16000 1.82e+01 8.96e+01
17 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 17000 1.94e+01 9.05e+01
18 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 18000 1.94e+01 9.07e+01
19 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 19000 1.90e+01 9.03e+01
20 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 5 20000 1.89e+01 8.94e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 1000 1.82e-02 4.51e+00
2 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 2000 5.63e-02 6.26e+00
3 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 3000 3.50e-01 1.49e+01
4 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 4000 8.50e-01 2.45e+01
5 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 5000 1.42e+00 3.35e+01
6 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 6000 1.90e+00 4.31e+01
7 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 7000 2.40e+00 5.22e+01
8 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 8000 2.71e+00 6.14e+01
9 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 9000 3.04e+00 7.09e+01
10 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 10000 3.49e+00 7.98e+01
11 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 11000 4.08e+00 8.44e+01
12 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 12000 4.68e+00 8.81e+01
13 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 13000 4.91e+00 8.78e+01
14 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 14000 5.64e+00 8.75e+01
15 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 15000 6.53e+00 8.80e+01
16 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 16000 7.76e+00 8.91e+01
17 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 17000 8.71e+00 8.79e+01
18 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 18000 9.53e+00 8.66e+01
19 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 19000 1.08e+01 8.47e+01
20 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 6 20000 1.12e+01 8.23e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 1000 3.21e-02 4.00e+00
2 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 2000 7.69e-02 6.42e+00
3 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 3000 3.70e-01 1.16e+01
4 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 4000 6.70e-01 2.07e+01
5 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 5000 8.30e-01 2.94e+01
6 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 6000 1.00e+00 3.69e+01
7 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 7000 1.30e+00 4.45e+01
8 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 8000 1.63e+00 5.47e+01
9 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 9000 1.84e+00 6.43e+01
10 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 10000 2.15e+00 7.35e+01
11 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 11000 2.44e+00 8.28e+01
12 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 12000 2.59e+00 8.76e+01
13 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 13000 2.57e+00 8.91e+01
14 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 14000 2.70e+00 8.81e+01
15 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 15000 2.86e+00 8.81e+01
16 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 16000 2.87e+00 9.05e+01
17 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 17000 2.83e+00 8.99e+01
18 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 18000 2.90e+00 8.98e+01
19 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 19000 2.98e+00 9.00e+01
20 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 7 20000 2.95e+00 8.90e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 1000 3.17e-02 4.49e+00
2 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 2000 6.29e-02 7.03e+00
3 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 3000 4.10e-01 1.37e+01
4 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 4000 7.30e-01 2.31e+01
5 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 5000 1.21e+00 3.29e+01
6 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 6000 1.67e+00 4.26e+01
7 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 7000 2.20e+00 5.23e+01
8 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 8000 2.65e+00 6.09e+01
9 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 9000 3.05e+00 6.95e+01
10 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 10000 3.79e+00 7.90e+01
11 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 11000 4.19e+00 8.80e+01
12 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 12000 4.28e+00 8.90e+01
13 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 13000 4.33e+00 8.99e+01
14 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 14000 4.39e+00 8.99e+01
15 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 15000 4.53e+00 9.10e+01
16 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 16000 4.53e+00 9.03e+01
17 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 17000 4.55e+00 9.08e+01
18 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 18000 4.58e+00 8.97e+01
19 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 19000 4.51e+00 9.01e+01
20 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 8 20000 4.42e+00 9.06e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 1000 1.18e-02 3.92e+00
2 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 2000 3.36e-02 6.73e+00
3 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 3000 3.50e-01 1.51e+01
4 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 4000 6.90e-01 2.32e+01
5 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 5000 1.38e+00 3.27e+01
6 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 6000 2.01e+00 4.15e+01
7 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 7000 2.59e+00 5.10e+01
8 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 8000 3.57e+00 6.01e+01
9 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 9000 4.65e+00 6.76e+01
10 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 10000 5.25e+00 7.08e+01
11 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 11000 5.88e+00 7.45e+01
12 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 12000 6.49e+00 8.37e+01
13 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 13000 6.81e+00 8.55e+01
14 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 14000 7.32e+00 8.58e+01
15 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 15000 7.46e+00 8.50e+01
16 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 16000 7.80e+00 8.59e+01
17 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 17000 7.60e+00 8.66e+01
18 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 18000 7.40e+00 8.70e+01
19 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 19000 7.32e+00 8.78e+01
20 DQN 8 8 8 3 2.50e-01 False 2.50e-01 0 0 9 20000 7.47e+00 8.80e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 1000 4.44e-03 4.44e+00
2 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 2000 1.25e-02 4.17e+00
3 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 3000 1.30e-01 1.28e+01
4 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 4000 3.00e-01 2.19e+01
5 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 5000 5.20e-01 3.16e+01
6 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 6000 7.70e-01 4.14e+01
7 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 7000 8.80e-01 5.09e+01
8 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 8000 1.07e+00 6.05e+01
9 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 9000 1.22e+00 7.00e+01
10 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 10000 1.39e+00 7.55e+01
11 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 11000 1.66e+00 8.14e+01
12 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 12000 1.79e+00 8.60e+01
13 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 13000 1.98e+00 8.75e+01
14 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 14000 2.44e+00 8.84e+01
15 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 15000 3.22e+00 8.60e+01
16 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 16000 3.91e+00 8.47e+01
17 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 17000 5.18e+00 8.68e+01
18 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 18000 5.97e+00 8.77e+01
19 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 19000 6.65e+00 8.69e+01
20 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 0 20000 7.95e+00 8.50e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 1000 1.29e-02 4.29e+00
2 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 2000 6.82e-02 7.45e+00
3 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 3000 2.10e-01 1.68e+01
4 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 4000 3.30e-01 2.57e+01
5 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 5000 2.50e-01 2.37e+01
6 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 6000 1.06e-02 5.33e+00
7 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 7000 6.29e-03 6.25e+00
8 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 8000 1.70e-01 1.66e+01
9 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 9000 2.40e-01 2.64e+01
10 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 10000 3.60e-01 3.61e+01
11 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 11000 4.80e-01 4.51e+01
12 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 12000 7.20e-01 5.40e+01
13 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 13000 8.70e-01 6.27e+01
14 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 14000 9.50e-01 7.17e+01
15 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 15000 1.10e+00 7.61e+01
16 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 16000 1.08e+00 7.69e+01
17 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 17000 1.26e+00 7.79e+01
18 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 18000 1.39e+00 7.81e+01
19 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 19000 1.66e+00 8.06e+01
20 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 1 20000 1.74e+00 8.75e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 1000 8.10e-03 4.05e+00
2 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 2000 1.67e-02 5.56e+00
3 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 3000 4.72e-02 8.55e+00
4 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 4000 2.40e-01 1.89e+01
5 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 5000 4.00e-01 2.84e+01
6 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 6000 6.00e-01 3.78e+01
7 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 7000 7.40e-01 4.75e+01
8 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 8000 9.80e-01 5.78e+01
9 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 9000 1.18e+00 6.73e+01
10 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 10000 1.33e+00 7.57e+01
11 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 11000 1.48e+00 8.34e+01
12 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 12000 1.51e+00 8.40e+01
13 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 13000 1.54e+00 8.72e+01
14 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 14000 1.50e+00 8.81e+01
15 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 15000 1.54e+00 8.89e+01
16 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 16000 1.54e+00 8.79e+01
17 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 17000 1.48e+00 8.85e+01
18 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 18000 1.54e+00 8.95e+01
19 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 19000 1.58e+00 8.78e+01
20 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 2 20000 1.82e+00 8.82e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 1000 4.20e-03 4.19e+00
2 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 2000 3.29e-02 6.59e+00
3 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 3000 8.80e-03 2.92e+00
4 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 4000 1.00e-01 1.29e+01
5 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 5000 1.60e-01 2.24e+01
6 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 6000 2.40e-01 3.16e+01
7 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 7000 3.60e-01 4.03e+01
8 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 8000 4.60e-01 4.77e+01
9 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 9000 4.60e-01 5.54e+01
10 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 10000 5.60e-01 6.36e+01
11 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 11000 6.70e-01 7.22e+01
12 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 12000 8.70e-01 7.91e+01
13 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 13000 1.03e+00 8.08e+01
14 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 14000 9.70e-01 7.93e+01
15 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 15000 1.06e+00 8.06e+01
16 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 16000 1.18e+00 8.53e+01
17 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 17000 1.23e+00 8.44e+01
18 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 18000 1.51e+00 8.72e+01
19 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 19000 1.59e+00 8.74e+01
20 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 3 20000 1.67e+00 8.94e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 1000 8.30e-03 4.11e+00
2 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 2000 1.18e-02 5.97e+00
3 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 3000 7.00e-02 1.19e+01
4 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 4000 1.70e-01 2.13e+01
5 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 5000 3.10e-01 2.88e+01
6 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 6000 4.00e-01 3.52e+01
7 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 7000 6.10e-01 4.42e+01
8 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 8000 7.80e-01 5.33e+01
9 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 9000 9.00e-01 6.19e+01
10 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 10000 9.60e-01 7.03e+01
11 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 11000 1.08e+00 7.64e+01
12 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 12000 1.15e+00 8.00e+01
13 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 13000 1.17e+00 8.77e+01
14 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 14000 1.16e+00 8.82e+01
15 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 15000 1.07e+00 8.75e+01
16 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 16000 9.10e-01 8.85e+01
17 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 17000 9.10e-01 9.13e+01
18 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 18000 9.30e-01 9.01e+01
19 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 19000 8.60e-01 8.98e+01
20 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 4 20000 8.10e-01 8.94e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 1000 4.13e-03 4.13e+00
2 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 2000 2.30e-02 5.75e+00
3 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 3000 2.43e-03 2.43e+00
4 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 4000 0.00e+00 2.00e+00
5 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 5000 4.38e-03 2.13e+00
6 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 6000 4.22e-03 4.33e+00
7 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 7000 8.66e-03 4.29e+00
8 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 8000 2.92e-02 7.36e+00
9 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 9000 1.03e-02 5.12e+00
10 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 10000 4.23e-02 6.96e+00
11 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 11000 1.60e-01 1.56e+01
12 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 12000 3.10e-01 2.45e+01
13 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 13000 4.80e-01 3.47e+01
14 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 14000 6.80e-01 4.42e+01
15 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 15000 7.30e-01 5.34e+01
16 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 16000 7.40e-01 6.10e+01
17 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 17000 8.40e-01 6.88e+01
18 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 18000 8.50e-01 7.27e+01
19 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 19000 8.70e-01 7.53e+01
20 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 5 20000 1.11e+00 8.37e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 1000 3.86e-03 3.83e+00
2 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 2000 2.58e-02 6.48e+00
3 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 3000 1.30e-01 1.49e+01
4 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 4000 2.60e-01 2.44e+01
5 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 5000 3.20e-01 3.37e+01
6 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 6000 3.90e-01 4.27e+01
7 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 7000 5.40e-01 5.22e+01
8 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 8000 6.10e-01 6.15e+01
9 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 9000 6.70e-01 7.13e+01
10 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 10000 7.60e-01 8.00e+01
11 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 11000 9.10e-01 8.86e+01
12 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 12000 1.00e+00 8.99e+01
13 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 13000 1.10e+00 9.09e+01
14 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 14000 1.23e+00 9.05e+01
15 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 15000 1.23e+00 9.03e+01
16 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 16000 1.24e+00 8.95e+01
17 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 17000 1.39e+00 8.98e+01
18 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 18000 1.51e+00 9.05e+01
19 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 19000 1.54e+00 8.97e+01
20 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 6 20000 1.49e+00 8.88e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 1000 0.00e+00 4.23e+00
2 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 2000 1.79e-02 4.49e+00
3 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 3000 9.00e-02 1.13e+01
4 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 4000 7.00e-02 1.24e+01
5 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 5000 2.14e-02 5.68e+00
6 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 6000 3.00e-02 9.94e+00
7 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 7000 7.00e-02 1.91e+01
8 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 8000 1.40e-01 2.86e+01
9 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 9000 2.40e-01 3.78e+01
10 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 10000 3.70e-01 4.75e+01
11 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 11000 5.30e-01 5.74e+01
12 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 12000 5.60e-01 6.13e+01
13 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 13000 6.90e-01 6.22e+01
14 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 14000 7.10e-01 6.32e+01
15 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 15000 7.90e-01 7.17e+01
16 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 16000 9.10e-01 8.04e+01
17 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 17000 9.20e-01 8.25e+01
18 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 18000 8.80e-01 8.13e+01
19 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 19000 8.90e-01 8.05e+01
20 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 7 20000 9.80e-01 8.62e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 1000 3.89e-03 3.89e+00
2 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 2000 0.00e+00 3.98e+00
3 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 3000 2.42e-02 7.94e+00
4 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 4000 1.30e-01 1.42e+01
5 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 5000 6.00e-02 1.00e+01
6 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 6000 4.00e-02 6.65e+00
7 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 7000 1.50e-01 1.62e+01
8 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 8000 2.90e-01 2.56e+01
9 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 9000 4.70e-01 3.58e+01
10 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 10000 7.20e-01 4.45e+01
11 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 11000 1.01e+00 5.45e+01
12 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 12000 1.40e+00 6.41e+01
13 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 13000 1.91e+00 7.33e+01
14 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 14000 2.82e+00 8.24e+01
15 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 15000 3.67e+00 8.66e+01
16 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 16000 5.20e+00 8.91e+01
17 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 17000 6.68e+00 8.97e+01
18 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 18000 8.01e+00 8.93e+01
19 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 19000 9.28e+00 9.12e+01
20 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 8 20000 1.08e+01 9.35e+01
# training_iteration, algorithm, state_space_size, action_space_size, delay, sequence_length, reward_density, make_denser, terminal_state_density, transition_noise, reward_noise, dummy_seed, timesteps_total, episode_reward_mean, episode_len_mean
1 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 1000 0.00e+00 4.16e+00
2 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 2000 3.80e-03 3.83e+00
3 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 3000 2.99e-02 7.45e+00
4 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 4000 1.10e-01 1.70e+01
5 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 5000 2.46e-02 8.18e+00
6 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 6000 3.73e-02 7.44e+00
7 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 7000 2.97e-02 9.68e+00
8 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 8000 9.00e-02 1.77e+01
9 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 9000 1.20e-01 2.67e+01
10 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 10000 2.00e-01 3.66e+01
11 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 11000 3.20e-01 4.51e+01
12 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 12000 4.40e-01 5.49e+01
13 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 13000 5.20e-01 6.34e+01
14 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 14000 5.70e-01 6.96e+01
15 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 15000 5.90e-01 7.29e+01
16 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 16000 6.40e-01 7.79e+01
17 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 17000 6.70e-01 7.90e+01
18 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 18000 7.40e-01 8.19e+01
19 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 19000 7.10e-01 8.32e+01
20 DQN 8 8 8 4 2.50e-01 False 2.50e-01 0 0 9 20000 7.20e-01 8.19e+01
